Starting mismatch_tf.py...

=== Training Teacher (BigTransformer) on CIFAR-100 Subset ===
Files already downloaded and verified
Files already downloaded and verified
[Epoch 1] Loss: 4.4010
[Epoch 2] Loss: 4.1960
[Epoch 3] Loss: 4.0402
[Epoch 4] Loss: 3.9264
[Epoch 5] Loss: 3.8265
[Epoch 6] Loss: 3.7418
[Epoch 7] Loss: 3.6577
[Epoch 8] Loss: 3.5748
[Epoch 9] Loss: 3.4821
[Epoch 10] Loss: 3.4020
[Epoch 11] Loss: 3.3335
[Epoch 12] Loss: 3.2448
[Epoch 13] Loss: 3.1740
[Epoch 14] Loss: 3.1039
[Epoch 15] Loss: 3.0036
[Epoch 16] Loss: 2.9280
[Epoch 17] Loss: 2.8520
[Epoch 18] Loss: 2.7813
[Epoch 19] Loss: 2.7041
[Epoch 20] Loss: 2.6049
[Epoch 21] Loss: 2.5532
[Epoch 22] Loss: 2.4715
[Epoch 23] Loss: 2.4159
[Epoch 24] Loss: 2.3327
[Epoch 25] Loss: 2.2628
[Epoch 26] Loss: 2.2001
[Epoch 27] Loss: 2.1263
[Epoch 28] Loss: 2.0790
[Epoch 29] Loss: 2.0346
[Epoch 30] Loss: 1.9330
[Epoch 31] Loss: 1.9194
[Epoch 32] Loss: 1.8261
[Epoch 33] Loss: 1.8170
[Epoch 34] Loss: 1.7600
[Epoch 35] Loss: 1.6882
[Epoch 36] Loss: 1.6323
[Epoch 37] Loss: 1.5854
[Epoch 38] Loss: 1.5682
[Epoch 39] Loss: 1.5056
[Epoch 40] Loss: 1.5080
[Epoch 41] Loss: 1.4193
[Epoch 42] Loss: 1.3681
[Epoch 43] Loss: 1.3927
[Epoch 44] Loss: 1.3345
[Epoch 45] Loss: 1.2648
[Epoch 46] Loss: 1.2418
[Epoch 47] Loss: 1.2152
[Epoch 48] Loss: 1.2129
[Epoch 49] Loss: 1.1853
[Epoch 50] Loss: 1.1061
[Epoch 51] Loss: 1.1037
[Epoch 52] Loss: 1.0937
[Epoch 53] Loss: 1.0745
[Epoch 54] Loss: 1.0202
[Epoch 55] Loss: 0.9854
[Epoch 56] Loss: 0.9685
[Epoch 57] Loss: 0.8981
[Epoch 58] Loss: 0.9250
[Epoch 59] Loss: 0.9224
[Epoch 60] Loss: 0.8297
Files already downloaded and verified
Files already downloaded and verified

=== Run 1/5, seed=42 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline transformer model (raw only)...
[Epoch 1] Loss: 2.1482
[Epoch 2] Loss: 2.0376
[Epoch 3] Loss: 1.9817
[Epoch 4] Loss: 1.9161
[Epoch 5] Loss: 1.8729
[Epoch 6] Loss: 1.8334
[Epoch 7] Loss: 1.7798
[Epoch 8] Loss: 1.7618
[Epoch 9] Loss: 1.7460
[Epoch 10] Loss: 1.7054
[Epoch 11] Loss: 1.6904
[Epoch 12] Loss: 1.6617
[Epoch 13] Loss: 1.6664
[Epoch 14] Loss: 1.6129
[Epoch 15] Loss: 1.5902
[Epoch 16] Loss: 1.5684
[Epoch 17] Loss: 1.5557
[Epoch 18] Loss: 1.5162
[Epoch 19] Loss: 1.4812
[Epoch 20] Loss: 1.4804
[Epoch 21] Loss: 1.4305
[Epoch 22] Loss: 1.4325
[Epoch 23] Loss: 1.4056
[Epoch 24] Loss: 1.3946
[Epoch 25] Loss: 1.3950
[Epoch 26] Loss: 1.3353
[Epoch 27] Loss: 1.3400
[Epoch 28] Loss: 1.3049
[Epoch 29] Loss: 1.2865
[Epoch 30] Loss: 1.2559
Training linear probe transformer model (teacher fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.6795
[Linear Prob Epoch 2] Loss: 1.3973
[Linear Prob Epoch 3] Loss: 1.3213
[Linear Prob Epoch 4] Loss: 1.2783
[Linear Prob Epoch 5] Loss: 1.2455
[Linear Prob Epoch 6] Loss: 1.2128
[Linear Prob Epoch 7] Loss: 1.1670
[Linear Prob Epoch 8] Loss: 1.1413
[Linear Prob Epoch 9] Loss: 1.1565
[Linear Prob Epoch 10] Loss: 1.1132
[Linear Prob Epoch 11] Loss: 1.1115
[Linear Prob Epoch 12] Loss: 1.0941
[Linear Prob Epoch 13] Loss: 1.0767
[Linear Prob Epoch 14] Loss: 1.0753
[Linear Prob Epoch 15] Loss: 1.0519
[Linear Prob Epoch 16] Loss: 1.0647
[Linear Prob Epoch 17] Loss: 1.0513
[Linear Prob Epoch 18] Loss: 1.0611
[Linear Prob Epoch 19] Loss: 1.0451
[Linear Prob Epoch 20] Loss: 1.0049
[Linear Prob Epoch 21] Loss: 1.0262
[Linear Prob Epoch 22] Loss: 1.0312
[Linear Prob Epoch 23] Loss: 1.0354
[Linear Prob Epoch 24] Loss: 1.0217
[Linear Prob Epoch 25] Loss: 1.0194
[Linear Prob Epoch 26] Loss: 0.9952
[Linear Prob Epoch 27] Loss: 0.9954
[Linear Prob Epoch 28] Loss: 0.9854
[Linear Prob Epoch 29] Loss: 1.0088
[Linear Prob Epoch 30] Loss: 1.0180
Training enhanced transformer model (concatenation)...
[Enhanced Epoch 1] Loss: 1.6154
[Enhanced Epoch 2] Loss: 1.2210
[Enhanced Epoch 3] Loss: 1.0414
[Enhanced Epoch 4] Loss: 0.9178
[Enhanced Epoch 5] Loss: 0.8233
[Enhanced Epoch 6] Loss: 0.7432
[Enhanced Epoch 7] Loss: 0.6808
[Enhanced Epoch 8] Loss: 0.6283
[Enhanced Epoch 9] Loss: 0.5756
[Enhanced Epoch 10] Loss: 0.5414
[Enhanced Epoch 11] Loss: 0.5026
[Enhanced Epoch 12] Loss: 0.4673
[Enhanced Epoch 13] Loss: 0.4369
[Enhanced Epoch 14] Loss: 0.4149
[Enhanced Epoch 15] Loss: 0.3900
[Enhanced Epoch 16] Loss: 0.3679
[Enhanced Epoch 17] Loss: 0.3475
[Enhanced Epoch 18] Loss: 0.3275
[Enhanced Epoch 19] Loss: 0.3119
[Enhanced Epoch 20] Loss: 0.2947
[Enhanced Epoch 21] Loss: 0.2846
[Enhanced Epoch 22] Loss: 0.2682
[Enhanced Epoch 23] Loss: 0.2571
[Enhanced Epoch 24] Loss: 0.2457
[Enhanced Epoch 25] Loss: 0.2365
[Enhanced Epoch 26] Loss: 0.2269
[Enhanced Epoch 27] Loss: 0.2182
[Enhanced Epoch 28] Loss: 0.2093
[Enhanced Epoch 29] Loss: 0.2016
[Enhanced Epoch 30] Loss: 0.1960
Training baseline adapter transformer model (teacher frozen with adapter) on raw_set...
[Epoch 1] Loss: 2.0114
[Epoch 2] Loss: 1.5808
[Epoch 3] Loss: 1.4730
[Epoch 4] Loss: 1.3903
[Epoch 5] Loss: 1.3187
[Epoch 6] Loss: 1.2531
[Epoch 7] Loss: 1.1851
[Epoch 8] Loss: 1.1533
[Epoch 9] Loss: 1.0836
[Epoch 10] Loss: 1.0071
[Epoch 11] Loss: 0.9793
[Epoch 12] Loss: 0.8974
[Epoch 13] Loss: 0.8421
[Epoch 14] Loss: 0.8051
[Epoch 15] Loss: 0.7549
[Epoch 16] Loss: 0.7032
[Epoch 17] Loss: 0.6578
[Epoch 18] Loss: 0.6045
[Epoch 19] Loss: 0.5834
[Epoch 20] Loss: 0.5689
[Epoch 21] Loss: 0.5255
[Epoch 22] Loss: 0.5330
[Epoch 23] Loss: 0.4925
[Epoch 24] Loss: 0.4619
[Epoch 25] Loss: 0.4204
[Epoch 26] Loss: 0.4147
[Epoch 27] Loss: 0.3997
[Epoch 28] Loss: 0.3872
[Epoch 29] Loss: 0.3705
[Epoch 30] Loss: 0.3470

[Run 1 Results]
Baseline:           Acc=43.66% | AUC=0.8598 | F1=0.4363 | MinCAcc=20.60%
Linear Probe:       Acc=47.12% | AUC=0.8654 | F1=0.4645 | MinCAcc=26.30%
Enhanced (Concat):  Acc=45.42% | AUC=0.8525 | F1=0.4526 | MinCAcc=32.00%
Baseline Adapter:   Acc=45.20% | AUC=0.8487 | F1=0.4504 | MinCAcc=29.00%

=== Run 2/5, seed=43 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline transformer model (raw only)...
[Epoch 1] Loss: 2.1598
[Epoch 2] Loss: 2.0302
[Epoch 3] Loss: 1.9727
[Epoch 4] Loss: 1.9236
[Epoch 5] Loss: 1.8879
[Epoch 6] Loss: 1.8294
[Epoch 7] Loss: 1.8043
[Epoch 8] Loss: 1.7604
[Epoch 9] Loss: 1.7460
[Epoch 10] Loss: 1.7006
[Epoch 11] Loss: 1.6785
[Epoch 12] Loss: 1.6519
[Epoch 13] Loss: 1.5928
[Epoch 14] Loss: 1.6012
[Epoch 15] Loss: 1.5581
[Epoch 16] Loss: 1.5399
[Epoch 17] Loss: 1.5136
[Epoch 18] Loss: 1.4816
[Epoch 19] Loss: 1.4620
[Epoch 20] Loss: 1.4296
[Epoch 21] Loss: 1.4229
[Epoch 22] Loss: 1.3946
[Epoch 23] Loss: 1.3831
[Epoch 24] Loss: 1.3644
[Epoch 25] Loss: 1.3335
[Epoch 26] Loss: 1.3166
[Epoch 27] Loss: 1.2939
[Epoch 28] Loss: 1.2659
[Epoch 29] Loss: 1.2486
[Epoch 30] Loss: 1.2769
Training linear probe transformer model (teacher fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.6784
[Linear Prob Epoch 2] Loss: 1.4070
[Linear Prob Epoch 3] Loss: 1.3298
[Linear Prob Epoch 4] Loss: 1.2752
[Linear Prob Epoch 5] Loss: 1.2361
[Linear Prob Epoch 6] Loss: 1.2132
[Linear Prob Epoch 7] Loss: 1.1905
[Linear Prob Epoch 8] Loss: 1.1404
[Linear Prob Epoch 9] Loss: 1.1522
[Linear Prob Epoch 10] Loss: 1.1314
[Linear Prob Epoch 11] Loss: 1.1357
[Linear Prob Epoch 12] Loss: 1.1290
[Linear Prob Epoch 13] Loss: 1.1274
[Linear Prob Epoch 14] Loss: 1.0874
[Linear Prob Epoch 15] Loss: 1.0742
[Linear Prob Epoch 16] Loss: 1.0544
[Linear Prob Epoch 17] Loss: 1.0704
[Linear Prob Epoch 18] Loss: 1.0586
[Linear Prob Epoch 19] Loss: 1.0505
[Linear Prob Epoch 20] Loss: 1.0607
[Linear Prob Epoch 21] Loss: 1.0171
[Linear Prob Epoch 22] Loss: 1.0306
[Linear Prob Epoch 23] Loss: 1.0223
[Linear Prob Epoch 24] Loss: 1.0229
[Linear Prob Epoch 25] Loss: 1.0434
[Linear Prob Epoch 26] Loss: 0.9953
[Linear Prob Epoch 27] Loss: 0.9999
[Linear Prob Epoch 28] Loss: 1.0202
[Linear Prob Epoch 29] Loss: 0.9885
[Linear Prob Epoch 30] Loss: 1.0044
Training enhanced transformer model (concatenation)...
[Enhanced Epoch 1] Loss: 1.6288
[Enhanced Epoch 2] Loss: 1.2141
[Enhanced Epoch 3] Loss: 1.0445
[Enhanced Epoch 4] Loss: 0.9232
[Enhanced Epoch 5] Loss: 0.8242
[Enhanced Epoch 6] Loss: 0.7516
[Enhanced Epoch 7] Loss: 0.6858
[Enhanced Epoch 8] Loss: 0.6275
[Enhanced Epoch 9] Loss: 0.5807
[Enhanced Epoch 10] Loss: 0.5424
[Enhanced Epoch 11] Loss: 0.5004
[Enhanced Epoch 12] Loss: 0.4675
[Enhanced Epoch 13] Loss: 0.4416
[Enhanced Epoch 14] Loss: 0.4114
[Enhanced Epoch 15] Loss: 0.3885
[Enhanced Epoch 16] Loss: 0.3675
[Enhanced Epoch 17] Loss: 0.3501
[Enhanced Epoch 18] Loss: 0.3303
[Enhanced Epoch 19] Loss: 0.3143
[Enhanced Epoch 20] Loss: 0.2996
[Enhanced Epoch 21] Loss: 0.2838
[Enhanced Epoch 22] Loss: 0.2724
[Enhanced Epoch 23] Loss: 0.2603
[Enhanced Epoch 24] Loss: 0.2472
[Enhanced Epoch 25] Loss: 0.2378
[Enhanced Epoch 26] Loss: 0.2276
[Enhanced Epoch 27] Loss: 0.2196
[Enhanced Epoch 28] Loss: 0.2104
[Enhanced Epoch 29] Loss: 0.2043
[Enhanced Epoch 30] Loss: 0.1964
Training baseline adapter transformer model (teacher frozen with adapter) on raw_set...
[Epoch 1] Loss: 2.0150
[Epoch 2] Loss: 1.5831
[Epoch 3] Loss: 1.5067
[Epoch 4] Loss: 1.3763
[Epoch 5] Loss: 1.3361
[Epoch 6] Loss: 1.2431
[Epoch 7] Loss: 1.1816
[Epoch 8] Loss: 1.1376
[Epoch 9] Loss: 1.0723
[Epoch 10] Loss: 1.0299
[Epoch 11] Loss: 0.9535
[Epoch 12] Loss: 0.9231
[Epoch 13] Loss: 0.8589
[Epoch 14] Loss: 0.7921
[Epoch 15] Loss: 0.7915
[Epoch 16] Loss: 0.7466
[Epoch 17] Loss: 0.6839
[Epoch 18] Loss: 0.6152
[Epoch 19] Loss: 0.6250
[Epoch 20] Loss: 0.5828
[Epoch 21] Loss: 0.5604
[Epoch 22] Loss: 0.5210
[Epoch 23] Loss: 0.4880
[Epoch 24] Loss: 0.4424
[Epoch 25] Loss: 0.4520
[Epoch 26] Loss: 0.4435
[Epoch 27] Loss: 0.3985
[Epoch 28] Loss: 0.3845
[Epoch 29] Loss: 0.3531
[Epoch 30] Loss: 0.3898

[Run 2 Results]
Baseline:           Acc=44.01% | AUC=0.8631 | F1=0.4370 | MinCAcc=28.50%
Linear Probe:       Acc=45.20% | AUC=0.8641 | F1=0.4429 | MinCAcc=16.10%
Enhanced (Concat):  Acc=44.41% | AUC=0.8525 | F1=0.4436 | MinCAcc=30.70%
Baseline Adapter:   Acc=44.69% | AUC=0.8532 | F1=0.4446 | MinCAcc=28.80%

=== Run 3/5, seed=44 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline transformer model (raw only)...
[Epoch 1] Loss: 2.1619
[Epoch 2] Loss: 2.0466
[Epoch 3] Loss: 1.9997
[Epoch 4] Loss: 1.9529
[Epoch 5] Loss: 1.8849
[Epoch 6] Loss: 1.8310
[Epoch 7] Loss: 1.8102
[Epoch 8] Loss: 1.7908
[Epoch 9] Loss: 1.7474
[Epoch 10] Loss: 1.7248
[Epoch 11] Loss: 1.6852
[Epoch 12] Loss: 1.6503
[Epoch 13] Loss: 1.6526
[Epoch 14] Loss: 1.6227
[Epoch 15] Loss: 1.6109
[Epoch 16] Loss: 1.5661
[Epoch 17] Loss: 1.5582
[Epoch 18] Loss: 1.5142
[Epoch 19] Loss: 1.5032
[Epoch 20] Loss: 1.4715
[Epoch 21] Loss: 1.4527
[Epoch 22] Loss: 1.4379
[Epoch 23] Loss: 1.4300
[Epoch 24] Loss: 1.3968
[Epoch 25] Loss: 1.3691
[Epoch 26] Loss: 1.3397
[Epoch 27] Loss: 1.3242
[Epoch 28] Loss: 1.3117
[Epoch 29] Loss: 1.2874
[Epoch 30] Loss: 1.2480
Training linear probe transformer model (teacher fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.7007
[Linear Prob Epoch 2] Loss: 1.4475
[Linear Prob Epoch 3] Loss: 1.3716
[Linear Prob Epoch 4] Loss: 1.3000
[Linear Prob Epoch 5] Loss: 1.2798
[Linear Prob Epoch 6] Loss: 1.2332
[Linear Prob Epoch 7] Loss: 1.2261
[Linear Prob Epoch 8] Loss: 1.1965
[Linear Prob Epoch 9] Loss: 1.1958
[Linear Prob Epoch 10] Loss: 1.1647
[Linear Prob Epoch 11] Loss: 1.1341
[Linear Prob Epoch 12] Loss: 1.1239
[Linear Prob Epoch 13] Loss: 1.1346
[Linear Prob Epoch 14] Loss: 1.1377
[Linear Prob Epoch 15] Loss: 1.1180
[Linear Prob Epoch 16] Loss: 1.0681
[Linear Prob Epoch 17] Loss: 1.0934
[Linear Prob Epoch 18] Loss: 1.1037
[Linear Prob Epoch 19] Loss: 1.0618
[Linear Prob Epoch 20] Loss: 1.0737
[Linear Prob Epoch 21] Loss: 1.0565
[Linear Prob Epoch 22] Loss: 1.0439
[Linear Prob Epoch 23] Loss: 1.0116
[Linear Prob Epoch 24] Loss: 1.0459
[Linear Prob Epoch 25] Loss: 1.0484
[Linear Prob Epoch 26] Loss: 1.0578
[Linear Prob Epoch 27] Loss: 1.0174
[Linear Prob Epoch 28] Loss: 1.0349
[Linear Prob Epoch 29] Loss: 1.0318
[Linear Prob Epoch 30] Loss: 1.0219
Training enhanced transformer model (concatenation)...
[Enhanced Epoch 1] Loss: 1.6511
[Enhanced Epoch 2] Loss: 1.2467
[Enhanced Epoch 3] Loss: 1.0713
[Enhanced Epoch 4] Loss: 0.9484
[Enhanced Epoch 5] Loss: 0.8532
[Enhanced Epoch 6] Loss: 0.7699
[Enhanced Epoch 7] Loss: 0.7098
[Enhanced Epoch 8] Loss: 0.6539
[Enhanced Epoch 9] Loss: 0.6010
[Enhanced Epoch 10] Loss: 0.5582
[Enhanced Epoch 11] Loss: 0.5241
[Enhanced Epoch 12] Loss: 0.4848
[Enhanced Epoch 13] Loss: 0.4557
[Enhanced Epoch 14] Loss: 0.4322
[Enhanced Epoch 15] Loss: 0.4047
[Enhanced Epoch 16] Loss: 0.3812
[Enhanced Epoch 17] Loss: 0.3634
[Enhanced Epoch 18] Loss: 0.3457
[Enhanced Epoch 19] Loss: 0.3275
[Enhanced Epoch 20] Loss: 0.3127
[Enhanced Epoch 21] Loss: 0.2978
[Enhanced Epoch 22] Loss: 0.2835
[Enhanced Epoch 23] Loss: 0.2708
[Enhanced Epoch 24] Loss: 0.2598
[Enhanced Epoch 25] Loss: 0.2487
[Enhanced Epoch 26] Loss: 0.2380
[Enhanced Epoch 27] Loss: 0.2301
[Enhanced Epoch 28] Loss: 0.2208
[Enhanced Epoch 29] Loss: 0.2141
[Enhanced Epoch 30] Loss: 0.2045
Training baseline adapter transformer model (teacher frozen with adapter) on raw_set...
[Epoch 1] Loss: 2.0129
[Epoch 2] Loss: 1.6178
[Epoch 3] Loss: 1.4952
[Epoch 4] Loss: 1.4107
[Epoch 5] Loss: 1.3396
[Epoch 6] Loss: 1.2953
[Epoch 7] Loss: 1.2348
[Epoch 8] Loss: 1.1635
[Epoch 9] Loss: 1.0982
[Epoch 10] Loss: 1.0301
[Epoch 11] Loss: 0.9709
[Epoch 12] Loss: 0.9256
[Epoch 13] Loss: 0.8481
[Epoch 14] Loss: 0.8457
[Epoch 15] Loss: 0.7943
[Epoch 16] Loss: 0.7330
[Epoch 17] Loss: 0.6889
[Epoch 18] Loss: 0.6311
[Epoch 19] Loss: 0.5936
[Epoch 20] Loss: 0.5723
[Epoch 21] Loss: 0.5253
[Epoch 22] Loss: 0.5006
[Epoch 23] Loss: 0.5183
[Epoch 24] Loss: 0.4602
[Epoch 25] Loss: 0.4368
[Epoch 26] Loss: 0.4202
[Epoch 27] Loss: 0.4258
[Epoch 28] Loss: 0.3931
[Epoch 29] Loss: 0.3566
[Epoch 30] Loss: 0.3532

[Run 3 Results]
Baseline:           Acc=45.47% | AUC=0.8648 | F1=0.4450 | MinCAcc=13.20%
Linear Probe:       Acc=45.81% | AUC=0.8617 | F1=0.4545 | MinCAcc=21.80%
Enhanced (Concat):  Acc=44.46% | AUC=0.8506 | F1=0.4439 | MinCAcc=27.80%
Baseline Adapter:   Acc=44.00% | AUC=0.8484 | F1=0.4348 | MinCAcc=24.20%

=== Run 4/5, seed=45 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline transformer model (raw only)...
[Epoch 1] Loss: 2.1595
[Epoch 2] Loss: 2.0396
[Epoch 3] Loss: 1.9876
[Epoch 4] Loss: 1.9455
[Epoch 5] Loss: 1.8939
[Epoch 6] Loss: 1.8485
[Epoch 7] Loss: 1.8197
[Epoch 8] Loss: 1.7824
[Epoch 9] Loss: 1.7356
[Epoch 10] Loss: 1.7119
[Epoch 11] Loss: 1.6849
[Epoch 12] Loss: 1.6452
[Epoch 13] Loss: 1.6396
[Epoch 14] Loss: 1.6147
[Epoch 15] Loss: 1.5896
[Epoch 16] Loss: 1.5631
[Epoch 17] Loss: 1.5326
[Epoch 18] Loss: 1.5199
[Epoch 19] Loss: 1.4779
[Epoch 20] Loss: 1.4641
[Epoch 21] Loss: 1.4542
[Epoch 22] Loss: 1.4258
[Epoch 23] Loss: 1.4258
[Epoch 24] Loss: 1.3757
[Epoch 25] Loss: 1.3290
[Epoch 26] Loss: 1.3281
[Epoch 27] Loss: 1.3137
[Epoch 28] Loss: 1.3036
[Epoch 29] Loss: 1.2578
[Epoch 30] Loss: 1.2440
Training linear probe transformer model (teacher fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.7067
[Linear Prob Epoch 2] Loss: 1.4298
[Linear Prob Epoch 3] Loss: 1.3564
[Linear Prob Epoch 4] Loss: 1.3130
[Linear Prob Epoch 5] Loss: 1.2718
[Linear Prob Epoch 6] Loss: 1.2415
[Linear Prob Epoch 7] Loss: 1.1906
[Linear Prob Epoch 8] Loss: 1.1724
[Linear Prob Epoch 9] Loss: 1.1611
[Linear Prob Epoch 10] Loss: 1.1546
[Linear Prob Epoch 11] Loss: 1.1472
[Linear Prob Epoch 12] Loss: 1.1299
[Linear Prob Epoch 13] Loss: 1.0929
[Linear Prob Epoch 14] Loss: 1.0945
[Linear Prob Epoch 15] Loss: 1.1231
[Linear Prob Epoch 16] Loss: 1.0999
[Linear Prob Epoch 17] Loss: 1.0754
[Linear Prob Epoch 18] Loss: 1.0922
[Linear Prob Epoch 19] Loss: 1.0508
[Linear Prob Epoch 20] Loss: 1.0621
[Linear Prob Epoch 21] Loss: 1.0573
[Linear Prob Epoch 22] Loss: 1.0347
[Linear Prob Epoch 23] Loss: 1.0336
[Linear Prob Epoch 24] Loss: 1.0201
[Linear Prob Epoch 25] Loss: 1.0242
[Linear Prob Epoch 26] Loss: 1.0160
[Linear Prob Epoch 27] Loss: 1.0123
[Linear Prob Epoch 28] Loss: 1.0053
[Linear Prob Epoch 29] Loss: 1.0151
[Linear Prob Epoch 30] Loss: 1.0198
Training enhanced transformer model (concatenation)...
[Enhanced Epoch 1] Loss: 1.6457
[Enhanced Epoch 2] Loss: 1.2434
[Enhanced Epoch 3] Loss: 1.0666
[Enhanced Epoch 4] Loss: 0.9474
[Enhanced Epoch 5] Loss: 0.8460
[Enhanced Epoch 6] Loss: 0.7700
[Enhanced Epoch 7] Loss: 0.7057
[Enhanced Epoch 8] Loss: 0.6454
[Enhanced Epoch 9] Loss: 0.5961
[Enhanced Epoch 10] Loss: 0.5537
[Enhanced Epoch 11] Loss: 0.5189
[Enhanced Epoch 12] Loss: 0.4883
[Enhanced Epoch 13] Loss: 0.4550
[Enhanced Epoch 14] Loss: 0.4249
[Enhanced Epoch 15] Loss: 0.3989
[Enhanced Epoch 16] Loss: 0.3808
[Enhanced Epoch 17] Loss: 0.3593
[Enhanced Epoch 18] Loss: 0.3435
[Enhanced Epoch 19] Loss: 0.3254
[Enhanced Epoch 20] Loss: 0.3075
[Enhanced Epoch 21] Loss: 0.2936
[Enhanced Epoch 22] Loss: 0.2834
[Enhanced Epoch 23] Loss: 0.2693
[Enhanced Epoch 24] Loss: 0.2591
[Enhanced Epoch 25] Loss: 0.2478
[Enhanced Epoch 26] Loss: 0.2388
[Enhanced Epoch 27] Loss: 0.2290
[Enhanced Epoch 28] Loss: 0.2202
[Enhanced Epoch 29] Loss: 0.2100
[Enhanced Epoch 30] Loss: 0.2042
Training baseline adapter transformer model (teacher frozen with adapter) on raw_set...
[Epoch 1] Loss: 2.0107
[Epoch 2] Loss: 1.6158
[Epoch 3] Loss: 1.4962
[Epoch 4] Loss: 1.4225
[Epoch 5] Loss: 1.3336
[Epoch 6] Loss: 1.2927
[Epoch 7] Loss: 1.1936
[Epoch 8] Loss: 1.1434
[Epoch 9] Loss: 1.1132
[Epoch 10] Loss: 1.0394
[Epoch 11] Loss: 1.0015
[Epoch 12] Loss: 0.9127
[Epoch 13] Loss: 0.8771
[Epoch 14] Loss: 0.8298
[Epoch 15] Loss: 0.7773
[Epoch 16] Loss: 0.7482
[Epoch 17] Loss: 0.6893
[Epoch 18] Loss: 0.6544
[Epoch 19] Loss: 0.6209
[Epoch 20] Loss: 0.5495
[Epoch 21] Loss: 0.5492
[Epoch 22] Loss: 0.5224
[Epoch 23] Loss: 0.5175
[Epoch 24] Loss: 0.4668
[Epoch 25] Loss: 0.4600
[Epoch 26] Loss: 0.4238
[Epoch 27] Loss: 0.4256
[Epoch 28] Loss: 0.4101
[Epoch 29] Loss: 0.3777
[Epoch 30] Loss: 0.3673

[Run 4 Results]
Baseline:           Acc=45.02% | AUC=0.8678 | F1=0.4408 | MinCAcc=20.10%
Linear Probe:       Acc=46.58% | AUC=0.8661 | F1=0.4616 | MinCAcc=30.30%
Enhanced (Concat):  Acc=45.06% | AUC=0.8540 | F1=0.4497 | MinCAcc=30.50%
Baseline Adapter:   Acc=45.66% | AUC=0.8561 | F1=0.4506 | MinCAcc=26.30%

=== Run 5/5, seed=46 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline transformer model (raw only)...
[Epoch 1] Loss: 2.1405
[Epoch 2] Loss: 2.0347
[Epoch 3] Loss: 1.9610
[Epoch 4] Loss: 1.9114
[Epoch 5] Loss: 1.8610
[Epoch 6] Loss: 1.8520
[Epoch 7] Loss: 1.8006
[Epoch 8] Loss: 1.7662
[Epoch 9] Loss: 1.7416
[Epoch 10] Loss: 1.6892
[Epoch 11] Loss: 1.6826
[Epoch 12] Loss: 1.6357
[Epoch 13] Loss: 1.6174
[Epoch 14] Loss: 1.5914
[Epoch 15] Loss: 1.5788
[Epoch 16] Loss: 1.5496
[Epoch 17] Loss: 1.5454
[Epoch 18] Loss: 1.4972
[Epoch 19] Loss: 1.4601
[Epoch 20] Loss: 1.4817
[Epoch 21] Loss: 1.4458
[Epoch 22] Loss: 1.4142
[Epoch 23] Loss: 1.3923
[Epoch 24] Loss: 1.3756
[Epoch 25] Loss: 1.3560
[Epoch 26] Loss: 1.3458
[Epoch 27] Loss: 1.3105
[Epoch 28] Loss: 1.2977
[Epoch 29] Loss: 1.2559
[Epoch 30] Loss: 1.2542
Training linear probe transformer model (teacher fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.6920
[Linear Prob Epoch 2] Loss: 1.4550
[Linear Prob Epoch 3] Loss: 1.3621
[Linear Prob Epoch 4] Loss: 1.3065
[Linear Prob Epoch 5] Loss: 1.2540
[Linear Prob Epoch 6] Loss: 1.2243
[Linear Prob Epoch 7] Loss: 1.1805
[Linear Prob Epoch 8] Loss: 1.1808
[Linear Prob Epoch 9] Loss: 1.1352
[Linear Prob Epoch 10] Loss: 1.1386
[Linear Prob Epoch 11] Loss: 1.1172
[Linear Prob Epoch 12] Loss: 1.1120
[Linear Prob Epoch 13] Loss: 1.1174
[Linear Prob Epoch 14] Loss: 1.0906
[Linear Prob Epoch 15] Loss: 1.0960
[Linear Prob Epoch 16] Loss: 1.0926
[Linear Prob Epoch 17] Loss: 1.0677
[Linear Prob Epoch 18] Loss: 1.0542
[Linear Prob Epoch 19] Loss: 1.0557
[Linear Prob Epoch 20] Loss: 1.0856
[Linear Prob Epoch 21] Loss: 1.0469
[Linear Prob Epoch 22] Loss: 1.0490
[Linear Prob Epoch 23] Loss: 1.0288
[Linear Prob Epoch 24] Loss: 1.0453
[Linear Prob Epoch 25] Loss: 1.0528
[Linear Prob Epoch 26] Loss: 1.0266
[Linear Prob Epoch 27] Loss: 1.0246
[Linear Prob Epoch 28] Loss: 1.0223
[Linear Prob Epoch 29] Loss: 1.0341
[Linear Prob Epoch 30] Loss: 1.0309
Training enhanced transformer model (concatenation)...
[Enhanced Epoch 1] Loss: 1.6386
[Enhanced Epoch 2] Loss: 1.2293
[Enhanced Epoch 3] Loss: 1.0634
[Enhanced Epoch 4] Loss: 0.9375
[Enhanced Epoch 5] Loss: 0.8417
[Enhanced Epoch 6] Loss: 0.7647
[Enhanced Epoch 7] Loss: 0.7053
[Enhanced Epoch 8] Loss: 0.6493
[Enhanced Epoch 9] Loss: 0.5980
[Enhanced Epoch 10] Loss: 0.5548
[Enhanced Epoch 11] Loss: 0.5174
[Enhanced Epoch 12] Loss: 0.4882
[Enhanced Epoch 13] Loss: 0.4589
[Enhanced Epoch 14] Loss: 0.4314
[Enhanced Epoch 15] Loss: 0.4072
[Enhanced Epoch 16] Loss: 0.3828
[Enhanced Epoch 17] Loss: 0.3638
[Enhanced Epoch 18] Loss: 0.3442
[Enhanced Epoch 19] Loss: 0.3307
[Enhanced Epoch 20] Loss: 0.3123
[Enhanced Epoch 21] Loss: 0.2973
[Enhanced Epoch 22] Loss: 0.2858
[Enhanced Epoch 23] Loss: 0.2714
[Enhanced Epoch 24] Loss: 0.2604
[Enhanced Epoch 25] Loss: 0.2511
[Enhanced Epoch 26] Loss: 0.2409
[Enhanced Epoch 27] Loss: 0.2313
[Enhanced Epoch 28] Loss: 0.2218
[Enhanced Epoch 29] Loss: 0.2141
[Enhanced Epoch 30] Loss: 0.2072
Training baseline adapter transformer model (teacher frozen with adapter) on raw_set...
[Epoch 1] Loss: 2.0253
[Epoch 2] Loss: 1.5911
[Epoch 3] Loss: 1.4916
[Epoch 4] Loss: 1.4107
[Epoch 5] Loss: 1.3277
[Epoch 6] Loss: 1.2763
[Epoch 7] Loss: 1.2168
[Epoch 8] Loss: 1.1501
[Epoch 9] Loss: 1.1024
[Epoch 10] Loss: 1.0511
[Epoch 11] Loss: 0.9824
[Epoch 12] Loss: 0.8976
[Epoch 13] Loss: 0.8888
[Epoch 14] Loss: 0.8114
[Epoch 15] Loss: 0.7910
[Epoch 16] Loss: 0.7291
[Epoch 17] Loss: 0.6622
[Epoch 18] Loss: 0.6396
[Epoch 19] Loss: 0.5994
[Epoch 20] Loss: 0.6033
[Epoch 21] Loss: 0.5448
[Epoch 22] Loss: 0.5351
[Epoch 23] Loss: 0.4973
[Epoch 24] Loss: 0.4596
[Epoch 25] Loss: 0.4458
[Epoch 26] Loss: 0.4099
[Epoch 27] Loss: 0.4212
[Epoch 28] Loss: 0.4041
[Epoch 29] Loss: 0.3905
[Epoch 30] Loss: 0.3557

[Run 5 Results]
Baseline:           Acc=43.71% | AUC=0.8583 | F1=0.4283 | MinCAcc=21.60%
Linear Probe:       Acc=45.51% | AUC=0.8641 | F1=0.4485 | MinCAcc=22.80%
Enhanced (Concat):  Acc=44.91% | AUC=0.8522 | F1=0.4470 | MinCAcc=27.40%
Baseline Adapter:   Acc=44.78% | AUC=0.8504 | F1=0.4419 | MinCAcc=25.40%

All done. Final mean/std results saved to: ./results/mismatch_tf.json
mismatch_tf.py completed successfully.
Starting imb_tf.py...
Files already downloaded and verified
Files already downloaded and verified

=== Pretraining External Model (BigTransformer) on 10k Imbalanced Samples ===
[Epoch 1] Loss: 1.5813
[Epoch 2] Loss: 1.4283
[Epoch 3] Loss: 1.3256
[Epoch 4] Loss: 1.2696
[Epoch 5] Loss: 1.1647
[Epoch 6] Loss: 1.1070
[Epoch 7] Loss: 1.0541
[Epoch 8] Loss: 0.9865
[Epoch 9] Loss: 0.9507
[Epoch 10] Loss: 0.9067
[Epoch 11] Loss: 0.8665
[Epoch 12] Loss: 0.8348
[Epoch 13] Loss: 0.8045
[Epoch 14] Loss: 0.7586
[Epoch 15] Loss: 0.7169
[Epoch 16] Loss: 0.6906
[Epoch 17] Loss: 0.6513
[Epoch 18] Loss: 0.6429
[Epoch 19] Loss: 0.6096
[Epoch 20] Loss: 0.5848
[Epoch 21] Loss: 0.5507
[Epoch 22] Loss: 0.5417
[Epoch 23] Loss: 0.5210
[Epoch 24] Loss: 0.4933
[Epoch 25] Loss: 0.4797
[Epoch 26] Loss: 0.4777
[Epoch 27] Loss: 0.4518
[Epoch 28] Loss: 0.4140
[Epoch 29] Loss: 0.3898
[Epoch 30] Loss: 0.3755
[Epoch 31] Loss: 0.3511
[Epoch 32] Loss: 0.3215
[Epoch 33] Loss: 0.3306
[Epoch 34] Loss: 0.3433
[Epoch 35] Loss: 0.3170
[Epoch 36] Loss: 0.3067
[Epoch 37] Loss: 0.2741
[Epoch 38] Loss: 0.2453
[Epoch 39] Loss: 0.2419
[Epoch 40] Loss: 0.2372
[Epoch 41] Loss: 0.2528
[Epoch 42] Loss: 0.2166
[Epoch 43] Loss: 0.2059
[Epoch 44] Loss: 0.1785
[Epoch 45] Loss: 0.2288
[Epoch 46] Loss: 0.1878
[Epoch 47] Loss: 0.1706
[Epoch 48] Loss: 0.1748
[Epoch 49] Loss: 0.1602
[Epoch 50] Loss: 0.1674
[Epoch 51] Loss: 0.1590
[Epoch 52] Loss: 0.1571
[Epoch 53] Loss: 0.1142
[Epoch 54] Loss: 0.1514
[Epoch 55] Loss: 0.1819
[Epoch 56] Loss: 0.1103
[Epoch 57] Loss: 0.1386
[Epoch 58] Loss: 0.0845
[Epoch 59] Loss: 0.1058
[Epoch 60] Loss: 0.1021
External Model Evaluation: Acc=41.74%, AUC=0.8182, F1=0.3973, MinCAcc=6.20%

=== Run 1/5, raw-set seed=42 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw set)...
[Epoch 1] Loss: 2.1448
[Epoch 2] Loss: 2.0497
[Epoch 3] Loss: 1.9840
[Epoch 4] Loss: 1.9783
[Epoch 5] Loss: 1.9177
[Epoch 6] Loss: 1.8781
[Epoch 7] Loss: 1.8405
[Epoch 8] Loss: 1.7961
[Epoch 9] Loss: 1.7545
[Epoch 10] Loss: 1.7349
[Epoch 11] Loss: 1.6968
[Epoch 12] Loss: 1.6948
[Epoch 13] Loss: 1.6536
[Epoch 14] Loss: 1.6420
[Epoch 15] Loss: 1.6032
[Epoch 16] Loss: 1.6003
[Epoch 17] Loss: 1.5603
[Epoch 18] Loss: 1.5551
[Epoch 19] Loss: 1.5366
[Epoch 20] Loss: 1.5108
[Epoch 21] Loss: 1.4888
[Epoch 22] Loss: 1.4623
[Epoch 23] Loss: 1.4949
[Epoch 24] Loss: 1.4261
[Epoch 25] Loss: 1.4009
[Epoch 26] Loss: 1.3660
[Epoch 27] Loss: 1.3654
[Epoch 28] Loss: 1.3491
[Epoch 29] Loss: 1.3100
[Epoch 30] Loss: 1.2773
Training linear probe model (fine-tuning external model's final classifier)...
[Linear Prob Epoch 1] Loss: 2.2953
[Linear Prob Epoch 2] Loss: 1.6217
[Linear Prob Epoch 3] Loss: 1.4814
[Linear Prob Epoch 4] Loss: 1.4179
[Linear Prob Epoch 5] Loss: 1.3628
[Linear Prob Epoch 6] Loss: 1.3204
[Linear Prob Epoch 7] Loss: 1.3085
[Linear Prob Epoch 8] Loss: 1.2752
[Linear Prob Epoch 9] Loss: 1.2452
[Linear Prob Epoch 10] Loss: 1.2425
[Linear Prob Epoch 11] Loss: 1.2457
[Linear Prob Epoch 12] Loss: 1.2125
[Linear Prob Epoch 13] Loss: 1.1962
[Linear Prob Epoch 14] Loss: 1.1870
[Linear Prob Epoch 15] Loss: 1.1871
[Linear Prob Epoch 16] Loss: 1.1658
[Linear Prob Epoch 17] Loss: 1.1640
[Linear Prob Epoch 18] Loss: 1.1508
[Linear Prob Epoch 19] Loss: 1.1583
[Linear Prob Epoch 20] Loss: 1.1312
[Linear Prob Epoch 21] Loss: 1.1518
[Linear Prob Epoch 22] Loss: 1.1302
[Linear Prob Epoch 23] Loss: 1.1130
[Linear Prob Epoch 24] Loss: 1.1200
[Linear Prob Epoch 25] Loss: 1.1282
[Linear Prob Epoch 26] Loss: 1.1262
[Linear Prob Epoch 27] Loss: 1.1129
[Linear Prob Epoch 28] Loss: 1.0835
[Linear Prob Epoch 29] Loss: 1.0874
[Linear Prob Epoch 30] Loss: 1.0921
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5813
[Enhanced Epoch 2] Loss: 1.3403
[Enhanced Epoch 3] Loss: 1.2563
[Enhanced Epoch 4] Loss: 1.2087
[Enhanced Epoch 5] Loss: 1.1646
[Enhanced Epoch 6] Loss: 1.1181
[Enhanced Epoch 7] Loss: 1.0865
[Enhanced Epoch 8] Loss: 1.0484
[Enhanced Epoch 9] Loss: 1.0146
[Enhanced Epoch 10] Loss: 0.9847
[Enhanced Epoch 11] Loss: 0.9614
[Enhanced Epoch 12] Loss: 0.9349
[Enhanced Epoch 13] Loss: 0.9103
[Enhanced Epoch 14] Loss: 0.8909
[Enhanced Epoch 15] Loss: 0.8715
[Enhanced Epoch 16] Loss: 0.8557
[Enhanced Epoch 17] Loss: 0.8282
[Enhanced Epoch 18] Loss: 0.8101
[Enhanced Epoch 19] Loss: 0.7964
[Enhanced Epoch 20] Loss: 0.7819
[Enhanced Epoch 21] Loss: 0.7714
[Enhanced Epoch 22] Loss: 0.7535
[Enhanced Epoch 23] Loss: 0.7418
[Enhanced Epoch 24] Loss: 0.7232
[Enhanced Epoch 25] Loss: 0.7079
[Enhanced Epoch 26] Loss: 0.6968
[Enhanced Epoch 27] Loss: 0.6829
[Enhanced Epoch 28] Loss: 0.6759
[Enhanced Epoch 29] Loss: 0.6600
[Enhanced Epoch 30] Loss: 0.6519
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.0293
[Epoch 2] Loss: 1.5323
[Epoch 3] Loss: 1.4594
[Epoch 4] Loss: 1.4208
[Epoch 5] Loss: 1.3862
[Epoch 6] Loss: 1.3615
[Epoch 7] Loss: 1.3385
[Epoch 8] Loss: 1.3098
[Epoch 9] Loss: 1.2941
[Epoch 10] Loss: 1.2760
[Epoch 11] Loss: 1.2698
[Epoch 12] Loss: 1.2514
[Epoch 13] Loss: 1.2171
[Epoch 14] Loss: 1.2054
[Epoch 15] Loss: 1.2034
[Epoch 16] Loss: 1.1652
[Epoch 17] Loss: 1.1540
[Epoch 18] Loss: 1.1318
[Epoch 19] Loss: 1.1051
[Epoch 20] Loss: 1.0907
[Epoch 21] Loss: 1.0598
[Epoch 22] Loss: 1.0696
[Epoch 23] Loss: 1.0480
[Epoch 24] Loss: 1.0210
[Epoch 25] Loss: 0.9907
[Epoch 26] Loss: 0.9650
[Epoch 27] Loss: 0.9481
[Epoch 28] Loss: 0.9392
[Epoch 29] Loss: 0.9249
[Epoch 30] Loss: 0.9015
Training knowledge distillation model (Transformer student with teacher external)...
[Distillation Epoch 1] Loss: 3.5440
[Distillation Epoch 2] Loss: 3.2906
[Distillation Epoch 3] Loss: 3.1131
[Distillation Epoch 4] Loss: 2.9919
[Distillation Epoch 5] Loss: 2.8698
[Distillation Epoch 6] Loss: 2.7601
[Distillation Epoch 7] Loss: 2.6662
[Distillation Epoch 8] Loss: 2.6150
[Distillation Epoch 9] Loss: 2.5758
[Distillation Epoch 10] Loss: 2.5745
[Distillation Epoch 11] Loss: 2.5072
[Distillation Epoch 12] Loss: 2.4580
[Distillation Epoch 13] Loss: 2.4175
[Distillation Epoch 14] Loss: 2.3629
[Distillation Epoch 15] Loss: 2.2808
[Distillation Epoch 16] Loss: 2.2968
[Distillation Epoch 17] Loss: 2.2775
[Distillation Epoch 18] Loss: 2.2426
[Distillation Epoch 19] Loss: 2.2355
[Distillation Epoch 20] Loss: 2.1923
[Distillation Epoch 21] Loss: 2.1312
[Distillation Epoch 22] Loss: 2.1161
[Distillation Epoch 23] Loss: 2.1243
[Distillation Epoch 24] Loss: 2.0682
[Distillation Epoch 25] Loss: 2.0602
[Distillation Epoch 26] Loss: 2.0627
[Distillation Epoch 27] Loss: 2.0183
[Distillation Epoch 28] Loss: 1.9841
[Distillation Epoch 29] Loss: 1.9579
[Distillation Epoch 30] Loss: 1.9548

[Run 1 Results]
Baseline:         Acc=47.65% | AUC=0.8730 | F1=0.4710 | MinCAcc=22.20%
Linear Probe:     Acc=48.04% | AUC=0.8742 | F1=0.4730 | MinCAcc=15.40%
Enhanced (Concat):Acc=47.42% | AUC=0.8684 | F1=0.4710 | MinCAcc=24.90%
Baseline Adapter: Acc=47.62% | AUC=0.8686 | F1=0.4736 | MinCAcc=26.40%
Distillation:     Acc=41.78% | AUC=0.8711 | F1=0.3950 | MinCAcc=2.80%

=== Run 2/5, raw-set seed=43 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw set)...
[Epoch 1] Loss: 2.1549
[Epoch 2] Loss: 2.0485
[Epoch 3] Loss: 2.0138
[Epoch 4] Loss: 1.9752
[Epoch 5] Loss: 1.9559
[Epoch 6] Loss: 1.9051
[Epoch 7] Loss: 1.8667
[Epoch 8] Loss: 1.8270
[Epoch 9] Loss: 1.7745
[Epoch 10] Loss: 1.7404
[Epoch 11] Loss: 1.7294
[Epoch 12] Loss: 1.7030
[Epoch 13] Loss: 1.6680
[Epoch 14] Loss: 1.6347
[Epoch 15] Loss: 1.6383
[Epoch 16] Loss: 1.5866
[Epoch 17] Loss: 1.5664
[Epoch 18] Loss: 1.5362
[Epoch 19] Loss: 1.5123
[Epoch 20] Loss: 1.5108
[Epoch 21] Loss: 1.4687
[Epoch 22] Loss: 1.4641
[Epoch 23] Loss: 1.4257
[Epoch 24] Loss: 1.3869
[Epoch 25] Loss: 1.3829
[Epoch 26] Loss: 1.3590
[Epoch 27] Loss: 1.3462
[Epoch 28] Loss: 1.3157
[Epoch 29] Loss: 1.3079
[Epoch 30] Loss: 1.2804
Training linear probe model (fine-tuning external model's final classifier)...
[Linear Prob Epoch 1] Loss: 2.1296
[Linear Prob Epoch 2] Loss: 1.5451
[Linear Prob Epoch 3] Loss: 1.3953
[Linear Prob Epoch 4] Loss: 1.3322
[Linear Prob Epoch 5] Loss: 1.2774
[Linear Prob Epoch 6] Loss: 1.2445
[Linear Prob Epoch 7] Loss: 1.2224
[Linear Prob Epoch 8] Loss: 1.1793
[Linear Prob Epoch 9] Loss: 1.1755
[Linear Prob Epoch 10] Loss: 1.1559
[Linear Prob Epoch 11] Loss: 1.1465
[Linear Prob Epoch 12] Loss: 1.1333
[Linear Prob Epoch 13] Loss: 1.1206
[Linear Prob Epoch 14] Loss: 1.1071
[Linear Prob Epoch 15] Loss: 1.1057
[Linear Prob Epoch 16] Loss: 1.0871
[Linear Prob Epoch 17] Loss: 1.0769
[Linear Prob Epoch 18] Loss: 1.0758
[Linear Prob Epoch 19] Loss: 1.0656
[Linear Prob Epoch 20] Loss: 1.0711
[Linear Prob Epoch 21] Loss: 1.0656
[Linear Prob Epoch 22] Loss: 1.0716
[Linear Prob Epoch 23] Loss: 1.0416
[Linear Prob Epoch 24] Loss: 1.0564
[Linear Prob Epoch 25] Loss: 1.0477
[Linear Prob Epoch 26] Loss: 1.0307
[Linear Prob Epoch 27] Loss: 1.0046
[Linear Prob Epoch 28] Loss: 1.0205
[Linear Prob Epoch 29] Loss: 1.0137
[Linear Prob Epoch 30] Loss: 1.0073
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5000
[Enhanced Epoch 2] Loss: 1.2526
[Enhanced Epoch 3] Loss: 1.1871
[Enhanced Epoch 4] Loss: 1.1312
[Enhanced Epoch 5] Loss: 1.0913
[Enhanced Epoch 6] Loss: 1.0566
[Enhanced Epoch 7] Loss: 1.0141
[Enhanced Epoch 8] Loss: 0.9832
[Enhanced Epoch 9] Loss: 0.9581
[Enhanced Epoch 10] Loss: 0.9290
[Enhanced Epoch 11] Loss: 0.8968
[Enhanced Epoch 12] Loss: 0.8768
[Enhanced Epoch 13] Loss: 0.8521
[Enhanced Epoch 14] Loss: 0.8335
[Enhanced Epoch 15] Loss: 0.8171
[Enhanced Epoch 16] Loss: 0.7929
[Enhanced Epoch 17] Loss: 0.7807
[Enhanced Epoch 18] Loss: 0.7660
[Enhanced Epoch 19] Loss: 0.7486
[Enhanced Epoch 20] Loss: 0.7275
[Enhanced Epoch 21] Loss: 0.7151
[Enhanced Epoch 22] Loss: 0.7049
[Enhanced Epoch 23] Loss: 0.6876
[Enhanced Epoch 24] Loss: 0.6735
[Enhanced Epoch 25] Loss: 0.6650
[Enhanced Epoch 26] Loss: 0.6541
[Enhanced Epoch 27] Loss: 0.6418
[Enhanced Epoch 28] Loss: 0.6288
[Enhanced Epoch 29] Loss: 0.6141
[Enhanced Epoch 30] Loss: 0.6089
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 1.9901
[Epoch 2] Loss: 1.4609
[Epoch 3] Loss: 1.3819
[Epoch 4] Loss: 1.3421
[Epoch 5] Loss: 1.2988
[Epoch 6] Loss: 1.2800
[Epoch 7] Loss: 1.2585
[Epoch 8] Loss: 1.2332
[Epoch 9] Loss: 1.2158
[Epoch 10] Loss: 1.2247
[Epoch 11] Loss: 1.1763
[Epoch 12] Loss: 1.1680
[Epoch 13] Loss: 1.1495
[Epoch 14] Loss: 1.1388
[Epoch 15] Loss: 1.1094
[Epoch 16] Loss: 1.0936
[Epoch 17] Loss: 1.0571
[Epoch 18] Loss: 1.0483
[Epoch 19] Loss: 1.0235
[Epoch 20] Loss: 1.0085
[Epoch 21] Loss: 0.9883
[Epoch 22] Loss: 0.9856
[Epoch 23] Loss: 0.9626
[Epoch 24] Loss: 0.9300
[Epoch 25] Loss: 0.9098
[Epoch 26] Loss: 0.9001
[Epoch 27] Loss: 0.8720
[Epoch 28] Loss: 0.8531
[Epoch 29] Loss: 0.8521
[Epoch 30] Loss: 0.8403
Training knowledge distillation model (Transformer student with teacher external)...
[Distillation Epoch 1] Loss: 3.6278
[Distillation Epoch 2] Loss: 3.3893
[Distillation Epoch 3] Loss: 3.2315
[Distillation Epoch 4] Loss: 3.0714
[Distillation Epoch 5] Loss: 2.9796
[Distillation Epoch 6] Loss: 2.8761
[Distillation Epoch 7] Loss: 2.8315
[Distillation Epoch 8] Loss: 2.7347
[Distillation Epoch 9] Loss: 2.6871
[Distillation Epoch 10] Loss: 2.6349
[Distillation Epoch 11] Loss: 2.5557
[Distillation Epoch 12] Loss: 2.5106
[Distillation Epoch 13] Loss: 2.4820
[Distillation Epoch 14] Loss: 2.4252
[Distillation Epoch 15] Loss: 2.3830
[Distillation Epoch 16] Loss: 2.3388
[Distillation Epoch 17] Loss: 2.3527
[Distillation Epoch 18] Loss: 2.2888
[Distillation Epoch 19] Loss: 2.2235
[Distillation Epoch 20] Loss: 2.1970
[Distillation Epoch 21] Loss: 2.2149
[Distillation Epoch 22] Loss: 2.1771
[Distillation Epoch 23] Loss: 2.1531
[Distillation Epoch 24] Loss: 2.1634
[Distillation Epoch 25] Loss: 2.0639
[Distillation Epoch 26] Loss: 2.0897
[Distillation Epoch 27] Loss: 2.0179
[Distillation Epoch 28] Loss: 1.9905
[Distillation Epoch 29] Loss: 2.0016
[Distillation Epoch 30] Loss: 1.9738

[Run 2 Results]
Baseline:         Acc=43.84% | AUC=0.8666 | F1=0.4230 | MinCAcc=10.30%
Linear Probe:     Acc=48.67% | AUC=0.8746 | F1=0.4813 | MinCAcc=20.60%
Enhanced (Concat):Acc=48.03% | AUC=0.8683 | F1=0.4764 | MinCAcc=28.00%
Baseline Adapter: Acc=47.04% | AUC=0.8628 | F1=0.4620 | MinCAcc=17.10%
Distillation:     Acc=41.36% | AUC=0.8609 | F1=0.3959 | MinCAcc=5.30%

=== Run 3/5, raw-set seed=44 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw set)...
[Epoch 1] Loss: 2.1689
[Epoch 2] Loss: 2.0540
[Epoch 3] Loss: 2.0041
[Epoch 4] Loss: 1.9609
[Epoch 5] Loss: 1.9260
[Epoch 6] Loss: 1.8795
[Epoch 7] Loss: 1.8397
[Epoch 8] Loss: 1.7864
[Epoch 9] Loss: 1.7568
[Epoch 10] Loss: 1.7341
[Epoch 11] Loss: 1.7022
[Epoch 12] Loss: 1.6980
[Epoch 13] Loss: 1.6610
[Epoch 14] Loss: 1.6311
[Epoch 15] Loss: 1.6300
[Epoch 16] Loss: 1.5867
[Epoch 17] Loss: 1.5455
[Epoch 18] Loss: 1.5340
[Epoch 19] Loss: 1.5121
[Epoch 20] Loss: 1.5195
[Epoch 21] Loss: 1.4816
[Epoch 22] Loss: 1.4515
[Epoch 23] Loss: 1.4307
[Epoch 24] Loss: 1.4104
[Epoch 25] Loss: 1.3634
[Epoch 26] Loss: 1.3586
[Epoch 27] Loss: 1.3375
[Epoch 28] Loss: 1.3286
[Epoch 29] Loss: 1.3136
[Epoch 30] Loss: 1.2865
Training linear probe model (fine-tuning external model's final classifier)...
[Linear Prob Epoch 1] Loss: 2.1548
[Linear Prob Epoch 2] Loss: 1.5394
[Linear Prob Epoch 3] Loss: 1.3895
[Linear Prob Epoch 4] Loss: 1.3266
[Linear Prob Epoch 5] Loss: 1.2710
[Linear Prob Epoch 6] Loss: 1.2469
[Linear Prob Epoch 7] Loss: 1.2120
[Linear Prob Epoch 8] Loss: 1.1842
[Linear Prob Epoch 9] Loss: 1.1903
[Linear Prob Epoch 10] Loss: 1.1499
[Linear Prob Epoch 11] Loss: 1.1596
[Linear Prob Epoch 12] Loss: 1.1299
[Linear Prob Epoch 13] Loss: 1.1223
[Linear Prob Epoch 14] Loss: 1.1208
[Linear Prob Epoch 15] Loss: 1.1207
[Linear Prob Epoch 16] Loss: 1.1118
[Linear Prob Epoch 17] Loss: 1.1127
[Linear Prob Epoch 18] Loss: 1.0930
[Linear Prob Epoch 19] Loss: 1.0823
[Linear Prob Epoch 20] Loss: 1.0798
[Linear Prob Epoch 21] Loss: 1.0777
[Linear Prob Epoch 22] Loss: 1.0770
[Linear Prob Epoch 23] Loss: 1.0563
[Linear Prob Epoch 24] Loss: 1.0506
[Linear Prob Epoch 25] Loss: 1.0420
[Linear Prob Epoch 26] Loss: 1.0365
[Linear Prob Epoch 27] Loss: 1.0407
[Linear Prob Epoch 28] Loss: 1.0221
[Linear Prob Epoch 29] Loss: 1.0509
[Linear Prob Epoch 30] Loss: 1.0245
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5023
[Enhanced Epoch 2] Loss: 1.2710
[Enhanced Epoch 3] Loss: 1.1944
[Enhanced Epoch 4] Loss: 1.1442
[Enhanced Epoch 5] Loss: 1.1026
[Enhanced Epoch 6] Loss: 1.0548
[Enhanced Epoch 7] Loss: 1.0236
[Enhanced Epoch 8] Loss: 0.9866
[Enhanced Epoch 9] Loss: 0.9631
[Enhanced Epoch 10] Loss: 0.9277
[Enhanced Epoch 11] Loss: 0.9119
[Enhanced Epoch 12] Loss: 0.8873
[Enhanced Epoch 13] Loss: 0.8637
[Enhanced Epoch 14] Loss: 0.8393
[Enhanced Epoch 15] Loss: 0.8288
[Enhanced Epoch 16] Loss: 0.8030
[Enhanced Epoch 17] Loss: 0.7870
[Enhanced Epoch 18] Loss: 0.7755
[Enhanced Epoch 19] Loss: 0.7561
[Enhanced Epoch 20] Loss: 0.7419
[Enhanced Epoch 21] Loss: 0.7227
[Enhanced Epoch 22] Loss: 0.7126
[Enhanced Epoch 23] Loss: 0.6983
[Enhanced Epoch 24] Loss: 0.6866
[Enhanced Epoch 25] Loss: 0.6680
[Enhanced Epoch 26] Loss: 0.6559
[Enhanced Epoch 27] Loss: 0.6490
[Enhanced Epoch 28] Loss: 0.6333
[Enhanced Epoch 29] Loss: 0.6281
[Enhanced Epoch 30] Loss: 0.6146
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.0093
[Epoch 2] Loss: 1.4724
[Epoch 3] Loss: 1.3871
[Epoch 4] Loss: 1.3503
[Epoch 5] Loss: 1.3132
[Epoch 6] Loss: 1.3080
[Epoch 7] Loss: 1.2695
[Epoch 8] Loss: 1.2528
[Epoch 9] Loss: 1.2284
[Epoch 10] Loss: 1.2002
[Epoch 11] Loss: 1.1966
[Epoch 12] Loss: 1.1739
[Epoch 13] Loss: 1.1637
[Epoch 14] Loss: 1.1311
[Epoch 15] Loss: 1.1329
[Epoch 16] Loss: 1.0898
[Epoch 17] Loss: 1.0909
[Epoch 18] Loss: 1.0680
[Epoch 19] Loss: 1.0498
[Epoch 20] Loss: 1.0189
[Epoch 21] Loss: 1.0153
[Epoch 22] Loss: 0.9732
[Epoch 23] Loss: 0.9507
[Epoch 24] Loss: 0.9551
[Epoch 25] Loss: 0.9215
[Epoch 26] Loss: 0.9230
[Epoch 27] Loss: 0.8912
[Epoch 28] Loss: 0.8875
[Epoch 29] Loss: 0.8814
[Epoch 30] Loss: 0.8246
Training knowledge distillation model (Transformer student with teacher external)...
[Distillation Epoch 1] Loss: 3.6699
[Distillation Epoch 2] Loss: 3.4196
[Distillation Epoch 3] Loss: 3.2529
[Distillation Epoch 4] Loss: 3.1264
[Distillation Epoch 5] Loss: 2.9932
[Distillation Epoch 6] Loss: 2.9006
[Distillation Epoch 7] Loss: 2.8313
[Distillation Epoch 8] Loss: 2.7588
[Distillation Epoch 9] Loss: 2.7205
[Distillation Epoch 10] Loss: 2.6232
[Distillation Epoch 11] Loss: 2.5933
[Distillation Epoch 12] Loss: 2.5658
[Distillation Epoch 13] Loss: 2.5072
[Distillation Epoch 14] Loss: 2.5208
[Distillation Epoch 15] Loss: 2.4934
[Distillation Epoch 16] Loss: 2.4104
[Distillation Epoch 17] Loss: 2.3357
[Distillation Epoch 18] Loss: 2.3735
[Distillation Epoch 19] Loss: 2.2857
[Distillation Epoch 20] Loss: 2.2495
[Distillation Epoch 21] Loss: 2.2256
[Distillation Epoch 22] Loss: 2.2513
[Distillation Epoch 23] Loss: 2.1942
[Distillation Epoch 24] Loss: 2.1615
[Distillation Epoch 25] Loss: 2.1593
[Distillation Epoch 26] Loss: 2.0842
[Distillation Epoch 27] Loss: 2.0839
[Distillation Epoch 28] Loss: 2.1038
[Distillation Epoch 29] Loss: 2.0311
[Distillation Epoch 30] Loss: 2.0113

[Run 3 Results]
Baseline:         Acc=44.51% | AUC=0.8666 | F1=0.4334 | MinCAcc=12.30%
Linear Probe:     Acc=48.06% | AUC=0.8744 | F1=0.4795 | MinCAcc=32.20%
Enhanced (Concat):Acc=47.77% | AUC=0.8690 | F1=0.4746 | MinCAcc=33.80%
Baseline Adapter: Acc=47.76% | AUC=0.8668 | F1=0.4720 | MinCAcc=27.80%
Distillation:     Acc=42.24% | AUC=0.8626 | F1=0.3963 | MinCAcc=3.80%

=== Run 4/5, raw-set seed=45 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw set)...
[Epoch 1] Loss: 2.1369
[Epoch 2] Loss: 2.0441
[Epoch 3] Loss: 1.9980
[Epoch 4] Loss: 1.9547
[Epoch 5] Loss: 1.9043
[Epoch 6] Loss: 1.8665
[Epoch 7] Loss: 1.8023
[Epoch 8] Loss: 1.7706
[Epoch 9] Loss: 1.7471
[Epoch 10] Loss: 1.7264
[Epoch 11] Loss: 1.7230
[Epoch 12] Loss: 1.6526
[Epoch 13] Loss: 1.6753
[Epoch 14] Loss: 1.6390
[Epoch 15] Loss: 1.6123
[Epoch 16] Loss: 1.5936
[Epoch 17] Loss: 1.5448
[Epoch 18] Loss: 1.5330
[Epoch 19] Loss: 1.5224
[Epoch 20] Loss: 1.4952
[Epoch 21] Loss: 1.4577
[Epoch 22] Loss: 1.4168
[Epoch 23] Loss: 1.4361
[Epoch 24] Loss: 1.3952
[Epoch 25] Loss: 1.3657
[Epoch 26] Loss: 1.3726
[Epoch 27] Loss: 1.3095
[Epoch 28] Loss: 1.3326
[Epoch 29] Loss: 1.2994
[Epoch 30] Loss: 1.2933
Training linear probe model (fine-tuning external model's final classifier)...
[Linear Prob Epoch 1] Loss: 2.1246
[Linear Prob Epoch 2] Loss: 1.5308
[Linear Prob Epoch 3] Loss: 1.3997
[Linear Prob Epoch 4] Loss: 1.3303
[Linear Prob Epoch 5] Loss: 1.2923
[Linear Prob Epoch 6] Loss: 1.2714
[Linear Prob Epoch 7] Loss: 1.2369
[Linear Prob Epoch 8] Loss: 1.2135
[Linear Prob Epoch 9] Loss: 1.1847
[Linear Prob Epoch 10] Loss: 1.1655
[Linear Prob Epoch 11] Loss: 1.1390
[Linear Prob Epoch 12] Loss: 1.1395
[Linear Prob Epoch 13] Loss: 1.1617
[Linear Prob Epoch 14] Loss: 1.1170
[Linear Prob Epoch 15] Loss: 1.1225
[Linear Prob Epoch 16] Loss: 1.1034
[Linear Prob Epoch 17] Loss: 1.0770
[Linear Prob Epoch 18] Loss: 1.0850
[Linear Prob Epoch 19] Loss: 1.0812
[Linear Prob Epoch 20] Loss: 1.0903
[Linear Prob Epoch 21] Loss: 1.0590
[Linear Prob Epoch 22] Loss: 1.0553
[Linear Prob Epoch 23] Loss: 1.0612
[Linear Prob Epoch 24] Loss: 1.0547
[Linear Prob Epoch 25] Loss: 1.0366
[Linear Prob Epoch 26] Loss: 1.0325
[Linear Prob Epoch 27] Loss: 1.0379
[Linear Prob Epoch 28] Loss: 1.0364
[Linear Prob Epoch 29] Loss: 1.0170
[Linear Prob Epoch 30] Loss: 1.0295
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5073
[Enhanced Epoch 2] Loss: 1.2711
[Enhanced Epoch 3] Loss: 1.1981
[Enhanced Epoch 4] Loss: 1.1341
[Enhanced Epoch 5] Loss: 1.0936
[Enhanced Epoch 6] Loss: 1.0654
[Enhanced Epoch 7] Loss: 1.0178
[Enhanced Epoch 8] Loss: 0.9924
[Enhanced Epoch 9] Loss: 0.9610
[Enhanced Epoch 10] Loss: 0.9352
[Enhanced Epoch 11] Loss: 0.9006
[Enhanced Epoch 12] Loss: 0.8817
[Enhanced Epoch 13] Loss: 0.8620
[Enhanced Epoch 14] Loss: 0.8432
[Enhanced Epoch 15] Loss: 0.8223
[Enhanced Epoch 16] Loss: 0.8025
[Enhanced Epoch 17] Loss: 0.7900
[Enhanced Epoch 18] Loss: 0.7722
[Enhanced Epoch 19] Loss: 0.7595
[Enhanced Epoch 20] Loss: 0.7409
[Enhanced Epoch 21] Loss: 0.7256
[Enhanced Epoch 22] Loss: 0.7094
[Enhanced Epoch 23] Loss: 0.6951
[Enhanced Epoch 24] Loss: 0.6859
[Enhanced Epoch 25] Loss: 0.6736
[Enhanced Epoch 26] Loss: 0.6554
[Enhanced Epoch 27] Loss: 0.6449
[Enhanced Epoch 28] Loss: 0.6363
[Enhanced Epoch 29] Loss: 0.6268
[Enhanced Epoch 30] Loss: 0.6120
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.0051
[Epoch 2] Loss: 1.4778
[Epoch 3] Loss: 1.3903
[Epoch 4] Loss: 1.3675
[Epoch 5] Loss: 1.3171
[Epoch 6] Loss: 1.2976
[Epoch 7] Loss: 1.2661
[Epoch 8] Loss: 1.2540
[Epoch 9] Loss: 1.2211
[Epoch 10] Loss: 1.2147
[Epoch 11] Loss: 1.1889
[Epoch 12] Loss: 1.1672
[Epoch 13] Loss: 1.1594
[Epoch 14] Loss: 1.1385
[Epoch 15] Loss: 1.1170
[Epoch 16] Loss: 1.0990
[Epoch 17] Loss: 1.0796
[Epoch 18] Loss: 1.0795
[Epoch 19] Loss: 1.0504
[Epoch 20] Loss: 1.0460
[Epoch 21] Loss: 1.0171
[Epoch 22] Loss: 1.0052
[Epoch 23] Loss: 0.9794
[Epoch 24] Loss: 0.9460
[Epoch 25] Loss: 0.9355
[Epoch 26] Loss: 0.9079
[Epoch 27] Loss: 0.8873
[Epoch 28] Loss: 0.8833
[Epoch 29] Loss: 0.8481
[Epoch 30] Loss: 0.8494
Training knowledge distillation model (Transformer student with teacher external)...
[Distillation Epoch 1] Loss: 3.6636
[Distillation Epoch 2] Loss: 3.3714
[Distillation Epoch 3] Loss: 3.1999
[Distillation Epoch 4] Loss: 3.0637
[Distillation Epoch 5] Loss: 2.9376
[Distillation Epoch 6] Loss: 2.8603
[Distillation Epoch 7] Loss: 2.7778
[Distillation Epoch 8] Loss: 2.7127
[Distillation Epoch 9] Loss: 2.7033
[Distillation Epoch 10] Loss: 2.6330
[Distillation Epoch 11] Loss: 2.6063
[Distillation Epoch 12] Loss: 2.5763
[Distillation Epoch 13] Loss: 2.5073
[Distillation Epoch 14] Loss: 2.4547
[Distillation Epoch 15] Loss: 2.4387
[Distillation Epoch 16] Loss: 2.3912
[Distillation Epoch 17] Loss: 2.3788
[Distillation Epoch 18] Loss: 2.3292
[Distillation Epoch 19] Loss: 2.2808
[Distillation Epoch 20] Loss: 2.2513
[Distillation Epoch 21] Loss: 2.2526
[Distillation Epoch 22] Loss: 2.2220
[Distillation Epoch 23] Loss: 2.1706
[Distillation Epoch 24] Loss: 2.1303
[Distillation Epoch 25] Loss: 2.1556
[Distillation Epoch 26] Loss: 2.0890
[Distillation Epoch 27] Loss: 2.0824
[Distillation Epoch 28] Loss: 2.0694
[Distillation Epoch 29] Loss: 2.0603
[Distillation Epoch 30] Loss: 2.0198

[Run 4 Results]
Baseline:         Acc=44.15% | AUC=0.8649 | F1=0.4217 | MinCAcc=16.20%
Linear Probe:     Acc=48.43% | AUC=0.8750 | F1=0.4796 | MinCAcc=27.70%
Enhanced (Concat):Acc=48.06% | AUC=0.8700 | F1=0.4789 | MinCAcc=32.50%
Baseline Adapter: Acc=47.64% | AUC=0.8717 | F1=0.4652 | MinCAcc=24.50%
Distillation:     Acc=42.80% | AUC=0.8646 | F1=0.4085 | MinCAcc=3.50%

=== Run 5/5, raw-set seed=46 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw set)...
[Epoch 1] Loss: 2.1380
[Epoch 2] Loss: 2.0478
[Epoch 3] Loss: 2.0038
[Epoch 4] Loss: 1.9531
[Epoch 5] Loss: 1.9096
[Epoch 6] Loss: 1.8837
[Epoch 7] Loss: 1.8422
[Epoch 8] Loss: 1.8234
[Epoch 9] Loss: 1.7511
[Epoch 10] Loss: 1.7413
[Epoch 11] Loss: 1.7097
[Epoch 12] Loss: 1.6731
[Epoch 13] Loss: 1.6680
[Epoch 14] Loss: 1.6548
[Epoch 15] Loss: 1.6117
[Epoch 16] Loss: 1.5619
[Epoch 17] Loss: 1.5551
[Epoch 18] Loss: 1.5070
[Epoch 19] Loss: 1.4729
[Epoch 20] Loss: 1.4725
[Epoch 21] Loss: 1.4599
[Epoch 22] Loss: 1.4008
[Epoch 23] Loss: 1.4437
[Epoch 24] Loss: 1.4094
[Epoch 25] Loss: 1.3546
[Epoch 26] Loss: 1.3455
[Epoch 27] Loss: 1.3164
[Epoch 28] Loss: 1.2940
[Epoch 29] Loss: 1.2930
[Epoch 30] Loss: 1.2904
Training linear probe model (fine-tuning external model's final classifier)...
[Linear Prob Epoch 1] Loss: 2.1683
[Linear Prob Epoch 2] Loss: 1.5129
[Linear Prob Epoch 3] Loss: 1.3874
[Linear Prob Epoch 4] Loss: 1.3241
[Linear Prob Epoch 5] Loss: 1.2779
[Linear Prob Epoch 6] Loss: 1.2459
[Linear Prob Epoch 7] Loss: 1.2140
[Linear Prob Epoch 8] Loss: 1.1894
[Linear Prob Epoch 9] Loss: 1.1734
[Linear Prob Epoch 10] Loss: 1.1523
[Linear Prob Epoch 11] Loss: 1.1344
[Linear Prob Epoch 12] Loss: 1.1355
[Linear Prob Epoch 13] Loss: 1.1255
[Linear Prob Epoch 14] Loss: 1.1060
[Linear Prob Epoch 15] Loss: 1.1083
[Linear Prob Epoch 16] Loss: 1.0918
[Linear Prob Epoch 17] Loss: 1.0891
[Linear Prob Epoch 18] Loss: 1.0826
[Linear Prob Epoch 19] Loss: 1.0635
[Linear Prob Epoch 20] Loss: 1.0654
[Linear Prob Epoch 21] Loss: 1.0697
[Linear Prob Epoch 22] Loss: 1.0595
[Linear Prob Epoch 23] Loss: 1.0318
[Linear Prob Epoch 24] Loss: 1.0615
[Linear Prob Epoch 25] Loss: 1.0459
[Linear Prob Epoch 26] Loss: 1.0520
[Linear Prob Epoch 27] Loss: 1.0340
[Linear Prob Epoch 28] Loss: 1.0155
[Linear Prob Epoch 29] Loss: 1.0136
[Linear Prob Epoch 30] Loss: 1.0278
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5164
[Enhanced Epoch 2] Loss: 1.2628
[Enhanced Epoch 3] Loss: 1.1851
[Enhanced Epoch 4] Loss: 1.1298
[Enhanced Epoch 5] Loss: 1.0874
[Enhanced Epoch 6] Loss: 1.0421
[Enhanced Epoch 7] Loss: 1.0064
[Enhanced Epoch 8] Loss: 0.9686
[Enhanced Epoch 9] Loss: 0.9465
[Enhanced Epoch 10] Loss: 0.9150
[Enhanced Epoch 11] Loss: 0.8855
[Enhanced Epoch 12] Loss: 0.8738
[Enhanced Epoch 13] Loss: 0.8443
[Enhanced Epoch 14] Loss: 0.8282
[Enhanced Epoch 15] Loss: 0.8049
[Enhanced Epoch 16] Loss: 0.7846
[Enhanced Epoch 17] Loss: 0.7691
[Enhanced Epoch 18] Loss: 0.7499
[Enhanced Epoch 19] Loss: 0.7338
[Enhanced Epoch 20] Loss: 0.7227
[Enhanced Epoch 21] Loss: 0.7030
[Enhanced Epoch 22] Loss: 0.6881
[Enhanced Epoch 23] Loss: 0.6763
[Enhanced Epoch 24] Loss: 0.6652
[Enhanced Epoch 25] Loss: 0.6508
[Enhanced Epoch 26] Loss: 0.6394
[Enhanced Epoch 27] Loss: 0.6251
[Enhanced Epoch 28] Loss: 0.6164
[Enhanced Epoch 29] Loss: 0.6051
[Enhanced Epoch 30] Loss: 0.5947
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.0168
[Epoch 2] Loss: 1.4895
[Epoch 3] Loss: 1.3787
[Epoch 4] Loss: 1.3728
[Epoch 5] Loss: 1.3238
[Epoch 6] Loss: 1.2898
[Epoch 7] Loss: 1.2716
[Epoch 8] Loss: 1.2399
[Epoch 9] Loss: 1.2374
[Epoch 10] Loss: 1.2042
[Epoch 11] Loss: 1.1857
[Epoch 12] Loss: 1.1629
[Epoch 13] Loss: 1.1461
[Epoch 14] Loss: 1.1571
[Epoch 15] Loss: 1.0895
[Epoch 16] Loss: 1.1020
[Epoch 17] Loss: 1.0871
[Epoch 18] Loss: 1.0495
[Epoch 19] Loss: 1.0437
[Epoch 20] Loss: 1.0203
[Epoch 21] Loss: 1.0101
[Epoch 22] Loss: 0.9564
[Epoch 23] Loss: 0.9687
[Epoch 24] Loss: 0.9487
[Epoch 25] Loss: 0.9143
[Epoch 26] Loss: 0.8852
[Epoch 27] Loss: 0.8698
[Epoch 28] Loss: 0.8601
[Epoch 29] Loss: 0.8394
[Epoch 30] Loss: 0.8338
Training knowledge distillation model (Transformer student with teacher external)...
[Distillation Epoch 1] Loss: 3.6161
[Distillation Epoch 2] Loss: 3.3910
[Distillation Epoch 3] Loss: 3.2670
[Distillation Epoch 4] Loss: 3.1766
[Distillation Epoch 5] Loss: 3.0252
[Distillation Epoch 6] Loss: 2.9523
[Distillation Epoch 7] Loss: 2.8726
[Distillation Epoch 8] Loss: 2.7845
[Distillation Epoch 9] Loss: 2.7217
[Distillation Epoch 10] Loss: 2.6531
[Distillation Epoch 11] Loss: 2.6750
[Distillation Epoch 12] Loss: 2.5794
[Distillation Epoch 13] Loss: 2.5687
[Distillation Epoch 14] Loss: 2.4718
[Distillation Epoch 15] Loss: 2.4836
[Distillation Epoch 16] Loss: 2.4092
[Distillation Epoch 17] Loss: 2.3975
[Distillation Epoch 18] Loss: 2.3406
[Distillation Epoch 19] Loss: 2.3018
[Distillation Epoch 20] Loss: 2.2815
[Distillation Epoch 21] Loss: 2.2818
[Distillation Epoch 22] Loss: 2.2044
[Distillation Epoch 23] Loss: 2.2099
[Distillation Epoch 24] Loss: 2.1703
[Distillation Epoch 25] Loss: 2.1571
[Distillation Epoch 26] Loss: 2.1269
[Distillation Epoch 27] Loss: 2.0955
[Distillation Epoch 28] Loss: 2.1156
[Distillation Epoch 29] Loss: 2.0456
[Distillation Epoch 30] Loss: 2.0504

[Run 5 Results]
Baseline:         Acc=45.69% | AUC=0.8682 | F1=0.4466 | MinCAcc=20.20%
Linear Probe:     Acc=49.00% | AUC=0.8764 | F1=0.4891 | MinCAcc=33.80%
Enhanced (Concat):Acc=47.76% | AUC=0.8697 | F1=0.4766 | MinCAcc=28.00%
Baseline Adapter: Acc=47.79% | AUC=0.8690 | F1=0.4720 | MinCAcc=30.50%
Distillation:     Acc=43.08% | AUC=0.8658 | F1=0.4023 | MinCAcc=3.90%

All done. Final mean/std results saved to: ./results/imb_tf.json
imb_tf.py completed successfully.
Starting adversarial_tf.py...
Files already downloaded and verified
Files already downloaded and verified

=== Training External Model (BigTransformer) on Adversarial Pretraining Data ===
Files already downloaded and verified
Files already downloaded and verified
[Epoch 1] Loss: 2.1672
[Epoch 2] Loss: 2.0501
[Epoch 3] Loss: 1.9916
[Epoch 4] Loss: 1.9348
[Epoch 5] Loss: 1.8845
[Epoch 6] Loss: 1.8428
[Epoch 7] Loss: 1.8085
[Epoch 8] Loss: 1.7833
[Epoch 9] Loss: 1.7598
[Epoch 10] Loss: 1.7384
[Epoch 11] Loss: 1.7160
[Epoch 12] Loss: 1.7137
[Epoch 13] Loss: 1.6742
[Epoch 14] Loss: 1.6505
[Epoch 15] Loss: 1.6379
[Epoch 16] Loss: 1.5944
[Epoch 17] Loss: 1.5548
[Epoch 18] Loss: 1.5025
[Epoch 19] Loss: 1.4890
[Epoch 20] Loss: 1.4629
[Epoch 21] Loss: 1.4410
[Epoch 22] Loss: 1.4106
[Epoch 23] Loss: 1.3915
[Epoch 24] Loss: 1.3719
[Epoch 25] Loss: 1.3348
[Epoch 26] Loss: 1.3209
[Epoch 27] Loss: 1.3191
[Epoch 28] Loss: 1.2923
[Epoch 29] Loss: 1.2876
[Epoch 30] Loss: 1.2748
[Epoch 31] Loss: 1.2618
[Epoch 32] Loss: 1.2474
[Epoch 33] Loss: 1.2185
[Epoch 34] Loss: 1.2278
[Epoch 35] Loss: 1.2074
[Epoch 36] Loss: 1.1992
[Epoch 37] Loss: 1.1724
[Epoch 38] Loss: 1.1744
[Epoch 39] Loss: 1.1737
[Epoch 40] Loss: 1.1384
[Epoch 41] Loss: 1.1506
[Epoch 42] Loss: 1.1282
[Epoch 43] Loss: 1.1069
[Epoch 44] Loss: 1.1023
[Epoch 45] Loss: 1.0959
[Epoch 46] Loss: 1.0788
[Epoch 47] Loss: 1.0671
[Epoch 48] Loss: 1.0806
[Epoch 49] Loss: 1.0595
[Epoch 50] Loss: 1.0497
[Epoch 51] Loss: 1.0351
[Epoch 52] Loss: 1.0142
[Epoch 53] Loss: 1.0306
[Epoch 54] Loss: 1.0029
[Epoch 55] Loss: 0.9885
[Epoch 56] Loss: 0.9787
[Epoch 57] Loss: 0.9898
[Epoch 58] Loss: 0.9887
[Epoch 59] Loss: 0.9634
[Epoch 60] Loss: 0.9629
External Model (Pretrained) Evaluation: Acc=41.61%, AUC=0.8841, F1=0.3575, MinCAcc=0.00%

=== Run 1/5, seed=42 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw_set)...
[Epoch 1] Loss: 2.1234
[Epoch 2] Loss: 2.0224
[Epoch 3] Loss: 1.9794
[Epoch 4] Loss: 1.9464
[Epoch 5] Loss: 1.9137
[Epoch 6] Loss: 1.8574
[Epoch 7] Loss: 1.8161
[Epoch 8] Loss: 1.7871
[Epoch 9] Loss: 1.7782
[Epoch 10] Loss: 1.7301
[Epoch 11] Loss: 1.6927
[Epoch 12] Loss: 1.6824
[Epoch 13] Loss: 1.6589
[Epoch 14] Loss: 1.6127
[Epoch 15] Loss: 1.5955
[Epoch 16] Loss: 1.5646
[Epoch 17] Loss: 1.5363
[Epoch 18] Loss: 1.5193
[Epoch 19] Loss: 1.4874
[Epoch 20] Loss: 1.4662
[Epoch 21] Loss: 1.4347
[Epoch 22] Loss: 1.4170
[Epoch 23] Loss: 1.4167
[Epoch 24] Loss: 1.3870
[Epoch 25] Loss: 1.3523
[Epoch 26] Loss: 1.3449
[Epoch 27] Loss: 1.3121
[Epoch 28] Loss: 1.2781
[Epoch 29] Loss: 1.2738
[Epoch 30] Loss: 1.2448
Training linear probe model (external model fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.4184
[Linear Prob Epoch 2] Loss: 1.3207
[Linear Prob Epoch 3] Loss: 1.2976
[Linear Prob Epoch 4] Loss: 1.2953
[Linear Prob Epoch 5] Loss: 1.2794
[Linear Prob Epoch 6] Loss: 1.2795
[Linear Prob Epoch 7] Loss: 1.2680
[Linear Prob Epoch 8] Loss: 1.2630
[Linear Prob Epoch 9] Loss: 1.2711
[Linear Prob Epoch 10] Loss: 1.2530
[Linear Prob Epoch 11] Loss: 1.2494
[Linear Prob Epoch 12] Loss: 1.2440
[Linear Prob Epoch 13] Loss: 1.2434
[Linear Prob Epoch 14] Loss: 1.2472
[Linear Prob Epoch 15] Loss: 1.2415
[Linear Prob Epoch 16] Loss: 1.2433
[Linear Prob Epoch 17] Loss: 1.2350
[Linear Prob Epoch 18] Loss: 1.2402
[Linear Prob Epoch 19] Loss: 1.2325
[Linear Prob Epoch 20] Loss: 1.2276
[Linear Prob Epoch 21] Loss: 1.2224
[Linear Prob Epoch 22] Loss: 1.2249
[Linear Prob Epoch 23] Loss: 1.2189
[Linear Prob Epoch 24] Loss: 1.2202
[Linear Prob Epoch 25] Loss: 1.2186
[Linear Prob Epoch 26] Loss: 1.2191
[Linear Prob Epoch 27] Loss: 1.2146
[Linear Prob Epoch 28] Loss: 1.2128
[Linear Prob Epoch 29] Loss: 1.2142
[Linear Prob Epoch 30] Loss: 1.2050
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.6118
[Enhanced Epoch 2] Loss: 1.3325
[Enhanced Epoch 3] Loss: 1.2937
[Enhanced Epoch 4] Loss: 1.2746
[Enhanced Epoch 5] Loss: 1.2553
[Enhanced Epoch 6] Loss: 1.2410
[Enhanced Epoch 7] Loss: 1.2340
[Enhanced Epoch 8] Loss: 1.2264
[Enhanced Epoch 9] Loss: 1.2164
[Enhanced Epoch 10] Loss: 1.2081
[Enhanced Epoch 11] Loss: 1.2046
[Enhanced Epoch 12] Loss: 1.2010
[Enhanced Epoch 13] Loss: 1.1916
[Enhanced Epoch 14] Loss: 1.1858
[Enhanced Epoch 15] Loss: 1.1824
[Enhanced Epoch 16] Loss: 1.1776
[Enhanced Epoch 17] Loss: 1.1714
[Enhanced Epoch 18] Loss: 1.1682
[Enhanced Epoch 19] Loss: 1.1629
[Enhanced Epoch 20] Loss: 1.1547
[Enhanced Epoch 21] Loss: 1.1523
[Enhanced Epoch 22] Loss: 1.1418
[Enhanced Epoch 23] Loss: 1.1465
[Enhanced Epoch 24] Loss: 1.1337
[Enhanced Epoch 25] Loss: 1.1341
[Enhanced Epoch 26] Loss: 1.1273
[Enhanced Epoch 27] Loss: 1.1239
[Enhanced Epoch 28] Loss: 1.1134
[Enhanced Epoch 29] Loss: 1.1083
[Enhanced Epoch 30] Loss: 1.1021
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.1124
[Epoch 2] Loss: 1.3991
[Epoch 3] Loss: 1.3484
[Epoch 4] Loss: 1.3364
[Epoch 5] Loss: 1.3284
[Epoch 6] Loss: 1.3139
[Epoch 7] Loss: 1.3038
[Epoch 8] Loss: 1.3032
[Epoch 9] Loss: 1.2953
[Epoch 10] Loss: 1.2913
[Epoch 11] Loss: 1.2769
[Epoch 12] Loss: 1.2773
[Epoch 13] Loss: 1.2736
[Epoch 14] Loss: 1.2699
[Epoch 15] Loss: 1.2560
[Epoch 16] Loss: 1.2624
[Epoch 17] Loss: 1.2519
[Epoch 18] Loss: 1.2493
[Epoch 19] Loss: 1.2454
[Epoch 20] Loss: 1.2470
[Epoch 21] Loss: 1.2384
[Epoch 22] Loss: 1.2431
[Epoch 23] Loss: 1.2366
[Epoch 24] Loss: 1.2344
[Epoch 25] Loss: 1.2266
[Epoch 26] Loss: 1.2357
[Epoch 27] Loss: 1.2279
[Epoch 28] Loss: 1.2323
[Epoch 29] Loss: 1.2204
[Epoch 30] Loss: 1.2160
Training knowledge distillation model (TransformerClassifier student with teacher external)...
[Distillation Epoch 1] Loss: 2.2619
[Distillation Epoch 2] Loss: 2.1023
[Distillation Epoch 3] Loss: 1.9664
[Distillation Epoch 4] Loss: 1.8601
[Distillation Epoch 5] Loss: 1.8088
[Distillation Epoch 6] Loss: 1.7361
[Distillation Epoch 7] Loss: 1.6984
[Distillation Epoch 8] Loss: 1.6378
[Distillation Epoch 9] Loss: 1.5958
[Distillation Epoch 10] Loss: 1.6082
[Distillation Epoch 11] Loss: 1.5028
[Distillation Epoch 12] Loss: 1.4884
[Distillation Epoch 13] Loss: 1.4335
[Distillation Epoch 14] Loss: 1.4248
[Distillation Epoch 15] Loss: 1.3910
[Distillation Epoch 16] Loss: 1.3313
[Distillation Epoch 17] Loss: 1.3113
[Distillation Epoch 18] Loss: 1.2837
[Distillation Epoch 19] Loss: 1.2726
[Distillation Epoch 20] Loss: 1.2152
[Distillation Epoch 21] Loss: 1.2070
[Distillation Epoch 22] Loss: 1.1905
[Distillation Epoch 23] Loss: 1.1710
[Distillation Epoch 24] Loss: 1.1650
[Distillation Epoch 25] Loss: 1.1578
[Distillation Epoch 26] Loss: 1.1224
[Distillation Epoch 27] Loss: 1.0997
[Distillation Epoch 28] Loss: 1.0785
[Distillation Epoch 29] Loss: 1.0934
[Distillation Epoch 30] Loss: 1.0687

[Run 1 Results]
Baseline:          Acc=44.49% | AUC=0.8653 | F1=0.4326 | MinCAcc=9.60%
Linear Probe:      Acc=52.85% | AUC=0.9091 | F1=0.5131 | MinCAcc=15.20%
Enhanced (Concat): Acc=55.13% | AUC=0.9148 | F1=0.5494 | MinCAcc=34.50%
Baseline Adapter:  Acc=52.21% | AUC=0.9078 | F1=0.5143 | MinCAcc=34.60%
Distillation:      Acc=46.03% | AUC=0.8721 | F1=0.4420 | MinCAcc=11.70%

=== Run 2/5, seed=43 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw_set)...
[Epoch 1] Loss: 2.1401
[Epoch 2] Loss: 2.0179
[Epoch 3] Loss: 1.9696
[Epoch 4] Loss: 1.8982
[Epoch 5] Loss: 1.8681
[Epoch 6] Loss: 1.8366
[Epoch 7] Loss: 1.7910
[Epoch 8] Loss: 1.7584
[Epoch 9] Loss: 1.7243
[Epoch 10] Loss: 1.7091
[Epoch 11] Loss: 1.6864
[Epoch 12] Loss: 1.6457
[Epoch 13] Loss: 1.6465
[Epoch 14] Loss: 1.5913
[Epoch 15] Loss: 1.5774
[Epoch 16] Loss: 1.5685
[Epoch 17] Loss: 1.5517
[Epoch 18] Loss: 1.4892
[Epoch 19] Loss: 1.5040
[Epoch 20] Loss: 1.4885
[Epoch 21] Loss: 1.4429
[Epoch 22] Loss: 1.4185
[Epoch 23] Loss: 1.3961
[Epoch 24] Loss: 1.3836
[Epoch 25] Loss: 1.3618
[Epoch 26] Loss: 1.3505
[Epoch 27] Loss: 1.3161
[Epoch 28] Loss: 1.2895
[Epoch 29] Loss: 1.2809
[Epoch 30] Loss: 1.2741
Training linear probe model (external model fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.3233
[Linear Prob Epoch 2] Loss: 1.2441
[Linear Prob Epoch 3] Loss: 1.2217
[Linear Prob Epoch 4] Loss: 1.2172
[Linear Prob Epoch 5] Loss: 1.2058
[Linear Prob Epoch 6] Loss: 1.1993
[Linear Prob Epoch 7] Loss: 1.1917
[Linear Prob Epoch 8] Loss: 1.1816
[Linear Prob Epoch 9] Loss: 1.1898
[Linear Prob Epoch 10] Loss: 1.1869
[Linear Prob Epoch 11] Loss: 1.1755
[Linear Prob Epoch 12] Loss: 1.1722
[Linear Prob Epoch 13] Loss: 1.1694
[Linear Prob Epoch 14] Loss: 1.1750
[Linear Prob Epoch 15] Loss: 1.1650
[Linear Prob Epoch 16] Loss: 1.1706
[Linear Prob Epoch 17] Loss: 1.1598
[Linear Prob Epoch 18] Loss: 1.1667
[Linear Prob Epoch 19] Loss: 1.1594
[Linear Prob Epoch 20] Loss: 1.1600
[Linear Prob Epoch 21] Loss: 1.1517
[Linear Prob Epoch 22] Loss: 1.1507
[Linear Prob Epoch 23] Loss: 1.1503
[Linear Prob Epoch 24] Loss: 1.1460
[Linear Prob Epoch 25] Loss: 1.1518
[Linear Prob Epoch 26] Loss: 1.1446
[Linear Prob Epoch 27] Loss: 1.1362
[Linear Prob Epoch 28] Loss: 1.1372
[Linear Prob Epoch 29] Loss: 1.1479
[Linear Prob Epoch 30] Loss: 1.1383
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5462
[Enhanced Epoch 2] Loss: 1.2549
[Enhanced Epoch 3] Loss: 1.2106
[Enhanced Epoch 4] Loss: 1.1863
[Enhanced Epoch 5] Loss: 1.1672
[Enhanced Epoch 6] Loss: 1.1543
[Enhanced Epoch 7] Loss: 1.1450
[Enhanced Epoch 8] Loss: 1.1334
[Enhanced Epoch 9] Loss: 1.1230
[Enhanced Epoch 10] Loss: 1.1223
[Enhanced Epoch 11] Loss: 1.1140
[Enhanced Epoch 12] Loss: 1.1111
[Enhanced Epoch 13] Loss: 1.1028
[Enhanced Epoch 14] Loss: 1.0873
[Enhanced Epoch 15] Loss: 1.0933
[Enhanced Epoch 16] Loss: 1.0896
[Enhanced Epoch 17] Loss: 1.0750
[Enhanced Epoch 18] Loss: 1.0729
[Enhanced Epoch 19] Loss: 1.0729
[Enhanced Epoch 20] Loss: 1.0713
[Enhanced Epoch 21] Loss: 1.0548
[Enhanced Epoch 22] Loss: 1.0583
[Enhanced Epoch 23] Loss: 1.0512
[Enhanced Epoch 24] Loss: 1.0438
[Enhanced Epoch 25] Loss: 1.0361
[Enhanced Epoch 26] Loss: 1.0342
[Enhanced Epoch 27] Loss: 1.0265
[Enhanced Epoch 28] Loss: 1.0246
[Enhanced Epoch 29] Loss: 1.0214
[Enhanced Epoch 30] Loss: 1.0108
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.0922
[Epoch 2] Loss: 1.3230
[Epoch 3] Loss: 1.2690
[Epoch 4] Loss: 1.2563
[Epoch 5] Loss: 1.2416
[Epoch 6] Loss: 1.2305
[Epoch 7] Loss: 1.2162
[Epoch 8] Loss: 1.2227
[Epoch 9] Loss: 1.2194
[Epoch 10] Loss: 1.2014
[Epoch 11] Loss: 1.2036
[Epoch 12] Loss: 1.1939
[Epoch 13] Loss: 1.1949
[Epoch 14] Loss: 1.1878
[Epoch 15] Loss: 1.1813
[Epoch 16] Loss: 1.1831
[Epoch 17] Loss: 1.1710
[Epoch 18] Loss: 1.1718
[Epoch 19] Loss: 1.1759
[Epoch 20] Loss: 1.1741
[Epoch 21] Loss: 1.1671
[Epoch 22] Loss: 1.1548
[Epoch 23] Loss: 1.1701
[Epoch 24] Loss: 1.1500
[Epoch 25] Loss: 1.1526
[Epoch 26] Loss: 1.1433
[Epoch 27] Loss: 1.1538
[Epoch 28] Loss: 1.1344
[Epoch 29] Loss: 1.1566
[Epoch 30] Loss: 1.1432
Training knowledge distillation model (TransformerClassifier student with teacher external)...
[Distillation Epoch 1] Loss: 2.2928
[Distillation Epoch 2] Loss: 2.1258
[Distillation Epoch 3] Loss: 2.0301
[Distillation Epoch 4] Loss: 1.9452
[Distillation Epoch 5] Loss: 1.9042
[Distillation Epoch 6] Loss: 1.8258
[Distillation Epoch 7] Loss: 1.7741
[Distillation Epoch 8] Loss: 1.7533
[Distillation Epoch 9] Loss: 1.6851
[Distillation Epoch 10] Loss: 1.6287
[Distillation Epoch 11] Loss: 1.5582
[Distillation Epoch 12] Loss: 1.5219
[Distillation Epoch 13] Loss: 1.4922
[Distillation Epoch 14] Loss: 1.4357
[Distillation Epoch 15] Loss: 1.4243
[Distillation Epoch 16] Loss: 1.3833
[Distillation Epoch 17] Loss: 1.3247
[Distillation Epoch 18] Loss: 1.3336
[Distillation Epoch 19] Loss: 1.2843
[Distillation Epoch 20] Loss: 1.2444
[Distillation Epoch 21] Loss: 1.2594
[Distillation Epoch 22] Loss: 1.2256
[Distillation Epoch 23] Loss: 1.1926
[Distillation Epoch 24] Loss: 1.1804
[Distillation Epoch 25] Loss: 1.1883
[Distillation Epoch 26] Loss: 1.1485
[Distillation Epoch 27] Loss: 1.1250
[Distillation Epoch 28] Loss: 1.1331
[Distillation Epoch 29] Loss: 1.1109
[Distillation Epoch 30] Loss: 1.0811

[Run 2 Results]
Baseline:          Acc=45.15% | AUC=0.8609 | F1=0.4486 | MinCAcc=28.90%
Linear Probe:      Acc=53.40% | AUC=0.9092 | F1=0.5313 | MinCAcc=35.20%
Enhanced (Concat): Acc=55.03% | AUC=0.9139 | F1=0.5498 | MinCAcc=35.90%
Baseline Adapter:  Acc=52.44% | AUC=0.9100 | F1=0.5243 | MinCAcc=30.10%
Distillation:      Acc=45.24% | AUC=0.8700 | F1=0.4477 | MinCAcc=23.20%

=== Run 3/5, seed=44 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw_set)...
[Epoch 1] Loss: 2.1646
[Epoch 2] Loss: 2.0551
[Epoch 3] Loss: 2.0010
[Epoch 4] Loss: 1.9508
[Epoch 5] Loss: 1.9165
[Epoch 6] Loss: 1.8726
[Epoch 7] Loss: 1.8379
[Epoch 8] Loss: 1.8123
[Epoch 9] Loss: 1.7688
[Epoch 10] Loss: 1.7313
[Epoch 11] Loss: 1.7235
[Epoch 12] Loss: 1.6686
[Epoch 13] Loss: 1.6679
[Epoch 14] Loss: 1.6372
[Epoch 15] Loss: 1.6097
[Epoch 16] Loss: 1.5985
[Epoch 17] Loss: 1.5397
[Epoch 18] Loss: 1.5181
[Epoch 19] Loss: 1.5129
[Epoch 20] Loss: 1.4893
[Epoch 21] Loss: 1.4773
[Epoch 22] Loss: 1.4260
[Epoch 23] Loss: 1.4257
[Epoch 24] Loss: 1.4048
[Epoch 25] Loss: 1.3662
[Epoch 26] Loss: 1.3631
[Epoch 27] Loss: 1.3253
[Epoch 28] Loss: 1.3143
[Epoch 29] Loss: 1.3057
[Epoch 30] Loss: 1.2633
Training linear probe model (external model fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.3585
[Linear Prob Epoch 2] Loss: 1.2748
[Linear Prob Epoch 3] Loss: 1.2502
[Linear Prob Epoch 4] Loss: 1.2503
[Linear Prob Epoch 5] Loss: 1.2420
[Linear Prob Epoch 6] Loss: 1.2286
[Linear Prob Epoch 7] Loss: 1.2303
[Linear Prob Epoch 8] Loss: 1.2282
[Linear Prob Epoch 9] Loss: 1.2248
[Linear Prob Epoch 10] Loss: 1.2166
[Linear Prob Epoch 11] Loss: 1.2133
[Linear Prob Epoch 12] Loss: 1.2123
[Linear Prob Epoch 13] Loss: 1.2075
[Linear Prob Epoch 14] Loss: 1.2029
[Linear Prob Epoch 15] Loss: 1.2007
[Linear Prob Epoch 16] Loss: 1.1966
[Linear Prob Epoch 17] Loss: 1.1962
[Linear Prob Epoch 18] Loss: 1.1903
[Linear Prob Epoch 19] Loss: 1.1872
[Linear Prob Epoch 20] Loss: 1.1871
[Linear Prob Epoch 21] Loss: 1.1880
[Linear Prob Epoch 22] Loss: 1.1842
[Linear Prob Epoch 23] Loss: 1.1869
[Linear Prob Epoch 24] Loss: 1.1748
[Linear Prob Epoch 25] Loss: 1.1728
[Linear Prob Epoch 26] Loss: 1.1779
[Linear Prob Epoch 27] Loss: 1.1798
[Linear Prob Epoch 28] Loss: 1.1750
[Linear Prob Epoch 29] Loss: 1.1749
[Linear Prob Epoch 30] Loss: 1.1681
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5727
[Enhanced Epoch 2] Loss: 1.2881
[Enhanced Epoch 3] Loss: 1.2464
[Enhanced Epoch 4] Loss: 1.2166
[Enhanced Epoch 5] Loss: 1.2006
[Enhanced Epoch 6] Loss: 1.1917
[Enhanced Epoch 7] Loss: 1.1845
[Enhanced Epoch 8] Loss: 1.1733
[Enhanced Epoch 9] Loss: 1.1685
[Enhanced Epoch 10] Loss: 1.1569
[Enhanced Epoch 11] Loss: 1.1495
[Enhanced Epoch 12] Loss: 1.1491
[Enhanced Epoch 13] Loss: 1.1384
[Enhanced Epoch 14] Loss: 1.1453
[Enhanced Epoch 15] Loss: 1.1295
[Enhanced Epoch 16] Loss: 1.1235
[Enhanced Epoch 17] Loss: 1.1276
[Enhanced Epoch 18] Loss: 1.1171
[Enhanced Epoch 19] Loss: 1.1083
[Enhanced Epoch 20] Loss: 1.1129
[Enhanced Epoch 21] Loss: 1.1052
[Enhanced Epoch 22] Loss: 1.0953
[Enhanced Epoch 23] Loss: 1.0919
[Enhanced Epoch 24] Loss: 1.0904
[Enhanced Epoch 25] Loss: 1.0811
[Enhanced Epoch 26] Loss: 1.0804
[Enhanced Epoch 27] Loss: 1.0744
[Enhanced Epoch 28] Loss: 1.0699
[Enhanced Epoch 29] Loss: 1.0657
[Enhanced Epoch 30] Loss: 1.0657
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.1194
[Epoch 2] Loss: 1.3545
[Epoch 3] Loss: 1.3016
[Epoch 4] Loss: 1.2861
[Epoch 5] Loss: 1.2810
[Epoch 6] Loss: 1.2777
[Epoch 7] Loss: 1.2605
[Epoch 8] Loss: 1.2440
[Epoch 9] Loss: 1.2519
[Epoch 10] Loss: 1.2399
[Epoch 11] Loss: 1.2334
[Epoch 12] Loss: 1.2309
[Epoch 13] Loss: 1.2304
[Epoch 14] Loss: 1.2295
[Epoch 15] Loss: 1.2244
[Epoch 16] Loss: 1.2117
[Epoch 17] Loss: 1.2163
[Epoch 18] Loss: 1.2061
[Epoch 19] Loss: 1.2081
[Epoch 20] Loss: 1.2036
[Epoch 21] Loss: 1.1996
[Epoch 22] Loss: 1.1989
[Epoch 23] Loss: 1.1859
[Epoch 24] Loss: 1.1943
[Epoch 25] Loss: 1.1800
[Epoch 26] Loss: 1.1863
[Epoch 27] Loss: 1.1835
[Epoch 28] Loss: 1.1789
[Epoch 29] Loss: 1.1678
[Epoch 30] Loss: 1.1752
Training knowledge distillation model (TransformerClassifier student with teacher external)...
[Distillation Epoch 1] Loss: 2.3151
[Distillation Epoch 2] Loss: 2.1373
[Distillation Epoch 3] Loss: 2.0426
[Distillation Epoch 4] Loss: 1.9459
[Distillation Epoch 5] Loss: 1.8676
[Distillation Epoch 6] Loss: 1.8036
[Distillation Epoch 7] Loss: 1.7150
[Distillation Epoch 8] Loss: 1.7002
[Distillation Epoch 9] Loss: 1.6330
[Distillation Epoch 10] Loss: 1.5646
[Distillation Epoch 11] Loss: 1.5419
[Distillation Epoch 12] Loss: 1.5214
[Distillation Epoch 13] Loss: 1.4881
[Distillation Epoch 14] Loss: 1.4024
[Distillation Epoch 15] Loss: 1.3743
[Distillation Epoch 16] Loss: 1.3420
[Distillation Epoch 17] Loss: 1.3132
[Distillation Epoch 18] Loss: 1.2812
[Distillation Epoch 19] Loss: 1.2853
[Distillation Epoch 20] Loss: 1.2741
[Distillation Epoch 21] Loss: 1.2310
[Distillation Epoch 22] Loss: 1.2251
[Distillation Epoch 23] Loss: 1.1752
[Distillation Epoch 24] Loss: 1.1678
[Distillation Epoch 25] Loss: 1.1376
[Distillation Epoch 26] Loss: 1.1324
[Distillation Epoch 27] Loss: 1.1341
[Distillation Epoch 28] Loss: 1.1049
[Distillation Epoch 29] Loss: 1.1176
[Distillation Epoch 30] Loss: 1.0689

[Run 3 Results]
Baseline:          Acc=45.24% | AUC=0.8668 | F1=0.4517 | MinCAcc=26.70%
Linear Probe:      Acc=53.98% | AUC=0.9090 | F1=0.5343 | MinCAcc=29.50%
Enhanced (Concat): Acc=54.05% | AUC=0.9120 | F1=0.5367 | MinCAcc=27.60%
Baseline Adapter:  Acc=52.85% | AUC=0.9097 | F1=0.5228 | MinCAcc=37.00%
Distillation:      Acc=48.08% | AUC=0.8772 | F1=0.4682 | MinCAcc=13.30%

=== Run 4/5, seed=45 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw_set)...
[Epoch 1] Loss: 2.1373
[Epoch 2] Loss: 2.0445
[Epoch 3] Loss: 1.9920
[Epoch 4] Loss: 1.9357
[Epoch 5] Loss: 1.8814
[Epoch 6] Loss: 1.8463
[Epoch 7] Loss: 1.8036
[Epoch 8] Loss: 1.7674
[Epoch 9] Loss: 1.7198
[Epoch 10] Loss: 1.7082
[Epoch 11] Loss: 1.6904
[Epoch 12] Loss: 1.6530
[Epoch 13] Loss: 1.6786
[Epoch 14] Loss: 1.6071
[Epoch 15] Loss: 1.6005
[Epoch 16] Loss: 1.5677
[Epoch 17] Loss: 1.5401
[Epoch 18] Loss: 1.5137
[Epoch 19] Loss: 1.5046
[Epoch 20] Loss: 1.4847
[Epoch 21] Loss: 1.4504
[Epoch 22] Loss: 1.4524
[Epoch 23] Loss: 1.4159
[Epoch 24] Loss: 1.3681
[Epoch 25] Loss: 1.3805
[Epoch 26] Loss: 1.3429
[Epoch 27] Loss: 1.3339
[Epoch 28] Loss: 1.2870
[Epoch 29] Loss: 1.2688
[Epoch 30] Loss: 1.2863
Training linear probe model (external model fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.3287
[Linear Prob Epoch 2] Loss: 1.2535
[Linear Prob Epoch 3] Loss: 1.2354
[Linear Prob Epoch 4] Loss: 1.2252
[Linear Prob Epoch 5] Loss: 1.2230
[Linear Prob Epoch 6] Loss: 1.2118
[Linear Prob Epoch 7] Loss: 1.2082
[Linear Prob Epoch 8] Loss: 1.2008
[Linear Prob Epoch 9] Loss: 1.2021
[Linear Prob Epoch 10] Loss: 1.1958
[Linear Prob Epoch 11] Loss: 1.1961
[Linear Prob Epoch 12] Loss: 1.1886
[Linear Prob Epoch 13] Loss: 1.1825
[Linear Prob Epoch 14] Loss: 1.1837
[Linear Prob Epoch 15] Loss: 1.1805
[Linear Prob Epoch 16] Loss: 1.1747
[Linear Prob Epoch 17] Loss: 1.1782
[Linear Prob Epoch 18] Loss: 1.1688
[Linear Prob Epoch 19] Loss: 1.1733
[Linear Prob Epoch 20] Loss: 1.1644
[Linear Prob Epoch 21] Loss: 1.1601
[Linear Prob Epoch 22] Loss: 1.1598
[Linear Prob Epoch 23] Loss: 1.1673
[Linear Prob Epoch 24] Loss: 1.1661
[Linear Prob Epoch 25] Loss: 1.1553
[Linear Prob Epoch 26] Loss: 1.1505
[Linear Prob Epoch 27] Loss: 1.1474
[Linear Prob Epoch 28] Loss: 1.1492
[Linear Prob Epoch 29] Loss: 1.1566
[Linear Prob Epoch 30] Loss: 1.1441
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5664
[Enhanced Epoch 2] Loss: 1.2680
[Enhanced Epoch 3] Loss: 1.2179
[Enhanced Epoch 4] Loss: 1.2009
[Enhanced Epoch 5] Loss: 1.1796
[Enhanced Epoch 6] Loss: 1.1710
[Enhanced Epoch 7] Loss: 1.1595
[Enhanced Epoch 8] Loss: 1.1489
[Enhanced Epoch 9] Loss: 1.1475
[Enhanced Epoch 10] Loss: 1.1324
[Enhanced Epoch 11] Loss: 1.1388
[Enhanced Epoch 12] Loss: 1.1314
[Enhanced Epoch 13] Loss: 1.1280
[Enhanced Epoch 14] Loss: 1.1211
[Enhanced Epoch 15] Loss: 1.1116
[Enhanced Epoch 16] Loss: 1.1108
[Enhanced Epoch 17] Loss: 1.1042
[Enhanced Epoch 18] Loss: 1.0979
[Enhanced Epoch 19] Loss: 1.0909
[Enhanced Epoch 20] Loss: 1.0914
[Enhanced Epoch 21] Loss: 1.0876
[Enhanced Epoch 22] Loss: 1.0789
[Enhanced Epoch 23] Loss: 1.0753
[Enhanced Epoch 24] Loss: 1.0723
[Enhanced Epoch 25] Loss: 1.0652
[Enhanced Epoch 26] Loss: 1.0693
[Enhanced Epoch 27] Loss: 1.0619
[Enhanced Epoch 28] Loss: 1.0513
[Enhanced Epoch 29] Loss: 1.0497
[Enhanced Epoch 30] Loss: 1.0504
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.1087
[Epoch 2] Loss: 1.3359
[Epoch 3] Loss: 1.2746
[Epoch 4] Loss: 1.2735
[Epoch 5] Loss: 1.2649
[Epoch 6] Loss: 1.2504
[Epoch 7] Loss: 1.2405
[Epoch 8] Loss: 1.2329
[Epoch 9] Loss: 1.2258
[Epoch 10] Loss: 1.2290
[Epoch 11] Loss: 1.2266
[Epoch 12] Loss: 1.2105
[Epoch 13] Loss: 1.1914
[Epoch 14] Loss: 1.2051
[Epoch 15] Loss: 1.1869
[Epoch 16] Loss: 1.2018
[Epoch 17] Loss: 1.1954
[Epoch 18] Loss: 1.1788
[Epoch 19] Loss: 1.1879
[Epoch 20] Loss: 1.1705
[Epoch 21] Loss: 1.1705
[Epoch 22] Loss: 1.1771
[Epoch 23] Loss: 1.1668
[Epoch 24] Loss: 1.1609
[Epoch 25] Loss: 1.1757
[Epoch 26] Loss: 1.1684
[Epoch 27] Loss: 1.1589
[Epoch 28] Loss: 1.1643
[Epoch 29] Loss: 1.1644
[Epoch 30] Loss: 1.1581
Training knowledge distillation model (TransformerClassifier student with teacher external)...
[Distillation Epoch 1] Loss: 2.2615
[Distillation Epoch 2] Loss: 2.0980
[Distillation Epoch 3] Loss: 2.0242
[Distillation Epoch 4] Loss: 1.9464
[Distillation Epoch 5] Loss: 1.9195
[Distillation Epoch 6] Loss: 1.8056
[Distillation Epoch 7] Loss: 1.7657
[Distillation Epoch 8] Loss: 1.7177
[Distillation Epoch 9] Loss: 1.6656
[Distillation Epoch 10] Loss: 1.6519
[Distillation Epoch 11] Loss: 1.5686
[Distillation Epoch 12] Loss: 1.5235
[Distillation Epoch 13] Loss: 1.4706
[Distillation Epoch 14] Loss: 1.4592
[Distillation Epoch 15] Loss: 1.4111
[Distillation Epoch 16] Loss: 1.3851
[Distillation Epoch 17] Loss: 1.3421
[Distillation Epoch 18] Loss: 1.3284
[Distillation Epoch 19] Loss: 1.2956
[Distillation Epoch 20] Loss: 1.2687
[Distillation Epoch 21] Loss: 1.2597
[Distillation Epoch 22] Loss: 1.2062
[Distillation Epoch 23] Loss: 1.1781
[Distillation Epoch 24] Loss: 1.1760
[Distillation Epoch 25] Loss: 1.1645
[Distillation Epoch 26] Loss: 1.1331
[Distillation Epoch 27] Loss: 1.1173
[Distillation Epoch 28] Loss: 1.1116
[Distillation Epoch 29] Loss: 1.0865
[Distillation Epoch 30] Loss: 1.0991

[Run 4 Results]
Baseline:          Acc=46.26% | AUC=0.8671 | F1=0.4545 | MinCAcc=15.40%
Linear Probe:      Acc=53.11% | AUC=0.9088 | F1=0.5210 | MinCAcc=24.40%
Enhanced (Concat): Acc=54.13% | AUC=0.9139 | F1=0.5375 | MinCAcc=34.20%
Baseline Adapter:  Acc=53.16% | AUC=0.9084 | F1=0.5236 | MinCAcc=30.00%
Distillation:      Acc=44.92% | AUC=0.8721 | F1=0.4287 | MinCAcc=19.00%

=== Run 5/5, seed=46 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline model (TransformerClassifier on raw_set)...
[Epoch 1] Loss: 2.1472
[Epoch 2] Loss: 2.0311
[Epoch 3] Loss: 1.9756
[Epoch 4] Loss: 1.9391
[Epoch 5] Loss: 1.9094
[Epoch 6] Loss: 1.8568
[Epoch 7] Loss: 1.7958
[Epoch 8] Loss: 1.7702
[Epoch 9] Loss: 1.7396
[Epoch 10] Loss: 1.6976
[Epoch 11] Loss: 1.6852
[Epoch 12] Loss: 1.6566
[Epoch 13] Loss: 1.6271
[Epoch 14] Loss: 1.6274
[Epoch 15] Loss: 1.5666
[Epoch 16] Loss: 1.5722
[Epoch 17] Loss: 1.5531
[Epoch 18] Loss: 1.4959
[Epoch 19] Loss: 1.4723
[Epoch 20] Loss: 1.4706
[Epoch 21] Loss: 1.4349
[Epoch 22] Loss: 1.4356
[Epoch 23] Loss: 1.4012
[Epoch 24] Loss: 1.3599
[Epoch 25] Loss: 1.3495
[Epoch 26] Loss: 1.3135
[Epoch 27] Loss: 1.3331
[Epoch 28] Loss: 1.3003
[Epoch 29] Loss: 1.2995
[Epoch 30] Loss: 1.2441
Training linear probe model (external model fine-tuned on raw_set)...
[Linear Prob Epoch 1] Loss: 1.3361
[Linear Prob Epoch 2] Loss: 1.2459
[Linear Prob Epoch 3] Loss: 1.2376
[Linear Prob Epoch 4] Loss: 1.2249
[Linear Prob Epoch 5] Loss: 1.2149
[Linear Prob Epoch 6] Loss: 1.2080
[Linear Prob Epoch 7] Loss: 1.1985
[Linear Prob Epoch 8] Loss: 1.1974
[Linear Prob Epoch 9] Loss: 1.1927
[Linear Prob Epoch 10] Loss: 1.1843
[Linear Prob Epoch 11] Loss: 1.1792
[Linear Prob Epoch 12] Loss: 1.1840
[Linear Prob Epoch 13] Loss: 1.1831
[Linear Prob Epoch 14] Loss: 1.1734
[Linear Prob Epoch 15] Loss: 1.1654
[Linear Prob Epoch 16] Loss: 1.1679
[Linear Prob Epoch 17] Loss: 1.1711
[Linear Prob Epoch 18] Loss: 1.1560
[Linear Prob Epoch 19] Loss: 1.1674
[Linear Prob Epoch 20] Loss: 1.1605
[Linear Prob Epoch 21] Loss: 1.1605
[Linear Prob Epoch 22] Loss: 1.1533
[Linear Prob Epoch 23] Loss: 1.1581
[Linear Prob Epoch 24] Loss: 1.1465
[Linear Prob Epoch 25] Loss: 1.1483
[Linear Prob Epoch 26] Loss: 1.1520
[Linear Prob Epoch 27] Loss: 1.1462
[Linear Prob Epoch 28] Loss: 1.1440
[Linear Prob Epoch 29] Loss: 1.1549
[Linear Prob Epoch 30] Loss: 1.1396
Training enhanced model (concatenation)...
[Enhanced Epoch 1] Loss: 1.5538
[Enhanced Epoch 2] Loss: 1.2621
[Enhanced Epoch 3] Loss: 1.2190
[Enhanced Epoch 4] Loss: 1.1931
[Enhanced Epoch 5] Loss: 1.1797
[Enhanced Epoch 6] Loss: 1.1598
[Enhanced Epoch 7] Loss: 1.1450
[Enhanced Epoch 8] Loss: 1.1417
[Enhanced Epoch 9] Loss: 1.1353
[Enhanced Epoch 10] Loss: 1.1256
[Enhanced Epoch 11] Loss: 1.1155
[Enhanced Epoch 12] Loss: 1.1191
[Enhanced Epoch 13] Loss: 1.1040
[Enhanced Epoch 14] Loss: 1.1082
[Enhanced Epoch 15] Loss: 1.0996
[Enhanced Epoch 16] Loss: 1.0923
[Enhanced Epoch 17] Loss: 1.0847
[Enhanced Epoch 18] Loss: 1.0797
[Enhanced Epoch 19] Loss: 1.0749
[Enhanced Epoch 20] Loss: 1.0697
[Enhanced Epoch 21] Loss: 1.0695
[Enhanced Epoch 22] Loss: 1.0588
[Enhanced Epoch 23] Loss: 1.0572
[Enhanced Epoch 24] Loss: 1.0433
[Enhanced Epoch 25] Loss: 1.0411
[Enhanced Epoch 26] Loss: 1.0383
[Enhanced Epoch 27] Loss: 1.0361
[Enhanced Epoch 28] Loss: 1.0296
[Enhanced Epoch 29] Loss: 1.0218
[Enhanced Epoch 30] Loss: 1.0146
Training baseline adapter model (external frozen with adapter)...
[Epoch 1] Loss: 2.0990
[Epoch 2] Loss: 1.3292
[Epoch 3] Loss: 1.2760
[Epoch 4] Loss: 1.2568
[Epoch 5] Loss: 1.2518
[Epoch 6] Loss: 1.2349
[Epoch 7] Loss: 1.2321
[Epoch 8] Loss: 1.2314
[Epoch 9] Loss: 1.2242
[Epoch 10] Loss: 1.2058
[Epoch 11] Loss: 1.2055
[Epoch 12] Loss: 1.2032
[Epoch 13] Loss: 1.1942
[Epoch 14] Loss: 1.2022
[Epoch 15] Loss: 1.1997
[Epoch 16] Loss: 1.2002
[Epoch 17] Loss: 1.1894
[Epoch 18] Loss: 1.1842
[Epoch 19] Loss: 1.1758
[Epoch 20] Loss: 1.1832
[Epoch 21] Loss: 1.1619
[Epoch 22] Loss: 1.1731
[Epoch 23] Loss: 1.1662
[Epoch 24] Loss: 1.1513
[Epoch 25] Loss: 1.1563
[Epoch 26] Loss: 1.1569
[Epoch 27] Loss: 1.1414
[Epoch 28] Loss: 1.1650
[Epoch 29] Loss: 1.1562
[Epoch 30] Loss: 1.1454
Training knowledge distillation model (TransformerClassifier student with teacher external)...
[Distillation Epoch 1] Loss: 2.3076
[Distillation Epoch 2] Loss: 2.1343
[Distillation Epoch 3] Loss: 2.0234
[Distillation Epoch 4] Loss: 1.9583
[Distillation Epoch 5] Loss: 1.8778
[Distillation Epoch 6] Loss: 1.7916
[Distillation Epoch 7] Loss: 1.7863
[Distillation Epoch 8] Loss: 1.7101
[Distillation Epoch 9] Loss: 1.6542
[Distillation Epoch 10] Loss: 1.6147
[Distillation Epoch 11] Loss: 1.5621
[Distillation Epoch 12] Loss: 1.5420
[Distillation Epoch 13] Loss: 1.4672
[Distillation Epoch 14] Loss: 1.4444
[Distillation Epoch 15] Loss: 1.4052
[Distillation Epoch 16] Loss: 1.3834
[Distillation Epoch 17] Loss: 1.3372
[Distillation Epoch 18] Loss: 1.3169
[Distillation Epoch 19] Loss: 1.2914
[Distillation Epoch 20] Loss: 1.2644
[Distillation Epoch 21] Loss: 1.2832
[Distillation Epoch 22] Loss: 1.2536
[Distillation Epoch 23] Loss: 1.2114
[Distillation Epoch 24] Loss: 1.1795
[Distillation Epoch 25] Loss: 1.1569
[Distillation Epoch 26] Loss: 1.1391
[Distillation Epoch 27] Loss: 1.1292
[Distillation Epoch 28] Loss: 1.0981
[Distillation Epoch 29] Loss: 1.1045
[Distillation Epoch 30] Loss: 1.0793

[Run 5 Results]
Baseline:          Acc=45.66% | AUC=0.8712 | F1=0.4401 | MinCAcc=14.30%
Linear Probe:      Acc=53.89% | AUC=0.9088 | F1=0.5298 | MinCAcc=27.10%
Enhanced (Concat): Acc=54.74% | AUC=0.9124 | F1=0.5423 | MinCAcc=37.30%
Baseline Adapter:  Acc=52.70% | AUC=0.9087 | F1=0.5125 | MinCAcc=22.50%
Distillation:      Acc=45.76% | AUC=0.8767 | F1=0.4306 | MinCAcc=2.80%

All done. Final mean/std results saved to: ./results/adversarial_tf.json
adversarial_tf.py completed successfully.

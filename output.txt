Starting adversarial_tf_test100.py...
Files already downloaded and verified
Files already downloaded and verified

=== Pretraining BigTransformer on adversarial CIFAR-100 ===
Files already downloaded and verified
Files already downloaded and verified
[Epoch 1] Loss: 4.5465
[Epoch 2] Loss: 4.4594
[Epoch 3] Loss: 4.4288
[Epoch 4] Loss: 4.4100
[Epoch 5] Loss: 4.3988
[Epoch 6] Loss: 4.3889
[Epoch 7] Loss: 4.3811
[Epoch 8] Loss: 4.3571
[Epoch 9] Loss: 4.3512
[Epoch 10] Loss: 4.3250
[Epoch 11] Loss: 4.3175
[Epoch 12] Loss: 4.3054
[Epoch 13] Loss: 4.2978
[Epoch 14] Loss: 4.2872
[Epoch 15] Loss: 4.2712
[Epoch 16] Loss: 4.2552
[Epoch 17] Loss: 4.2515
[Epoch 18] Loss: 4.2301
[Epoch 19] Loss: 4.2392
[Epoch 20] Loss: 4.2330
[Epoch 21] Loss: 4.2188
[Epoch 22] Loss: 4.2055
[Epoch 23] Loss: 4.1901
[Epoch 24] Loss: 4.1752
[Epoch 25] Loss: 4.1644
[Epoch 26] Loss: 4.1510
[Epoch 27] Loss: 4.1313
[Epoch 28] Loss: 4.1366
[Epoch 29] Loss: 4.1128
[Epoch 30] Loss: 4.1051
[Epoch 31] Loss: 4.0962
[Epoch 32] Loss: 4.0885
[Epoch 33] Loss: 4.0881
[Epoch 34] Loss: 4.0800
[Epoch 35] Loss: 4.0642
[Epoch 36] Loss: 4.0541
[Epoch 37] Loss: 4.0600
[Epoch 38] Loss: 4.0469
[Epoch 39] Loss: 4.0428
[Epoch 40] Loss: 4.0313
[Epoch 41] Loss: 4.0165
[Epoch 42] Loss: 4.0159
[Epoch 43] Loss: 4.0153
[Epoch 44] Loss: 4.0080
[Epoch 45] Loss: 4.0071
[Epoch 46] Loss: 3.9921
[Epoch 47] Loss: 3.9939
[Epoch 48] Loss: 3.9870
[Epoch 49] Loss: 3.9812
[Epoch 50] Loss: 3.9779
[Epoch 51] Loss: 3.9618
[Epoch 52] Loss: 3.9575
[Epoch 53] Loss: 3.9566
[Epoch 54] Loss: 3.9500
[Epoch 55] Loss: 3.9405
[Epoch 56] Loss: 3.9321
[Epoch 57] Loss: 3.9161
[Epoch 58] Loss: 3.9198
[Epoch 59] Loss: 3.9161
[Epoch 60] Loss: 3.9034
Saved pretrained transformer to: ./model_test100/mismatch_tf.pt
External Transformer Eval: Acc=5.76% | AUC=0.7255 | F1=0.0413 | MinCAcc=0.00%

--- Run 1/5, seed=42 ---
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier...
[Epoch 1] Loss: 4.5059
[Epoch 2] Loss: 4.3367
[Epoch 3] Loss: 4.2559
[Epoch 4] Loss: 4.1815
[Epoch 5] Loss: 4.0959
[Epoch 6] Loss: 4.0265
[Epoch 7] Loss: 3.9514
[Epoch 8] Loss: 3.8886
[Epoch 9] Loss: 3.8108
[Epoch 10] Loss: 3.7574
[Epoch 11] Loss: 3.7145
[Epoch 12] Loss: 3.6775
[Epoch 13] Loss: 3.6431
[Epoch 14] Loss: 3.5649
[Epoch 15] Loss: 3.5181
[Epoch 16] Loss: 3.4820
[Epoch 17] Loss: 3.4517
[Epoch 18] Loss: 3.3864
[Epoch 19] Loss: 3.3446
[Epoch 20] Loss: 3.2993
[Epoch 21] Loss: 3.2248
[Epoch 22] Loss: 3.2204
[Epoch 23] Loss: 3.1588
[Epoch 24] Loss: 3.1318
[Epoch 25] Loss: 3.0658
[Epoch 26] Loss: 3.0335
[Epoch 27] Loss: 3.0119
[Epoch 28] Loss: 2.9878
[Epoch 29] Loss: 2.8905
[Epoch 30] Loss: 2.8720
Training linear probe on classifier head...
[Linear Prob Epoch 1] Loss: 4.2110
[Linear Prob Epoch 2] Loss: 3.9564
[Linear Prob Epoch 3] Loss: 3.8458
[Linear Prob Epoch 4] Loss: 3.7823
[Linear Prob Epoch 5] Loss: 3.7259
[Linear Prob Epoch 6] Loss: 3.6743
[Linear Prob Epoch 7] Loss: 3.6326
[Linear Prob Epoch 8] Loss: 3.5854
[Linear Prob Epoch 9] Loss: 3.5509
[Linear Prob Epoch 10] Loss: 3.5464
[Linear Prob Epoch 11] Loss: 3.5221
[Linear Prob Epoch 12] Loss: 3.4956
[Linear Prob Epoch 13] Loss: 3.4632
[Linear Prob Epoch 14] Loss: 3.4482
[Linear Prob Epoch 15] Loss: 3.4312
[Linear Prob Epoch 16] Loss: 3.4133
[Linear Prob Epoch 17] Loss: 3.3956
[Linear Prob Epoch 18] Loss: 3.3844
[Linear Prob Epoch 19] Loss: 3.3674
[Linear Prob Epoch 20] Loss: 3.3581
[Linear Prob Epoch 21] Loss: 3.3429
[Linear Prob Epoch 22] Loss: 3.3169
[Linear Prob Epoch 23] Loss: 3.3009
[Linear Prob Epoch 24] Loss: 3.3052
[Linear Prob Epoch 25] Loss: 3.2884
[Linear Prob Epoch 26] Loss: 3.2861
[Linear Prob Epoch 27] Loss: 3.2682
[Linear Prob Epoch 28] Loss: 3.2667
[Linear Prob Epoch 29] Loss: 3.2338
[Linear Prob Epoch 30] Loss: 3.2382
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 4.3197
[Enhanced Epoch 2] Loss: 3.9333
[Enhanced Epoch 3] Loss: 3.7867
[Enhanced Epoch 4] Loss: 3.6909
[Enhanced Epoch 5] Loss: 3.6301
[Enhanced Epoch 6] Loss: 3.5622
[Enhanced Epoch 7] Loss: 3.5089
[Enhanced Epoch 8] Loss: 3.4629
[Enhanced Epoch 9] Loss: 3.4219
[Enhanced Epoch 10] Loss: 3.3804
[Enhanced Epoch 11] Loss: 3.3268
[Enhanced Epoch 12] Loss: 3.2859
[Enhanced Epoch 13] Loss: 3.2413
[Enhanced Epoch 14] Loss: 3.1946
[Enhanced Epoch 15] Loss: 3.1503
[Enhanced Epoch 16] Loss: 3.1082
[Enhanced Epoch 17] Loss: 3.0625
[Enhanced Epoch 18] Loss: 3.0164
[Enhanced Epoch 19] Loss: 2.9745
[Enhanced Epoch 20] Loss: 2.9400
[Enhanced Epoch 21] Loss: 2.8857
[Enhanced Epoch 22] Loss: 2.8463
[Enhanced Epoch 23] Loss: 2.7954
[Enhanced Epoch 24] Loss: 2.7520
[Enhanced Epoch 25] Loss: 2.7057
[Enhanced Epoch 26] Loss: 2.6809
[Enhanced Epoch 27] Loss: 2.6398
[Enhanced Epoch 28] Loss: 2.6008
[Enhanced Epoch 29] Loss: 2.5425
[Enhanced Epoch 30] Loss: 2.5185
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5973
[Epoch 2] Loss: 4.4825
[Epoch 3] Loss: 4.2802
[Epoch 4] Loss: 4.1427
[Epoch 5] Loss: 4.0494
[Epoch 6] Loss: 4.0061
[Epoch 7] Loss: 3.9480
[Epoch 8] Loss: 3.9183
[Epoch 9] Loss: 3.8583
[Epoch 10] Loss: 3.8502
[Epoch 11] Loss: 3.8327
[Epoch 12] Loss: 3.8311
[Epoch 13] Loss: 3.7844
[Epoch 14] Loss: 3.7701
[Epoch 15] Loss: 3.7576
[Epoch 16] Loss: 3.7580
[Epoch 17] Loss: 3.7567
[Epoch 18] Loss: 3.7139
[Epoch 19] Loss: 3.6925
[Epoch 20] Loss: 3.6825
[Epoch 21] Loss: 3.6820
[Epoch 22] Loss: 3.6672
[Epoch 23] Loss: 3.6803
[Epoch 24] Loss: 3.6606
[Epoch 25] Loss: 3.6282
[Epoch 26] Loss: 3.6099
[Epoch 27] Loss: 3.6150
[Epoch 28] Loss: 3.6116
[Epoch 29] Loss: 3.5949
[Epoch 30] Loss: 3.5795
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 2.6666
[Distillation Epoch 2] Loss: 2.5256
[Distillation Epoch 3] Loss: 2.4644
[Distillation Epoch 4] Loss: 2.4072
[Distillation Epoch 5] Loss: 2.3495
[Distillation Epoch 6] Loss: 2.3047
[Distillation Epoch 7] Loss: 2.2749
[Distillation Epoch 8] Loss: 2.2510
[Distillation Epoch 9] Loss: 2.2255
[Distillation Epoch 10] Loss: 2.2007
[Distillation Epoch 11] Loss: 2.1837
[Distillation Epoch 12] Loss: 2.1601
[Distillation Epoch 13] Loss: 2.1395
[Distillation Epoch 14] Loss: 2.1248
[Distillation Epoch 15] Loss: 2.1086
[Distillation Epoch 16] Loss: 2.0991
[Distillation Epoch 17] Loss: 2.0861
[Distillation Epoch 18] Loss: 2.0733
[Distillation Epoch 19] Loss: 2.0606
[Distillation Epoch 20] Loss: 2.0499
[Distillation Epoch 21] Loss: 2.0330
[Distillation Epoch 22] Loss: 2.0284
[Distillation Epoch 23] Loss: 2.0093
[Distillation Epoch 24] Loss: 2.0051
[Distillation Epoch 25] Loss: 1.9935
[Distillation Epoch 26] Loss: 1.9833
[Distillation Epoch 27] Loss: 1.9705
[Distillation Epoch 28] Loss: 1.9634
[Distillation Epoch 29] Loss: 1.9582
[Distillation Epoch 30] Loss: 1.9547
[Run 1 results] Base=16.83% | LP=11.55% | ENH=14.32% | ADP=10.48% | DIST=16.01%

--- Run 2/5, seed=43 ---
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier...
[Epoch 1] Loss: 4.5032
[Epoch 2] Loss: 4.3130
[Epoch 3] Loss: 4.2105
[Epoch 4] Loss: 4.1474
[Epoch 5] Loss: 4.0660
[Epoch 6] Loss: 3.9884
[Epoch 7] Loss: 3.9202
[Epoch 8] Loss: 3.8597
[Epoch 9] Loss: 3.7999
[Epoch 10] Loss: 3.7600
[Epoch 11] Loss: 3.6893
[Epoch 12] Loss: 3.6353
[Epoch 13] Loss: 3.5864
[Epoch 14] Loss: 3.5384
[Epoch 15] Loss: 3.4913
[Epoch 16] Loss: 3.4313
[Epoch 17] Loss: 3.3959
[Epoch 18] Loss: 3.3287
[Epoch 19] Loss: 3.2855
[Epoch 20] Loss: 3.2642
[Epoch 21] Loss: 3.1981
[Epoch 22] Loss: 3.1555
[Epoch 23] Loss: 3.1133
[Epoch 24] Loss: 3.0561
[Epoch 25] Loss: 3.0240
[Epoch 26] Loss: 2.9703
[Epoch 27] Loss: 2.9246
[Epoch 28] Loss: 2.9014
[Epoch 29] Loss: 2.8743
[Epoch 30] Loss: 2.7975
Training linear probe on classifier head...
[Linear Prob Epoch 1] Loss: 4.1865
[Linear Prob Epoch 2] Loss: 3.9146
[Linear Prob Epoch 3] Loss: 3.8017
[Linear Prob Epoch 4] Loss: 3.7148
[Linear Prob Epoch 5] Loss: 3.6584
[Linear Prob Epoch 6] Loss: 3.6224
[Linear Prob Epoch 7] Loss: 3.5851
[Linear Prob Epoch 8] Loss: 3.5354
[Linear Prob Epoch 9] Loss: 3.5233
[Linear Prob Epoch 10] Loss: 3.4949
[Linear Prob Epoch 11] Loss: 3.4667
[Linear Prob Epoch 12] Loss: 3.4456
[Linear Prob Epoch 13] Loss: 3.4293
[Linear Prob Epoch 14] Loss: 3.4007
[Linear Prob Epoch 15] Loss: 3.3820
[Linear Prob Epoch 16] Loss: 3.3772
[Linear Prob Epoch 17] Loss: 3.3564
[Linear Prob Epoch 18] Loss: 3.3330
[Linear Prob Epoch 19] Loss: 3.3246
[Linear Prob Epoch 20] Loss: 3.2996
[Linear Prob Epoch 21] Loss: 3.2906
[Linear Prob Epoch 22] Loss: 3.2735
[Linear Prob Epoch 23] Loss: 3.2722
[Linear Prob Epoch 24] Loss: 3.2496
[Linear Prob Epoch 25] Loss: 3.2250
[Linear Prob Epoch 26] Loss: 3.2349
[Linear Prob Epoch 27] Loss: 3.2071
[Linear Prob Epoch 28] Loss: 3.2001
[Linear Prob Epoch 29] Loss: 3.2021
[Linear Prob Epoch 30] Loss: 3.1748
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 4.3017
[Enhanced Epoch 2] Loss: 3.9032
[Enhanced Epoch 3] Loss: 3.7617
[Enhanced Epoch 4] Loss: 3.6607
[Enhanced Epoch 5] Loss: 3.5896
[Enhanced Epoch 6] Loss: 3.5381
[Enhanced Epoch 7] Loss: 3.4906
[Enhanced Epoch 8] Loss: 3.4381
[Enhanced Epoch 9] Loss: 3.3887
[Enhanced Epoch 10] Loss: 3.3418
[Enhanced Epoch 11] Loss: 3.3130
[Enhanced Epoch 12] Loss: 3.2729
[Enhanced Epoch 13] Loss: 3.2244
[Enhanced Epoch 14] Loss: 3.1907
[Enhanced Epoch 15] Loss: 3.1432
[Enhanced Epoch 16] Loss: 3.1158
[Enhanced Epoch 17] Loss: 3.0723
[Enhanced Epoch 18] Loss: 3.0182
[Enhanced Epoch 19] Loss: 2.9800
[Enhanced Epoch 20] Loss: 2.9369
[Enhanced Epoch 21] Loss: 2.8996
[Enhanced Epoch 22] Loss: 2.8532
[Enhanced Epoch 23] Loss: 2.8269
[Enhanced Epoch 24] Loss: 2.7684
[Enhanced Epoch 25] Loss: 2.7329
[Enhanced Epoch 26] Loss: 2.6945
[Enhanced Epoch 27] Loss: 2.6552
[Enhanced Epoch 28] Loss: 2.6133
[Enhanced Epoch 29] Loss: 2.5895
[Enhanced Epoch 30] Loss: 2.5330
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5963
[Epoch 2] Loss: 4.4725
[Epoch 3] Loss: 4.2381
[Epoch 4] Loss: 4.1003
[Epoch 5] Loss: 4.0093
[Epoch 6] Loss: 3.9695
[Epoch 7] Loss: 3.9449
[Epoch 8] Loss: 3.8824
[Epoch 9] Loss: 3.8444
[Epoch 10] Loss: 3.8350
[Epoch 11] Loss: 3.8221
[Epoch 12] Loss: 3.8017
[Epoch 13] Loss: 3.7756
[Epoch 14] Loss: 3.7626
[Epoch 15] Loss: 3.7477
[Epoch 16] Loss: 3.7235
[Epoch 17] Loss: 3.6926
[Epoch 18] Loss: 3.7013
[Epoch 19] Loss: 3.6921
[Epoch 20] Loss: 3.6715
[Epoch 21] Loss: 3.6636
[Epoch 22] Loss: 3.6475
[Epoch 23] Loss: 3.6169
[Epoch 24] Loss: 3.6123
[Epoch 25] Loss: 3.6041
[Epoch 26] Loss: 3.5779
[Epoch 27] Loss: 3.5857
[Epoch 28] Loss: 3.5497
[Epoch 29] Loss: 3.5616
[Epoch 30] Loss: 3.5575
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 2.6674
[Distillation Epoch 2] Loss: 2.5272
[Distillation Epoch 3] Loss: 2.4665
[Distillation Epoch 4] Loss: 2.4152
[Distillation Epoch 5] Loss: 2.3447
[Distillation Epoch 6] Loss: 2.2969
[Distillation Epoch 7] Loss: 2.2657
[Distillation Epoch 8] Loss: 2.2414
[Distillation Epoch 9] Loss: 2.2148
[Distillation Epoch 10] Loss: 2.1876
[Distillation Epoch 11] Loss: 2.1743
[Distillation Epoch 12] Loss: 2.1626
[Distillation Epoch 13] Loss: 2.1385
[Distillation Epoch 14] Loss: 2.1291
[Distillation Epoch 15] Loss: 2.1081
[Distillation Epoch 16] Loss: 2.1031
[Distillation Epoch 17] Loss: 2.0781
[Distillation Epoch 18] Loss: 2.0655
[Distillation Epoch 19] Loss: 2.0524
[Distillation Epoch 20] Loss: 2.0460
[Distillation Epoch 21] Loss: 2.0336
[Distillation Epoch 22] Loss: 2.0250
[Distillation Epoch 23] Loss: 2.0057
[Distillation Epoch 24] Loss: 1.9958
[Distillation Epoch 25] Loss: 1.9890
[Distillation Epoch 26] Loss: 1.9782
[Distillation Epoch 27] Loss: 1.9586
[Distillation Epoch 28] Loss: 1.9558
[Distillation Epoch 29] Loss: 1.9399
[Distillation Epoch 30] Loss: 1.9316
[Run 2 results] Base=16.80% | LP=11.88% | ENH=14.55% | ADP=11.52% | DIST=15.07%

--- Run 3/5, seed=44 ---
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier...
[Epoch 1] Loss: 4.4895
[Epoch 2] Loss: 4.2941
[Epoch 3] Loss: 4.2123
[Epoch 4] Loss: 4.1345
[Epoch 5] Loss: 4.0643
[Epoch 6] Loss: 3.9883
[Epoch 7] Loss: 3.9290
[Epoch 8] Loss: 3.8794
[Epoch 9] Loss: 3.8110
[Epoch 10] Loss: 3.7711
[Epoch 11] Loss: 3.7304
[Epoch 12] Loss: 3.6567
[Epoch 13] Loss: 3.6266
[Epoch 14] Loss: 3.5663
[Epoch 15] Loss: 3.5118
[Epoch 16] Loss: 3.4679
[Epoch 17] Loss: 3.4347
[Epoch 18] Loss: 3.3918
[Epoch 19] Loss: 3.3415
[Epoch 20] Loss: 3.2738
[Epoch 21] Loss: 3.2497
[Epoch 22] Loss: 3.1908
[Epoch 23] Loss: 3.1535
[Epoch 24] Loss: 3.0907
[Epoch 25] Loss: 3.0754
[Epoch 26] Loss: 3.0256
[Epoch 27] Loss: 2.9830
[Epoch 28] Loss: 2.9496
[Epoch 29] Loss: 2.8911
[Epoch 30] Loss: 2.8359
Training linear probe on classifier head...
[Linear Prob Epoch 1] Loss: 4.1919
[Linear Prob Epoch 2] Loss: 3.9349
[Linear Prob Epoch 3] Loss: 3.8261
[Linear Prob Epoch 4] Loss: 3.7384
[Linear Prob Epoch 5] Loss: 3.6951
[Linear Prob Epoch 6] Loss: 3.6593
[Linear Prob Epoch 7] Loss: 3.6083
[Linear Prob Epoch 8] Loss: 3.5922
[Linear Prob Epoch 9] Loss: 3.5552
[Linear Prob Epoch 10] Loss: 3.5250
[Linear Prob Epoch 11] Loss: 3.4938
[Linear Prob Epoch 12] Loss: 3.4749
[Linear Prob Epoch 13] Loss: 3.4479
[Linear Prob Epoch 14] Loss: 3.4282
[Linear Prob Epoch 15] Loss: 3.4139
[Linear Prob Epoch 16] Loss: 3.3993
[Linear Prob Epoch 17] Loss: 3.3826
[Linear Prob Epoch 18] Loss: 3.3625
[Linear Prob Epoch 19] Loss: 3.3528
[Linear Prob Epoch 20] Loss: 3.3266
[Linear Prob Epoch 21] Loss: 3.3191
[Linear Prob Epoch 22] Loss: 3.3248
[Linear Prob Epoch 23] Loss: 3.2886
[Linear Prob Epoch 24] Loss: 3.2922
[Linear Prob Epoch 25] Loss: 3.2708
[Linear Prob Epoch 26] Loss: 3.2601
[Linear Prob Epoch 27] Loss: 3.2491
[Linear Prob Epoch 28] Loss: 3.2276
[Linear Prob Epoch 29] Loss: 3.2315
[Linear Prob Epoch 30] Loss: 3.2292
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 4.3118
[Enhanced Epoch 2] Loss: 3.9308
[Enhanced Epoch 3] Loss: 3.7798
[Enhanced Epoch 4] Loss: 3.6921
[Enhanced Epoch 5] Loss: 3.6199
[Enhanced Epoch 6] Loss: 3.5652
[Enhanced Epoch 7] Loss: 3.5086
[Enhanced Epoch 8] Loss: 3.4728
[Enhanced Epoch 9] Loss: 3.4302
[Enhanced Epoch 10] Loss: 3.3960
[Enhanced Epoch 11] Loss: 3.3488
[Enhanced Epoch 12] Loss: 3.3018
[Enhanced Epoch 13] Loss: 3.2649
[Enhanced Epoch 14] Loss: 3.2231
[Enhanced Epoch 15] Loss: 3.1897
[Enhanced Epoch 16] Loss: 3.1448
[Enhanced Epoch 17] Loss: 3.1025
[Enhanced Epoch 18] Loss: 3.0634
[Enhanced Epoch 19] Loss: 3.0132
[Enhanced Epoch 20] Loss: 2.9755
[Enhanced Epoch 21] Loss: 2.9229
[Enhanced Epoch 22] Loss: 2.8770
[Enhanced Epoch 23] Loss: 2.8438
[Enhanced Epoch 24] Loss: 2.7901
[Enhanced Epoch 25] Loss: 2.7434
[Enhanced Epoch 26] Loss: 2.7119
[Enhanced Epoch 27] Loss: 2.6513
[Enhanced Epoch 28] Loss: 2.6137
[Enhanced Epoch 29] Loss: 2.5675
[Enhanced Epoch 30] Loss: 2.5343
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.6008
[Epoch 2] Loss: 4.4926
[Epoch 3] Loss: 4.2857
[Epoch 4] Loss: 4.1325
[Epoch 5] Loss: 4.0475
[Epoch 6] Loss: 3.9751
[Epoch 7] Loss: 3.9540
[Epoch 8] Loss: 3.9178
[Epoch 9] Loss: 3.8952
[Epoch 10] Loss: 3.8679
[Epoch 11] Loss: 3.8360
[Epoch 12] Loss: 3.8173
[Epoch 13] Loss: 3.7915
[Epoch 14] Loss: 3.7858
[Epoch 15] Loss: 3.7748
[Epoch 16] Loss: 3.7668
[Epoch 17] Loss: 3.7364
[Epoch 18] Loss: 3.7173
[Epoch 19] Loss: 3.7137
[Epoch 20] Loss: 3.7115
[Epoch 21] Loss: 3.6950
[Epoch 22] Loss: 3.6704
[Epoch 23] Loss: 3.6573
[Epoch 24] Loss: 3.6447
[Epoch 25] Loss: 3.6351
[Epoch 26] Loss: 3.6156
[Epoch 27] Loss: 3.6028
[Epoch 28] Loss: 3.5946
[Epoch 29] Loss: 3.5871
[Epoch 30] Loss: 3.5783
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 2.6632
[Distillation Epoch 2] Loss: 2.5173
[Distillation Epoch 3] Loss: 2.4541
[Distillation Epoch 4] Loss: 2.4032
[Distillation Epoch 5] Loss: 2.3516
[Distillation Epoch 6] Loss: 2.3131
[Distillation Epoch 7] Loss: 2.2874
[Distillation Epoch 8] Loss: 2.2555
[Distillation Epoch 9] Loss: 2.2359
[Distillation Epoch 10] Loss: 2.2120
[Distillation Epoch 11] Loss: 2.1887
[Distillation Epoch 12] Loss: 2.1681
[Distillation Epoch 13] Loss: 2.1573
[Distillation Epoch 14] Loss: 2.1365
[Distillation Epoch 15] Loss: 2.1251
[Distillation Epoch 16] Loss: 2.1093
[Distillation Epoch 17] Loss: 2.0999
[Distillation Epoch 18] Loss: 2.0825
[Distillation Epoch 19] Loss: 2.0718
[Distillation Epoch 20] Loss: 2.0627
[Distillation Epoch 21] Loss: 2.0521
[Distillation Epoch 22] Loss: 2.0357
[Distillation Epoch 23] Loss: 2.0236
[Distillation Epoch 24] Loss: 2.0158
[Distillation Epoch 25] Loss: 2.0111
[Distillation Epoch 26] Loss: 2.0009
[Distillation Epoch 27] Loss: 1.9836
[Distillation Epoch 28] Loss: 1.9817
[Distillation Epoch 29] Loss: 1.9620
[Distillation Epoch 30] Loss: 1.9534
[Run 3 results] Base=15.46% | LP=12.04% | ENH=14.94% | ADP=11.01% | DIST=15.29%

--- Run 4/5, seed=45 ---
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier...
[Epoch 1] Loss: 4.5024
[Epoch 2] Loss: 4.3348
[Epoch 3] Loss: 4.2369
[Epoch 4] Loss: 4.1696
[Epoch 5] Loss: 4.1172
[Epoch 6] Loss: 4.0292
[Epoch 7] Loss: 3.9666
[Epoch 8] Loss: 3.8955
[Epoch 9] Loss: 3.8449
[Epoch 10] Loss: 3.7882
[Epoch 11] Loss: 3.7514
[Epoch 12] Loss: 3.7006
[Epoch 13] Loss: 3.6549
[Epoch 14] Loss: 3.6039
[Epoch 15] Loss: 3.5488
[Epoch 16] Loss: 3.4965
[Epoch 17] Loss: 3.4605
[Epoch 18] Loss: 3.4137
[Epoch 19] Loss: 3.3836
[Epoch 20] Loss: 3.3364
[Epoch 21] Loss: 3.2570
[Epoch 22] Loss: 3.2711
[Epoch 23] Loss: 3.2008
[Epoch 24] Loss: 3.1753
[Epoch 25] Loss: 3.1068
[Epoch 26] Loss: 3.0981
[Epoch 27] Loss: 3.0655
[Epoch 28] Loss: 2.9771
[Epoch 29] Loss: 2.9579
[Epoch 30] Loss: 2.9032
Training linear probe on classifier head...
[Linear Prob Epoch 1] Loss: 4.1905
[Linear Prob Epoch 2] Loss: 3.9264
[Linear Prob Epoch 3] Loss: 3.8212
[Linear Prob Epoch 4] Loss: 3.7514
[Linear Prob Epoch 5] Loss: 3.6809
[Linear Prob Epoch 6] Loss: 3.6472
[Linear Prob Epoch 7] Loss: 3.6066
[Linear Prob Epoch 8] Loss: 3.5763
[Linear Prob Epoch 9] Loss: 3.5474
[Linear Prob Epoch 10] Loss: 3.5258
[Linear Prob Epoch 11] Loss: 3.4943
[Linear Prob Epoch 12] Loss: 3.4680
[Linear Prob Epoch 13] Loss: 3.4608
[Linear Prob Epoch 14] Loss: 3.4427
[Linear Prob Epoch 15] Loss: 3.4195
[Linear Prob Epoch 16] Loss: 3.3985
[Linear Prob Epoch 17] Loss: 3.3904
[Linear Prob Epoch 18] Loss: 3.3764
[Linear Prob Epoch 19] Loss: 3.3478
[Linear Prob Epoch 20] Loss: 3.3360
[Linear Prob Epoch 21] Loss: 3.3394
[Linear Prob Epoch 22] Loss: 3.3109
[Linear Prob Epoch 23] Loss: 3.3116
[Linear Prob Epoch 24] Loss: 3.2953
[Linear Prob Epoch 25] Loss: 3.2677
[Linear Prob Epoch 26] Loss: 3.2660
[Linear Prob Epoch 27] Loss: 3.2499
[Linear Prob Epoch 28] Loss: 3.2438
[Linear Prob Epoch 29] Loss: 3.2351
[Linear Prob Epoch 30] Loss: 3.2282
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 4.3113
[Enhanced Epoch 2] Loss: 3.9374
[Enhanced Epoch 3] Loss: 3.7887
[Enhanced Epoch 4] Loss: 3.6911
[Enhanced Epoch 5] Loss: 3.6259
[Enhanced Epoch 6] Loss: 3.5609
[Enhanced Epoch 7] Loss: 3.5107
[Enhanced Epoch 8] Loss: 3.4581
[Enhanced Epoch 9] Loss: 3.4211
[Enhanced Epoch 10] Loss: 3.3865
[Enhanced Epoch 11] Loss: 3.3484
[Enhanced Epoch 12] Loss: 3.3134
[Enhanced Epoch 13] Loss: 3.2780
[Enhanced Epoch 14] Loss: 3.2329
[Enhanced Epoch 15] Loss: 3.1899
[Enhanced Epoch 16] Loss: 3.1481
[Enhanced Epoch 17] Loss: 3.1090
[Enhanced Epoch 18] Loss: 3.0735
[Enhanced Epoch 19] Loss: 3.0319
[Enhanced Epoch 20] Loss: 2.9813
[Enhanced Epoch 21] Loss: 2.9560
[Enhanced Epoch 22] Loss: 2.9042
[Enhanced Epoch 23] Loss: 2.8490
[Enhanced Epoch 24] Loss: 2.8127
[Enhanced Epoch 25] Loss: 2.7676
[Enhanced Epoch 26] Loss: 2.7394
[Enhanced Epoch 27] Loss: 2.6801
[Enhanced Epoch 28] Loss: 2.6370
[Enhanced Epoch 29] Loss: 2.6133
[Enhanced Epoch 30] Loss: 2.5679
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5976
[Epoch 2] Loss: 4.4732
[Epoch 3] Loss: 4.2524
[Epoch 4] Loss: 4.1411
[Epoch 5] Loss: 4.0514
[Epoch 6] Loss: 3.9986
[Epoch 7] Loss: 3.9535
[Epoch 8] Loss: 3.9271
[Epoch 9] Loss: 3.9118
[Epoch 10] Loss: 3.8719
[Epoch 11] Loss: 3.8593
[Epoch 12] Loss: 3.8227
[Epoch 13] Loss: 3.8206
[Epoch 14] Loss: 3.7860
[Epoch 15] Loss: 3.7665
[Epoch 16] Loss: 3.7512
[Epoch 17] Loss: 3.7408
[Epoch 18] Loss: 3.7397
[Epoch 19] Loss: 3.6948
[Epoch 20] Loss: 3.6991
[Epoch 21] Loss: 3.7041
[Epoch 22] Loss: 3.6725
[Epoch 23] Loss: 3.6558
[Epoch 24] Loss: 3.6493
[Epoch 25] Loss: 3.6287
[Epoch 26] Loss: 3.6216
[Epoch 27] Loss: 3.6141
[Epoch 28] Loss: 3.6147
[Epoch 29] Loss: 3.6086
[Epoch 30] Loss: 3.5830
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 2.6519
[Distillation Epoch 2] Loss: 2.5161
[Distillation Epoch 3] Loss: 2.4494
[Distillation Epoch 4] Loss: 2.3785
[Distillation Epoch 5] Loss: 2.3299
[Distillation Epoch 6] Loss: 2.2917
[Distillation Epoch 7] Loss: 2.2701
[Distillation Epoch 8] Loss: 2.2414
[Distillation Epoch 9] Loss: 2.2161
[Distillation Epoch 10] Loss: 2.2021
[Distillation Epoch 11] Loss: 2.1895
[Distillation Epoch 12] Loss: 2.1616
[Distillation Epoch 13] Loss: 2.1521
[Distillation Epoch 14] Loss: 2.1347
[Distillation Epoch 15] Loss: 2.1216
[Distillation Epoch 16] Loss: 2.1164
[Distillation Epoch 17] Loss: 2.1004
[Distillation Epoch 18] Loss: 2.0781
[Distillation Epoch 19] Loss: 2.0731
[Distillation Epoch 20] Loss: 2.0604
[Distillation Epoch 21] Loss: 2.0416
[Distillation Epoch 22] Loss: 2.0391
[Distillation Epoch 23] Loss: 2.0315
[Distillation Epoch 24] Loss: 2.0192
[Distillation Epoch 25] Loss: 2.0130
[Distillation Epoch 26] Loss: 2.0042
[Distillation Epoch 27] Loss: 1.9907
[Distillation Epoch 28] Loss: 1.9781
[Distillation Epoch 29] Loss: 1.9694
[Distillation Epoch 30] Loss: 1.9555
[Run 4 results] Base=15.64% | LP=12.32% | ENH=15.17% | ADP=11.73% | DIST=14.45%

--- Run 5/5, seed=46 ---
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier...
[Epoch 1] Loss: 4.5241
[Epoch 2] Loss: 4.3393
[Epoch 3] Loss: 4.2434
[Epoch 4] Loss: 4.1691
[Epoch 5] Loss: 4.0824
[Epoch 6] Loss: 3.9936
[Epoch 7] Loss: 3.9498
[Epoch 8] Loss: 3.8843
[Epoch 9] Loss: 3.8250
[Epoch 10] Loss: 3.7934
[Epoch 11] Loss: 3.7162
[Epoch 12] Loss: 3.6638
[Epoch 13] Loss: 3.6410
[Epoch 14] Loss: 3.5806
[Epoch 15] Loss: 3.5307
[Epoch 16] Loss: 3.4783
[Epoch 17] Loss: 3.4516
[Epoch 18] Loss: 3.4029
[Epoch 19] Loss: 3.3461
[Epoch 20] Loss: 3.3014
[Epoch 21] Loss: 3.2849
[Epoch 22] Loss: 3.2258
[Epoch 23] Loss: 3.1755
[Epoch 24] Loss: 3.1251
[Epoch 25] Loss: 3.0926
[Epoch 26] Loss: 3.0581
[Epoch 27] Loss: 3.0103
[Epoch 28] Loss: 2.9680
[Epoch 29] Loss: 2.9655
[Epoch 30] Loss: 2.8972
Training linear probe on classifier head...
[Linear Prob Epoch 1] Loss: 4.1832
[Linear Prob Epoch 2] Loss: 3.9224
[Linear Prob Epoch 3] Loss: 3.8081
[Linear Prob Epoch 4] Loss: 3.7441
[Linear Prob Epoch 5] Loss: 3.6945
[Linear Prob Epoch 6] Loss: 3.6398
[Linear Prob Epoch 7] Loss: 3.6115
[Linear Prob Epoch 8] Loss: 3.5898
[Linear Prob Epoch 9] Loss: 3.5436
[Linear Prob Epoch 10] Loss: 3.5137
[Linear Prob Epoch 11] Loss: 3.4989
[Linear Prob Epoch 12] Loss: 3.4851
[Linear Prob Epoch 13] Loss: 3.4483
[Linear Prob Epoch 14] Loss: 3.4359
[Linear Prob Epoch 15] Loss: 3.4156
[Linear Prob Epoch 16] Loss: 3.3999
[Linear Prob Epoch 17] Loss: 3.3609
[Linear Prob Epoch 18] Loss: 3.3596
[Linear Prob Epoch 19] Loss: 3.3521
[Linear Prob Epoch 20] Loss: 3.3296
[Linear Prob Epoch 21] Loss: 3.3254
[Linear Prob Epoch 22] Loss: 3.3082
[Linear Prob Epoch 23] Loss: 3.3065
[Linear Prob Epoch 24] Loss: 3.2830
[Linear Prob Epoch 25] Loss: 3.2819
[Linear Prob Epoch 26] Loss: 3.2580
[Linear Prob Epoch 27] Loss: 3.2607
[Linear Prob Epoch 28] Loss: 3.2457
[Linear Prob Epoch 29] Loss: 3.2377
[Linear Prob Epoch 30] Loss: 3.2346
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 4.3258
[Enhanced Epoch 2] Loss: 3.9269
[Enhanced Epoch 3] Loss: 3.7758
[Enhanced Epoch 4] Loss: 3.6844
[Enhanced Epoch 5] Loss: 3.6111
[Enhanced Epoch 6] Loss: 3.5466
[Enhanced Epoch 7] Loss: 3.5024
[Enhanced Epoch 8] Loss: 3.4528
[Enhanced Epoch 9] Loss: 3.4011
[Enhanced Epoch 10] Loss: 3.3704
[Enhanced Epoch 11] Loss: 3.3274
[Enhanced Epoch 12] Loss: 3.2838
[Enhanced Epoch 13] Loss: 3.2398
[Enhanced Epoch 14] Loss: 3.1999
[Enhanced Epoch 15] Loss: 3.1656
[Enhanced Epoch 16] Loss: 3.1147
[Enhanced Epoch 17] Loss: 3.0795
[Enhanced Epoch 18] Loss: 3.0415
[Enhanced Epoch 19] Loss: 2.9840
[Enhanced Epoch 20] Loss: 2.9317
[Enhanced Epoch 21] Loss: 2.8846
[Enhanced Epoch 22] Loss: 2.8465
[Enhanced Epoch 23] Loss: 2.7977
[Enhanced Epoch 24] Loss: 2.7618
[Enhanced Epoch 25] Loss: 2.7008
[Enhanced Epoch 26] Loss: 2.6533
[Enhanced Epoch 27] Loss: 2.6102
[Enhanced Epoch 28] Loss: 2.5792
[Enhanced Epoch 29] Loss: 2.5155
[Enhanced Epoch 30] Loss: 2.4729
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.6000
[Epoch 2] Loss: 4.4960
[Epoch 3] Loss: 4.2996
[Epoch 4] Loss: 4.1579
[Epoch 5] Loss: 4.0634
[Epoch 6] Loss: 4.0011
[Epoch 7] Loss: 3.9687
[Epoch 8] Loss: 3.9297
[Epoch 9] Loss: 3.8771
[Epoch 10] Loss: 3.8815
[Epoch 11] Loss: 3.8457
[Epoch 12] Loss: 3.8007
[Epoch 13] Loss: 3.7947
[Epoch 14] Loss: 3.7879
[Epoch 15] Loss: 3.7759
[Epoch 16] Loss: 3.7499
[Epoch 17] Loss: 3.7366
[Epoch 18] Loss: 3.7035
[Epoch 19] Loss: 3.7210
[Epoch 20] Loss: 3.6931
[Epoch 21] Loss: 3.6736
[Epoch 22] Loss: 3.6640
[Epoch 23] Loss: 3.6494
[Epoch 24] Loss: 3.6401
[Epoch 25] Loss: 3.6122
[Epoch 26] Loss: 3.6424
[Epoch 27] Loss: 3.5994
[Epoch 28] Loss: 3.6322
[Epoch 29] Loss: 3.5872
[Epoch 30] Loss: 3.5651
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 2.6562
[Distillation Epoch 2] Loss: 2.5150
[Distillation Epoch 3] Loss: 2.4506
[Distillation Epoch 4] Loss: 2.4047
[Distillation Epoch 5] Loss: 2.3407
[Distillation Epoch 6] Loss: 2.2975
[Distillation Epoch 7] Loss: 2.2783
[Distillation Epoch 8] Loss: 2.2474
[Distillation Epoch 9] Loss: 2.2213
[Distillation Epoch 10] Loss: 2.2026
[Distillation Epoch 11] Loss: 2.1802
[Distillation Epoch 12] Loss: 2.1567
[Distillation Epoch 13] Loss: 2.1511
[Distillation Epoch 14] Loss: 2.1317
[Distillation Epoch 15] Loss: 2.1217
[Distillation Epoch 16] Loss: 2.1019
[Distillation Epoch 17] Loss: 2.0832
[Distillation Epoch 18] Loss: 2.0794
[Distillation Epoch 19] Loss: 2.0622
[Distillation Epoch 20] Loss: 2.0614
[Distillation Epoch 21] Loss: 2.0426
[Distillation Epoch 22] Loss: 2.0345
[Distillation Epoch 23] Loss: 2.0256
[Distillation Epoch 24] Loss: 2.0196
[Distillation Epoch 25] Loss: 2.0084
[Distillation Epoch 26] Loss: 1.9919
[Distillation Epoch 27] Loss: 1.9844
[Distillation Epoch 28] Loss: 1.9728
[Distillation Epoch 29] Loss: 1.9636
[Distillation Epoch 30] Loss: 1.9558
[Run 5 results] Base=16.45% | LP=11.63% | ENH=15.71% | ADP=11.12% | DIST=14.21%

All done. Final mean/std results saved to: ./results_test100/adversarial_tf_cifar100_confusion.json
adversarial_tf_test100.py completed successfully.
Starting noise_tf_test100.py...
Files already downloaded and verified
Files already downloaded and verified
[Epoch 1] Loss: 4.3749
[Epoch 2] Loss: 4.1236
[Epoch 3] Loss: 3.9588
[Epoch 4] Loss: 3.8389
[Epoch 5] Loss: 3.7358
[Epoch 6] Loss: 3.6385
[Epoch 7] Loss: 3.5200
[Epoch 8] Loss: 3.4413
[Epoch 9] Loss: 3.3494
[Epoch 10] Loss: 3.2645
[Epoch 11] Loss: 3.1891
[Epoch 12] Loss: 3.0820
[Epoch 13] Loss: 2.9946
[Epoch 14] Loss: 2.8964
[Epoch 15] Loss: 2.8010
[Epoch 16] Loss: 2.7066
[Epoch 17] Loss: 2.6322
[Epoch 18] Loss: 2.5368
[Epoch 19] Loss: 2.4390
[Epoch 20] Loss: 2.3135
[Epoch 21] Loss: 2.2152
[Epoch 22] Loss: 2.1598
[Epoch 23] Loss: 2.0437
[Epoch 24] Loss: 1.9539
[Epoch 25] Loss: 1.8518
[Epoch 26] Loss: 1.7913
[Epoch 27] Loss: 1.6760
[Epoch 28] Loss: 1.5651
[Epoch 29] Loss: 1.4925
[Epoch 30] Loss: 1.3700
[Epoch 31] Loss: 1.2963
[Epoch 32] Loss: 1.2014
[Epoch 33] Loss: 1.0813
[Epoch 34] Loss: 1.0566
[Epoch 35] Loss: 0.9713
[Epoch 36] Loss: 0.9219
[Epoch 37] Loss: 0.8602
[Epoch 38] Loss: 0.7787
[Epoch 39] Loss: 0.6892
[Epoch 40] Loss: 0.6463
[Epoch 41] Loss: 0.6745
[Epoch 42] Loss: 0.6537
[Epoch 43] Loss: 0.5692
[Epoch 44] Loss: 0.5270
[Epoch 45] Loss: 0.4839
[Epoch 46] Loss: 0.4571
[Epoch 47] Loss: 0.4640
[Epoch 48] Loss: 0.4381
[Epoch 49] Loss: 0.4107
[Epoch 50] Loss: 0.3684
[Epoch 51] Loss: 0.3515
[Epoch 52] Loss: 0.3320
[Epoch 53] Loss: 0.3318
[Epoch 54] Loss: 0.3428
[Epoch 55] Loss: 0.2945
[Epoch 56] Loss: 0.3217
[Epoch 57] Loss: 0.3278
[Epoch 58] Loss: 0.2909
[Epoch 59] Loss: 0.2473
[Epoch 60] Loss: 0.2568
Trained and saved external transformer to: ./model_test100/noise_tf_cifar100_0.pt
External Transformer Eval: Acc=25.73% | AUC=0.8800 | F1=0.2561 | MinCAcc=5.00%

=== Run 1/5, raw-set seed=42 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier on raw set...
[Epoch 1] Loss: 4.5470
[Epoch 2] Loss: 4.3579
[Epoch 3] Loss: 4.2321
[Epoch 4] Loss: 4.1821
[Epoch 5] Loss: 4.1359
[Epoch 6] Loss: 4.0900
[Epoch 7] Loss: 4.0623
[Epoch 8] Loss: 4.0088
[Epoch 9] Loss: 3.9657
[Epoch 10] Loss: 3.9157
[Epoch 11] Loss: 3.8538
[Epoch 12] Loss: 3.7898
[Epoch 13] Loss: 3.7625
[Epoch 14] Loss: 3.7286
[Epoch 15] Loss: 3.6719
[Epoch 16] Loss: 3.6437
[Epoch 17] Loss: 3.5947
[Epoch 18] Loss: 3.5460
[Epoch 19] Loss: 3.5241
[Epoch 20] Loss: 3.4773
[Epoch 21] Loss: 3.4325
[Epoch 22] Loss: 3.3588
[Epoch 23] Loss: 3.3812
[Epoch 24] Loss: 3.3033
[Epoch 25] Loss: 3.2846
[Epoch 26] Loss: 3.2536
[Epoch 27] Loss: 3.1909
[Epoch 28] Loss: 3.1804
[Epoch 29] Loss: 3.1239
[Epoch 30] Loss: 3.0900
Training linear probe (freeze all but classifier)...
[Linear Prob Epoch 1] Loss: 5.0565
[Linear Prob Epoch 2] Loss: 4.0871
[Linear Prob Epoch 3] Loss: 3.5030
[Linear Prob Epoch 4] Loss: 3.0614
[Linear Prob Epoch 5] Loss: 2.6641
[Linear Prob Epoch 6] Loss: 2.3776
[Linear Prob Epoch 7] Loss: 2.1571
[Linear Prob Epoch 8] Loss: 1.9598
[Linear Prob Epoch 9] Loss: 1.7918
[Linear Prob Epoch 10] Loss: 1.6562
[Linear Prob Epoch 11] Loss: 1.5310
[Linear Prob Epoch 12] Loss: 1.4135
[Linear Prob Epoch 13] Loss: 1.2996
[Linear Prob Epoch 14] Loss: 1.2249
[Linear Prob Epoch 15] Loss: 1.1365
[Linear Prob Epoch 16] Loss: 1.1080
[Linear Prob Epoch 17] Loss: 1.0299
[Linear Prob Epoch 18] Loss: 0.9862
[Linear Prob Epoch 19] Loss: 0.9645
[Linear Prob Epoch 20] Loss: 0.9008
[Linear Prob Epoch 21] Loss: 0.8263
[Linear Prob Epoch 22] Loss: 0.7857
[Linear Prob Epoch 23] Loss: 0.8011
[Linear Prob Epoch 24] Loss: 0.7373
[Linear Prob Epoch 25] Loss: 0.7473
[Linear Prob Epoch 26] Loss: 0.6708
[Linear Prob Epoch 27] Loss: 0.6668
[Linear Prob Epoch 28] Loss: 0.6399
[Linear Prob Epoch 29] Loss: 0.6186
[Linear Prob Epoch 30] Loss: 0.6116
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 3.8230
[Enhanced Epoch 2] Loss: 2.5280
[Enhanced Epoch 3] Loss: 2.0392
[Enhanced Epoch 4] Loss: 1.7001
[Enhanced Epoch 5] Loss: 1.4332
[Enhanced Epoch 6] Loss: 1.2214
[Enhanced Epoch 7] Loss: 1.0421
[Enhanced Epoch 8] Loss: 0.8959
[Enhanced Epoch 9] Loss: 0.7746
[Enhanced Epoch 10] Loss: 0.6677
[Enhanced Epoch 11] Loss: 0.5824
[Enhanced Epoch 12] Loss: 0.5125
[Enhanced Epoch 13] Loss: 0.4550
[Enhanced Epoch 14] Loss: 0.4040
[Enhanced Epoch 15] Loss: 0.3628
[Enhanced Epoch 16] Loss: 0.3294
[Enhanced Epoch 17] Loss: 0.2994
[Enhanced Epoch 18] Loss: 0.2713
[Enhanced Epoch 19] Loss: 0.2509
[Enhanced Epoch 20] Loss: 0.2323
[Enhanced Epoch 21] Loss: 0.2158
[Enhanced Epoch 22] Loss: 0.2005
[Enhanced Epoch 23] Loss: 0.1882
[Enhanced Epoch 24] Loss: 0.1777
[Enhanced Epoch 25] Loss: 0.1660
[Enhanced Epoch 26] Loss: 0.1576
[Enhanced Epoch 27] Loss: 0.1502
[Enhanced Epoch 28] Loss: 0.1425
[Enhanced Epoch 29] Loss: 0.1356
[Enhanced Epoch 30] Loss: 0.1295
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.6015
[Epoch 2] Loss: 4.3993
[Epoch 3] Loss: 3.9257
[Epoch 4] Loss: 3.6107
[Epoch 5] Loss: 3.3941
[Epoch 6] Loss: 3.2652
[Epoch 7] Loss: 3.1726
[Epoch 8] Loss: 3.0993
[Epoch 9] Loss: 3.0160
[Epoch 10] Loss: 2.9349
[Epoch 11] Loss: 2.8766
[Epoch 12] Loss: 2.8195
[Epoch 13] Loss: 2.7722
[Epoch 14] Loss: 2.7384
[Epoch 15] Loss: 2.6582
[Epoch 16] Loss: 2.6365
[Epoch 17] Loss: 2.6323
[Epoch 18] Loss: 2.5922
[Epoch 19] Loss: 2.5423
[Epoch 20] Loss: 2.4977
[Epoch 21] Loss: 2.4888
[Epoch 22] Loss: 2.4540
[Epoch 23] Loss: 2.4267
[Epoch 24] Loss: 2.3966
[Epoch 25] Loss: 2.3741
[Epoch 26] Loss: 2.3509
[Epoch 27] Loss: 2.3247
[Epoch 28] Loss: 2.2483
[Epoch 29] Loss: 2.2870
[Epoch 30] Loss: 2.2534
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 6.7575
[Distillation Epoch 2] Loss: 6.3021
[Distillation Epoch 3] Loss: 6.1171
[Distillation Epoch 4] Loss: 5.9857
[Distillation Epoch 5] Loss: 5.8566
[Distillation Epoch 6] Loss: 5.6897
[Distillation Epoch 7] Loss: 5.5490
[Distillation Epoch 8] Loss: 5.4325
[Distillation Epoch 9] Loss: 5.2976
[Distillation Epoch 10] Loss: 5.1863
[Distillation Epoch 11] Loss: 5.0873
[Distillation Epoch 12] Loss: 5.0226
[Distillation Epoch 13] Loss: 4.9242
[Distillation Epoch 14] Loss: 4.8555
[Distillation Epoch 15] Loss: 4.7696
[Distillation Epoch 16] Loss: 4.7251
[Distillation Epoch 17] Loss: 4.6169
[Distillation Epoch 18] Loss: 4.5191
[Distillation Epoch 19] Loss: 4.4709
[Distillation Epoch 20] Loss: 4.4507
[Distillation Epoch 21] Loss: 4.3668
[Distillation Epoch 22] Loss: 4.3371
[Distillation Epoch 23] Loss: 4.2635
[Distillation Epoch 24] Loss: 4.1747
[Distillation Epoch 25] Loss: 4.1641
[Distillation Epoch 26] Loss: 4.1340
[Distillation Epoch 27] Loss: 4.0755
[Distillation Epoch 28] Loss: 4.0448
[Distillation Epoch 29] Loss: 3.9806
[Distillation Epoch 30] Loss: 3.9499
[Run 1 Results] Base=15.44% | LP=26.00% | ENH=25.11% | ADP=24.04% | DIST=18.74%

=== Run 2/5, raw-set seed=43 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier on raw set...
[Epoch 1] Loss: 4.5504
[Epoch 2] Loss: 4.3530
[Epoch 3] Loss: 4.2411
[Epoch 4] Loss: 4.1812
[Epoch 5] Loss: 4.1398
[Epoch 6] Loss: 4.0965
[Epoch 7] Loss: 4.0598
[Epoch 8] Loss: 4.0095
[Epoch 9] Loss: 3.9539
[Epoch 10] Loss: 3.9302
[Epoch 11] Loss: 3.8486
[Epoch 12] Loss: 3.8056
[Epoch 13] Loss: 3.7490
[Epoch 14] Loss: 3.7001
[Epoch 15] Loss: 3.6753
[Epoch 16] Loss: 3.6273
[Epoch 17] Loss: 3.5995
[Epoch 18] Loss: 3.5560
[Epoch 19] Loss: 3.5076
[Epoch 20] Loss: 3.4847
[Epoch 21] Loss: 3.4361
[Epoch 22] Loss: 3.4059
[Epoch 23] Loss: 3.3689
[Epoch 24] Loss: 3.3226
[Epoch 25] Loss: 3.2723
[Epoch 26] Loss: 3.2561
[Epoch 27] Loss: 3.2296
[Epoch 28] Loss: 3.1956
[Epoch 29] Loss: 3.1394
[Epoch 30] Loss: 3.1257
Training linear probe (freeze all but classifier)...
[Linear Prob Epoch 1] Loss: 4.0997
[Linear Prob Epoch 2] Loss: 3.3486
[Linear Prob Epoch 3] Loss: 2.8280
[Linear Prob Epoch 4] Loss: 2.5056
[Linear Prob Epoch 5] Loss: 2.2054
[Linear Prob Epoch 6] Loss: 2.0493
[Linear Prob Epoch 7] Loss: 1.7941
[Linear Prob Epoch 8] Loss: 1.6366
[Linear Prob Epoch 9] Loss: 1.5177
[Linear Prob Epoch 10] Loss: 1.3251
[Linear Prob Epoch 11] Loss: 1.2684
[Linear Prob Epoch 12] Loss: 1.1659
[Linear Prob Epoch 13] Loss: 1.0824
[Linear Prob Epoch 14] Loss: 1.0381
[Linear Prob Epoch 15] Loss: 0.9744
[Linear Prob Epoch 16] Loss: 0.9108
[Linear Prob Epoch 17] Loss: 0.8564
[Linear Prob Epoch 18] Loss: 0.8087
[Linear Prob Epoch 19] Loss: 0.7824
[Linear Prob Epoch 20] Loss: 0.7348
[Linear Prob Epoch 21] Loss: 0.6857
[Linear Prob Epoch 22] Loss: 0.6786
[Linear Prob Epoch 23] Loss: 0.6348
[Linear Prob Epoch 24] Loss: 0.6240
[Linear Prob Epoch 25] Loss: 0.5852
[Linear Prob Epoch 26] Loss: 0.5786
[Linear Prob Epoch 27] Loss: 0.5703
[Linear Prob Epoch 28] Loss: 0.5210
[Linear Prob Epoch 29] Loss: 0.5368
[Linear Prob Epoch 30] Loss: 0.5002
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 3.6960
[Enhanced Epoch 2] Loss: 2.3120
[Enhanced Epoch 3] Loss: 1.8303
[Enhanced Epoch 4] Loss: 1.5044
[Enhanced Epoch 5] Loss: 1.2593
[Enhanced Epoch 6] Loss: 1.0584
[Enhanced Epoch 7] Loss: 0.8981
[Enhanced Epoch 8] Loss: 0.7650
[Enhanced Epoch 9] Loss: 0.6554
[Enhanced Epoch 10] Loss: 0.5660
[Enhanced Epoch 11] Loss: 0.4879
[Enhanced Epoch 12] Loss: 0.4275
[Enhanced Epoch 13] Loss: 0.3756
[Enhanced Epoch 14] Loss: 0.3374
[Enhanced Epoch 15] Loss: 0.3017
[Enhanced Epoch 16] Loss: 0.2712
[Enhanced Epoch 17] Loss: 0.2466
[Enhanced Epoch 18] Loss: 0.2268
[Enhanced Epoch 19] Loss: 0.2089
[Enhanced Epoch 20] Loss: 0.1929
[Enhanced Epoch 21] Loss: 0.1800
[Enhanced Epoch 22] Loss: 0.1678
[Enhanced Epoch 23] Loss: 0.1579
[Enhanced Epoch 24] Loss: 0.1483
[Enhanced Epoch 25] Loss: 0.1394
[Enhanced Epoch 26] Loss: 0.1327
[Enhanced Epoch 27] Loss: 0.1264
[Enhanced Epoch 28] Loss: 0.1199
[Enhanced Epoch 29] Loss: 0.1141
[Enhanced Epoch 30] Loss: 0.1094
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5972
[Epoch 2] Loss: 4.3390
[Epoch 3] Loss: 3.8388
[Epoch 4] Loss: 3.4487
[Epoch 5] Loss: 3.2685
[Epoch 6] Loss: 3.1138
[Epoch 7] Loss: 3.0365
[Epoch 8] Loss: 2.9241
[Epoch 9] Loss: 2.8130
[Epoch 10] Loss: 2.7368
[Epoch 11] Loss: 2.7028
[Epoch 12] Loss: 2.6260
[Epoch 13] Loss: 2.5740
[Epoch 14] Loss: 2.5044
[Epoch 15] Loss: 2.4891
[Epoch 16] Loss: 2.4157
[Epoch 17] Loss: 2.4139
[Epoch 18] Loss: 2.4190
[Epoch 19] Loss: 2.3376
[Epoch 20] Loss: 2.3049
[Epoch 21] Loss: 2.2468
[Epoch 22] Loss: 2.3086
[Epoch 23] Loss: 2.2268
[Epoch 24] Loss: 2.2093
[Epoch 25] Loss: 2.1756
[Epoch 26] Loss: 2.1461
[Epoch 27] Loss: 2.1666
[Epoch 28] Loss: 2.1484
[Epoch 29] Loss: 2.0710
[Epoch 30] Loss: 2.0581
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 7.2649
[Distillation Epoch 2] Loss: 6.7782
[Distillation Epoch 3] Loss: 6.5769
[Distillation Epoch 4] Loss: 6.4248
[Distillation Epoch 5] Loss: 6.2632
[Distillation Epoch 6] Loss: 6.1354
[Distillation Epoch 7] Loss: 6.0014
[Distillation Epoch 8] Loss: 5.8986
[Distillation Epoch 9] Loss: 5.7742
[Distillation Epoch 10] Loss: 5.6837
[Distillation Epoch 11] Loss: 5.5496
[Distillation Epoch 12] Loss: 5.4887
[Distillation Epoch 13] Loss: 5.3366
[Distillation Epoch 14] Loss: 5.2632
[Distillation Epoch 15] Loss: 5.1971
[Distillation Epoch 16] Loss: 5.1355
[Distillation Epoch 17] Loss: 5.0542
[Distillation Epoch 18] Loss: 4.9572
[Distillation Epoch 19] Loss: 4.8503
[Distillation Epoch 20] Loss: 4.8339
[Distillation Epoch 21] Loss: 4.7937
[Distillation Epoch 22] Loss: 4.7106
[Distillation Epoch 23] Loss: 4.6416
[Distillation Epoch 24] Loss: 4.5870
[Distillation Epoch 25] Loss: 4.5258
[Distillation Epoch 26] Loss: 4.4625
[Distillation Epoch 27] Loss: 4.3943
[Distillation Epoch 28] Loss: 4.3969
[Distillation Epoch 29] Loss: 4.2956
[Distillation Epoch 30] Loss: 4.2458
[Run 2 Results] Base=14.68% | LP=26.42% | ENH=26.12% | ADP=25.25% | DIST=19.21%

=== Run 3/5, raw-set seed=44 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier on raw set...
[Epoch 1] Loss: 4.5477
[Epoch 2] Loss: 4.3606
[Epoch 3] Loss: 4.2699
[Epoch 4] Loss: 4.1961
[Epoch 5] Loss: 4.1507
[Epoch 6] Loss: 4.1123
[Epoch 7] Loss: 4.0769
[Epoch 8] Loss: 4.0303
[Epoch 9] Loss: 3.9738
[Epoch 10] Loss: 3.9356
[Epoch 11] Loss: 3.8774
[Epoch 12] Loss: 3.8373
[Epoch 13] Loss: 3.7933
[Epoch 14] Loss: 3.7639
[Epoch 15] Loss: 3.7021
[Epoch 16] Loss: 3.6700
[Epoch 17] Loss: 3.6240
[Epoch 18] Loss: 3.5843
[Epoch 19] Loss: 3.5312
[Epoch 20] Loss: 3.5005
[Epoch 21] Loss: 3.4517
[Epoch 22] Loss: 3.4293
[Epoch 23] Loss: 3.3869
[Epoch 24] Loss: 3.3367
[Epoch 25] Loss: 3.3092
[Epoch 26] Loss: 3.2480
[Epoch 27] Loss: 3.2138
[Epoch 28] Loss: 3.1972
[Epoch 29] Loss: 3.1460
[Epoch 30] Loss: 3.1073
Training linear probe (freeze all but classifier)...
[Linear Prob Epoch 1] Loss: 4.1557
[Linear Prob Epoch 2] Loss: 3.4212
[Linear Prob Epoch 3] Loss: 2.9295
[Linear Prob Epoch 4] Loss: 2.5878
[Linear Prob Epoch 5] Loss: 2.2895
[Linear Prob Epoch 6] Loss: 2.0383
[Linear Prob Epoch 7] Loss: 1.8434
[Linear Prob Epoch 8] Loss: 1.6622
[Linear Prob Epoch 9] Loss: 1.5029
[Linear Prob Epoch 10] Loss: 1.3940
[Linear Prob Epoch 11] Loss: 1.3107
[Linear Prob Epoch 12] Loss: 1.1660
[Linear Prob Epoch 13] Loss: 1.1210
[Linear Prob Epoch 14] Loss: 1.0426
[Linear Prob Epoch 15] Loss: 0.9590
[Linear Prob Epoch 16] Loss: 0.8953
[Linear Prob Epoch 17] Loss: 0.8715
[Linear Prob Epoch 18] Loss: 0.8023
[Linear Prob Epoch 19] Loss: 0.7816
[Linear Prob Epoch 20] Loss: 0.7373
[Linear Prob Epoch 21] Loss: 0.7117
[Linear Prob Epoch 22] Loss: 0.6599
[Linear Prob Epoch 23] Loss: 0.6508
[Linear Prob Epoch 24] Loss: 0.6304
[Linear Prob Epoch 25] Loss: 0.6118
[Linear Prob Epoch 26] Loss: 0.5596
[Linear Prob Epoch 27] Loss: 0.5525
[Linear Prob Epoch 28] Loss: 0.5420
[Linear Prob Epoch 29] Loss: 0.5069
[Linear Prob Epoch 30] Loss: 0.4887
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 3.6931
[Enhanced Epoch 2] Loss: 2.3284
[Enhanced Epoch 3] Loss: 1.8272
[Enhanced Epoch 4] Loss: 1.5086
[Enhanced Epoch 5] Loss: 1.2679
[Enhanced Epoch 6] Loss: 1.0670
[Enhanced Epoch 7] Loss: 0.9096
[Enhanced Epoch 8] Loss: 0.7774
[Enhanced Epoch 9] Loss: 0.6620
[Enhanced Epoch 10] Loss: 0.5724
[Enhanced Epoch 11] Loss: 0.4992
[Enhanced Epoch 12] Loss: 0.4339
[Enhanced Epoch 13] Loss: 0.3834
[Enhanced Epoch 14] Loss: 0.3409
[Enhanced Epoch 15] Loss: 0.3041
[Enhanced Epoch 16] Loss: 0.2762
[Enhanced Epoch 17] Loss: 0.2506
[Enhanced Epoch 18] Loss: 0.2287
[Enhanced Epoch 19] Loss: 0.2101
[Enhanced Epoch 20] Loss: 0.1949
[Enhanced Epoch 21] Loss: 0.1811
[Enhanced Epoch 22] Loss: 0.1675
[Enhanced Epoch 23] Loss: 0.1593
[Enhanced Epoch 24] Loss: 0.1492
[Enhanced Epoch 25] Loss: 0.1408
[Enhanced Epoch 26] Loss: 0.1337
[Enhanced Epoch 27] Loss: 0.1267
[Enhanced Epoch 28] Loss: 0.1203
[Enhanced Epoch 29] Loss: 0.1148
[Enhanced Epoch 30] Loss: 0.1093
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5929
[Epoch 2] Loss: 4.3190
[Epoch 3] Loss: 3.8278
[Epoch 4] Loss: 3.4642
[Epoch 5] Loss: 3.2803
[Epoch 6] Loss: 3.1357
[Epoch 7] Loss: 3.0172
[Epoch 8] Loss: 2.9082
[Epoch 9] Loss: 2.8445
[Epoch 10] Loss: 2.7827
[Epoch 11] Loss: 2.7000
[Epoch 12] Loss: 2.6701
[Epoch 13] Loss: 2.6101
[Epoch 14] Loss: 2.5297
[Epoch 15] Loss: 2.4810
[Epoch 16] Loss: 2.4599
[Epoch 17] Loss: 2.4262
[Epoch 18] Loss: 2.3850
[Epoch 19] Loss: 2.3300
[Epoch 20] Loss: 2.3131
[Epoch 21] Loss: 2.3156
[Epoch 22] Loss: 2.2722
[Epoch 23] Loss: 2.2533
[Epoch 24] Loss: 2.2286
[Epoch 25] Loss: 2.2179
[Epoch 26] Loss: 2.1766
[Epoch 27] Loss: 2.1754
[Epoch 28] Loss: 2.1258
[Epoch 29] Loss: 2.1061
[Epoch 30] Loss: 2.1115
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 7.2554
[Distillation Epoch 2] Loss: 6.7846
[Distillation Epoch 3] Loss: 6.5763
[Distillation Epoch 4] Loss: 6.4661
[Distillation Epoch 5] Loss: 6.3081
[Distillation Epoch 6] Loss: 6.1501
[Distillation Epoch 7] Loss: 5.9762
[Distillation Epoch 8] Loss: 5.8714
[Distillation Epoch 9] Loss: 5.7332
[Distillation Epoch 10] Loss: 5.6039
[Distillation Epoch 11] Loss: 5.5130
[Distillation Epoch 12] Loss: 5.4340
[Distillation Epoch 13] Loss: 5.2934
[Distillation Epoch 14] Loss: 5.1853
[Distillation Epoch 15] Loss: 5.1360
[Distillation Epoch 16] Loss: 5.0723
[Distillation Epoch 17] Loss: 4.9850
[Distillation Epoch 18] Loss: 4.9318
[Distillation Epoch 19] Loss: 4.8852
[Distillation Epoch 20] Loss: 4.7707
[Distillation Epoch 21] Loss: 4.7214
[Distillation Epoch 22] Loss: 4.6496
[Distillation Epoch 23] Loss: 4.6304
[Distillation Epoch 24] Loss: 4.5742
[Distillation Epoch 25] Loss: 4.5460
[Distillation Epoch 26] Loss: 4.4586
[Distillation Epoch 27] Loss: 4.4348
[Distillation Epoch 28] Loss: 4.3501
[Distillation Epoch 29] Loss: 4.3066
[Distillation Epoch 30] Loss: 4.2738
[Run 3 Results] Base=15.41% | LP=26.21% | ENH=26.03% | ADP=25.35% | DIST=19.13%

=== Run 4/5, raw-set seed=45 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier on raw set...
[Epoch 1] Loss: 4.5486
[Epoch 2] Loss: 4.3421
[Epoch 3] Loss: 4.2413
[Epoch 4] Loss: 4.1768
[Epoch 5] Loss: 4.1403
[Epoch 6] Loss: 4.0958
[Epoch 7] Loss: 4.0436
[Epoch 8] Loss: 3.9850
[Epoch 9] Loss: 3.9345
[Epoch 10] Loss: 3.9028
[Epoch 11] Loss: 3.8377
[Epoch 12] Loss: 3.7900
[Epoch 13] Loss: 3.7354
[Epoch 14] Loss: 3.6960
[Epoch 15] Loss: 3.6542
[Epoch 16] Loss: 3.5953
[Epoch 17] Loss: 3.5894
[Epoch 18] Loss: 3.5327
[Epoch 19] Loss: 3.5104
[Epoch 20] Loss: 3.4536
[Epoch 21] Loss: 3.4401
[Epoch 22] Loss: 3.4091
[Epoch 23] Loss: 3.3805
[Epoch 24] Loss: 3.3291
[Epoch 25] Loss: 3.2800
[Epoch 26] Loss: 3.2570
[Epoch 27] Loss: 3.2066
[Epoch 28] Loss: 3.1686
[Epoch 29] Loss: 3.1424
[Epoch 30] Loss: 3.1182
Training linear probe (freeze all but classifier)...
[Linear Prob Epoch 1] Loss: 4.1473
[Linear Prob Epoch 2] Loss: 3.4088
[Linear Prob Epoch 3] Loss: 2.9419
[Linear Prob Epoch 4] Loss: 2.5547
[Linear Prob Epoch 5] Loss: 2.2600
[Linear Prob Epoch 6] Loss: 2.0021
[Linear Prob Epoch 7] Loss: 1.7928
[Linear Prob Epoch 8] Loss: 1.6465
[Linear Prob Epoch 9] Loss: 1.5300
[Linear Prob Epoch 10] Loss: 1.3865
[Linear Prob Epoch 11] Loss: 1.2865
[Linear Prob Epoch 12] Loss: 1.1967
[Linear Prob Epoch 13] Loss: 1.1063
[Linear Prob Epoch 14] Loss: 1.0299
[Linear Prob Epoch 15] Loss: 0.9678
[Linear Prob Epoch 16] Loss: 0.9160
[Linear Prob Epoch 17] Loss: 0.8845
[Linear Prob Epoch 18] Loss: 0.8281
[Linear Prob Epoch 19] Loss: 0.7909
[Linear Prob Epoch 20] Loss: 0.7176
[Linear Prob Epoch 21] Loss: 0.7050
[Linear Prob Epoch 22] Loss: 0.6776
[Linear Prob Epoch 23] Loss: 0.6353
[Linear Prob Epoch 24] Loss: 0.6234
[Linear Prob Epoch 25] Loss: 0.5841
[Linear Prob Epoch 26] Loss: 0.5827
[Linear Prob Epoch 27] Loss: 0.5454
[Linear Prob Epoch 28] Loss: 0.5376
[Linear Prob Epoch 29] Loss: 0.5173
[Linear Prob Epoch 30] Loss: 0.5039
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 3.7082
[Enhanced Epoch 2] Loss: 2.3177
[Enhanced Epoch 3] Loss: 1.8374
[Enhanced Epoch 4] Loss: 1.5112
[Enhanced Epoch 5] Loss: 1.2588
[Enhanced Epoch 6] Loss: 1.0715
[Enhanced Epoch 7] Loss: 0.9003
[Enhanced Epoch 8] Loss: 0.7690
[Enhanced Epoch 9] Loss: 0.6556
[Enhanced Epoch 10] Loss: 0.5665
[Enhanced Epoch 11] Loss: 0.4939
[Enhanced Epoch 12] Loss: 0.4333
[Enhanced Epoch 13] Loss: 0.3805
[Enhanced Epoch 14] Loss: 0.3384
[Enhanced Epoch 15] Loss: 0.3045
[Enhanced Epoch 16] Loss: 0.2750
[Enhanced Epoch 17] Loss: 0.2507
[Enhanced Epoch 18] Loss: 0.2315
[Enhanced Epoch 19] Loss: 0.2125
[Enhanced Epoch 20] Loss: 0.1970
[Enhanced Epoch 21] Loss: 0.1830
[Enhanced Epoch 22] Loss: 0.1706
[Enhanced Epoch 23] Loss: 0.1606
[Enhanced Epoch 24] Loss: 0.1508
[Enhanced Epoch 25] Loss: 0.1416
[Enhanced Epoch 26] Loss: 0.1351
[Enhanced Epoch 27] Loss: 0.1277
[Enhanced Epoch 28] Loss: 0.1211
[Enhanced Epoch 29] Loss: 0.1157
[Enhanced Epoch 30] Loss: 0.1112
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5936
[Epoch 2] Loss: 4.3319
[Epoch 3] Loss: 3.8129
[Epoch 4] Loss: 3.4808
[Epoch 5] Loss: 3.2749
[Epoch 6] Loss: 3.1155
[Epoch 7] Loss: 2.9722
[Epoch 8] Loss: 2.9435
[Epoch 9] Loss: 2.8655
[Epoch 10] Loss: 2.7724
[Epoch 11] Loss: 2.6896
[Epoch 12] Loss: 2.6373
[Epoch 13] Loss: 2.6046
[Epoch 14] Loss: 2.5724
[Epoch 15] Loss: 2.5058
[Epoch 16] Loss: 2.4702
[Epoch 17] Loss: 2.3986
[Epoch 18] Loss: 2.3968
[Epoch 19] Loss: 2.3465
[Epoch 20] Loss: 2.3706
[Epoch 21] Loss: 2.2809
[Epoch 22] Loss: 2.2917
[Epoch 23] Loss: 2.2395
[Epoch 24] Loss: 2.2191
[Epoch 25] Loss: 2.2425
[Epoch 26] Loss: 2.1270
[Epoch 27] Loss: 2.1067
[Epoch 28] Loss: 2.1160
[Epoch 29] Loss: 2.1039
[Epoch 30] Loss: 2.0938
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 7.2085
[Distillation Epoch 2] Loss: 6.7440
[Distillation Epoch 3] Loss: 6.5506
[Distillation Epoch 4] Loss: 6.4246
[Distillation Epoch 5] Loss: 6.2673
[Distillation Epoch 6] Loss: 6.1126
[Distillation Epoch 7] Loss: 5.9452
[Distillation Epoch 8] Loss: 5.8287
[Distillation Epoch 9] Loss: 5.6819
[Distillation Epoch 10] Loss: 5.5733
[Distillation Epoch 11] Loss: 5.4650
[Distillation Epoch 12] Loss: 5.4137
[Distillation Epoch 13] Loss: 5.2880
[Distillation Epoch 14] Loss: 5.2125
[Distillation Epoch 15] Loss: 5.1551
[Distillation Epoch 16] Loss: 5.0847
[Distillation Epoch 17] Loss: 4.9493
[Distillation Epoch 18] Loss: 4.9175
[Distillation Epoch 19] Loss: 4.8495
[Distillation Epoch 20] Loss: 4.7777
[Distillation Epoch 21] Loss: 4.6968
[Distillation Epoch 22] Loss: 4.6225
[Distillation Epoch 23] Loss: 4.5810
[Distillation Epoch 24] Loss: 4.5692
[Distillation Epoch 25] Loss: 4.5098
[Distillation Epoch 26] Loss: 4.4551
[Distillation Epoch 27] Loss: 4.3967
[Distillation Epoch 28] Loss: 4.3456
[Distillation Epoch 29] Loss: 4.2820
[Distillation Epoch 30] Loss: 4.2939
[Run 4 Results] Base=15.66% | LP=26.16% | ENH=25.71% | ADP=24.61% | DIST=18.76%

=== Run 5/5, raw-set seed=46 ===
Files already downloaded and verified
Files already downloaded and verified
Training baseline TransformerClassifier on raw set...
[Epoch 1] Loss: 4.5431
[Epoch 2] Loss: 4.3469
[Epoch 3] Loss: 4.2566
[Epoch 4] Loss: 4.1902
[Epoch 5] Loss: 4.1529
[Epoch 6] Loss: 4.0979
[Epoch 7] Loss: 4.0580
[Epoch 8] Loss: 3.9984
[Epoch 9] Loss: 3.9645
[Epoch 10] Loss: 3.8892
[Epoch 11] Loss: 3.8425
[Epoch 12] Loss: 3.7584
[Epoch 13] Loss: 3.7117
[Epoch 14] Loss: 3.6903
[Epoch 15] Loss: 3.6388
[Epoch 16] Loss: 3.5932
[Epoch 17] Loss: 3.5385
[Epoch 18] Loss: 3.5170
[Epoch 19] Loss: 3.4507
[Epoch 20] Loss: 3.4198
[Epoch 21] Loss: 3.3872
[Epoch 22] Loss: 3.3305
[Epoch 23] Loss: 3.2989
[Epoch 24] Loss: 3.2590
[Epoch 25] Loss: 3.2113
[Epoch 26] Loss: 3.1744
[Epoch 27] Loss: 3.1357
[Epoch 28] Loss: 3.0928
[Epoch 29] Loss: 3.0961
[Epoch 30] Loss: 3.0083
Training linear probe (freeze all but classifier)...
[Linear Prob Epoch 1] Loss: 4.1067
[Linear Prob Epoch 2] Loss: 3.4337
[Linear Prob Epoch 3] Loss: 2.9253
[Linear Prob Epoch 4] Loss: 2.5939
[Linear Prob Epoch 5] Loss: 2.2676
[Linear Prob Epoch 6] Loss: 2.0224
[Linear Prob Epoch 7] Loss: 1.8318
[Linear Prob Epoch 8] Loss: 1.6458
[Linear Prob Epoch 9] Loss: 1.4870
[Linear Prob Epoch 10] Loss: 1.3819
[Linear Prob Epoch 11] Loss: 1.2818
[Linear Prob Epoch 12] Loss: 1.2154
[Linear Prob Epoch 13] Loss: 1.1403
[Linear Prob Epoch 14] Loss: 1.0418
[Linear Prob Epoch 15] Loss: 0.9801
[Linear Prob Epoch 16] Loss: 0.9021
[Linear Prob Epoch 17] Loss: 0.8452
[Linear Prob Epoch 18] Loss: 0.7901
[Linear Prob Epoch 19] Loss: 0.7751
[Linear Prob Epoch 20] Loss: 0.7449
[Linear Prob Epoch 21] Loss: 0.7337
[Linear Prob Epoch 22] Loss: 0.6794
[Linear Prob Epoch 23] Loss: 0.6440
[Linear Prob Epoch 24] Loss: 0.6377
[Linear Prob Epoch 25] Loss: 0.5875
[Linear Prob Epoch 26] Loss: 0.5960
[Linear Prob Epoch 27] Loss: 0.5335
[Linear Prob Epoch 28] Loss: 0.5378
[Linear Prob Epoch 29] Loss: 0.5138
[Linear Prob Epoch 30] Loss: 0.5030
Training EnhancedTransformer (concat features)...
[Enhanced Epoch 1] Loss: 3.6756
[Enhanced Epoch 2] Loss: 2.2978
[Enhanced Epoch 3] Loss: 1.8246
[Enhanced Epoch 4] Loss: 1.5055
[Enhanced Epoch 5] Loss: 1.2623
[Enhanced Epoch 6] Loss: 1.0640
[Enhanced Epoch 7] Loss: 0.8997
[Enhanced Epoch 8] Loss: 0.7665
[Enhanced Epoch 9] Loss: 0.6547
[Enhanced Epoch 10] Loss: 0.5659
[Enhanced Epoch 11] Loss: 0.4901
[Enhanced Epoch 12] Loss: 0.4325
[Enhanced Epoch 13] Loss: 0.3810
[Enhanced Epoch 14] Loss: 0.3381
[Enhanced Epoch 15] Loss: 0.3051
[Enhanced Epoch 16] Loss: 0.2745
[Enhanced Epoch 17] Loss: 0.2510
[Enhanced Epoch 18] Loss: 0.2298
[Enhanced Epoch 19] Loss: 0.2115
[Enhanced Epoch 20] Loss: 0.1961
[Enhanced Epoch 21] Loss: 0.1819
[Enhanced Epoch 22] Loss: 0.1699
[Enhanced Epoch 23] Loss: 0.1598
[Enhanced Epoch 24] Loss: 0.1500
[Enhanced Epoch 25] Loss: 0.1419
[Enhanced Epoch 26] Loss: 0.1346
[Enhanced Epoch 27] Loss: 0.1279
[Enhanced Epoch 28] Loss: 0.1212
[Enhanced Epoch 29] Loss: 0.1157
[Enhanced Epoch 30] Loss: 0.1112
Training BaselineAdapterTransformer...
[Epoch 1] Loss: 4.5898
[Epoch 2] Loss: 4.3022
[Epoch 3] Loss: 3.7804
[Epoch 4] Loss: 3.4513
[Epoch 5] Loss: 3.2619
[Epoch 6] Loss: 3.0906
[Epoch 7] Loss: 3.0041
[Epoch 8] Loss: 2.9036
[Epoch 9] Loss: 2.8204
[Epoch 10] Loss: 2.7664
[Epoch 11] Loss: 2.6907
[Epoch 12] Loss: 2.6540
[Epoch 13] Loss: 2.5694
[Epoch 14] Loss: 2.5033
[Epoch 15] Loss: 2.4692
[Epoch 16] Loss: 2.4778
[Epoch 17] Loss: 2.3969
[Epoch 18] Loss: 2.3621
[Epoch 19] Loss: 2.3556
[Epoch 20] Loss: 2.3448
[Epoch 21] Loss: 2.2728
[Epoch 22] Loss: 2.2410
[Epoch 23] Loss: 2.1893
[Epoch 24] Loss: 2.1929
[Epoch 25] Loss: 2.1826
[Epoch 26] Loss: 2.1918
[Epoch 27] Loss: 2.1449
[Epoch 28] Loss: 2.1140
[Epoch 29] Loss: 2.0953
[Epoch 30] Loss: 2.0868
Training distillation student TransformerClassifier...
[Distillation Epoch 1] Loss: 7.2278
[Distillation Epoch 2] Loss: 6.7618
[Distillation Epoch 3] Loss: 6.5579
[Distillation Epoch 4] Loss: 6.4363
[Distillation Epoch 5] Loss: 6.3130
[Distillation Epoch 6] Loss: 6.1339
[Distillation Epoch 7] Loss: 5.9879
[Distillation Epoch 8] Loss: 5.8468
[Distillation Epoch 9] Loss: 5.7119
[Distillation Epoch 10] Loss: 5.6192
[Distillation Epoch 11] Loss: 5.4725
[Distillation Epoch 12] Loss: 5.4083
[Distillation Epoch 13] Loss: 5.3227
[Distillation Epoch 14] Loss: 5.2010
[Distillation Epoch 15] Loss: 5.1253
[Distillation Epoch 16] Loss: 5.0775
[Distillation Epoch 17] Loss: 4.9858
[Distillation Epoch 18] Loss: 4.9352
[Distillation Epoch 19] Loss: 4.8528
[Distillation Epoch 20] Loss: 4.7514
[Distillation Epoch 21] Loss: 4.6983
[Distillation Epoch 22] Loss: 4.6514
[Distillation Epoch 23] Loss: 4.6024
[Distillation Epoch 24] Loss: 4.5117
[Distillation Epoch 25] Loss: 4.4591
[Distillation Epoch 26] Loss: 4.4537
[Distillation Epoch 27] Loss: 4.3557
[Distillation Epoch 28] Loss: 4.2934
[Distillation Epoch 29] Loss: 4.2379
[Distillation Epoch 30] Loss: 4.1737
[Run 5 Results] Base=15.40% | LP=25.84% | ENH=25.64% | ADP=25.00% | DIST=18.82%

All done. Final mean/std results saved to: ./results_test100/noise_tf_cifar100_0.json
noise_tf_test100.py completed successfully.
